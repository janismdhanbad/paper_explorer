{"204e3073870fae3d05bcbc2f6a8e263d9b72e776": {"id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}, {"authorId": "145024664", "name": "Llion Jones", "paperCount": 21, "citationCount": 50175, "hIndex": 15}, {"authorId": "19177000", "name": "Aidan N. Gomez", "paperCount": 32, "citationCount": 48744, "hIndex": 13}, {"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "3443442", "name": "Illia Polosukhin", "paperCount": 13, "citationCount": 48594, "hIndex": 10}], "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 39, "citation_count": 47281, "influential_paper_citations": 10581, "is_open_access": false, "citations": ["370b680057a6e324e67576a6bf1bf580af9fdd74", "659597b1699ba5b73da9a8628bf7e4ad9bebd242", "2435ffb8ed3212156d6b6f19f633a861399cf30e", "44732fe24025c2f17777028bb49bb4a09c31af2a", "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "f83382307aa3e55243da15ea17d7c39720fa1708", "1c83f3f9789df43bf937ae2618721e2da83dcc06", "b9915d00a7f46e51e587aff4365e1c48ab8ef496", "c6d2d950b64973f1bec3b0dd0ed46fbf0319351d", "4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c"], "references": ["032274e57f7d8b456bd255fe76b909b2c1d7458e", "43428880d75b3a14257c3ee9bda054e61eb869c0", "4550a4c714920ef57d19878e31c9ebae37b049b2", "204a4a70428f3938d2c538a4d74c7ae0416306d8", "79baf48bd560060549998d7b61751286de062e2a", "13d9323a8716131911bfda048a40e2cde1a76a46", "510e26733aaff585d65701b9f1be7ca9d5afc586", "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "63e39cdf1ad884da6bc69096bb3413b5b1100559", "98445f4172659ec5e891e031d8202c102135c644", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "b60abe57bc195616063be10638c6437358c81d1e", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "7345843e87c81e24e42264859b214d26042f8d51", "13fe71da009484f240c46f14d9330e932f8de210", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "d76c07211479e233f7c6a6f32d5346c983c5598f", "1af68821518f03568f913ab03fc02080247a27ff", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "34f25a8704614163c4095b3ee2fc969b60de4698", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "174bbdb96252454cbb40a9c4e53335996235a008", "5bfd8d40bc071fffaf93685a46974b122ee4239d", "f52de7242e574b70410ca6fb70b79c811919fc00", "78a9513e70f596077179101f6cb6eadc51602039", "aed054834e2c696807cc8b227ac7a4197196e211", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "0b44fcbeea9415d400c5f5789d6b892b6f98daff"], "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776"}, "032274e57f7d8b456bd255fe76b909b2c1d7458e": {"id": "032274e57f7d8b456bd255fe76b909b2c1d7458e", "title": "A Deep Reinforced Model for Abstractive Summarization", "authors": [{"authorId": "2896063", "name": "Romain Paulus", "paperCount": 8, "citationCount": 2479, "hIndex": 6}, {"authorId": "2228109", "name": "Caiming Xiong", "paperCount": 212, "citationCount": 17287, "hIndex": 57}, {"authorId": "2166511", "name": "R. Socher", "paperCount": 202, "citationCount": 106675, "hIndex": 76}], "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 43, "citation_count": 1187, "influential_paper_citations": 181, "is_open_access": false, "citations": ["3cfb319689f06bf04c2e28399361f414ca32c4b3", "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "63748e59f4e106cbda6b65939b77589f40e48fcb", "1c71771c701aadfd72c5866170a9f5d71464bb88", "9b539d413393047b28bb7be9b195f142aaf7a80e", "0c5598424cc96d8fb500eb553cb7969f86a0ede0", "cc27ec53160d88c25fc5096c0df65536eb780de4", "dbeeca8466e0c177ec67c60d529899232415ca87", "b19729b27a1b4c24b52f87308c907653300afa7f"], "references": ["668db48c6a79826456341680ee1175dfc4cced71", "6c8353697cdbb98dfba4f493875778c4286d3e3a", "1bc49abe5145055f1fa259bd4e700b1eb6b7f08d", "424aef7340ee618132cc3314669400e23ad910ba", "efbd381493bb9636f489b965a2034d529cd56bcd", "63e39cdf1ad884da6bc69096bb3413b5b1100559", "489955574c435169abd72285cfe2f055f538a401", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "267aef492d17592a293aa17ec8a25f7264645bcb", "1d9600229a4cf8ba5e8f5ad4d05b41af9c8f80a6", "2f160ce71f01ac2043de67536ff0e413ff6f58c5", "5ab72d44237533534de8402e30f3ccce25ce30de", "7a67159fc7bc76d0b37930b55005a69b51241635", "507d6e09f51b2fc93f756ab748f6eadd11b7b86e", "aa5b35dcf8b024f5352db73cc3944e8fad4f3793", "129cbad01be98ee88a930e31898cb76be79c41c1", "e957747f4f8600940be4c5bb001aa70c84e53a53", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "13fe71da009484f240c46f14d9330e932f8de210", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "adcfef04625c2763028815759750d47c7c3fe689", "5082a1a13daea5c7026706738f8528391a1e6d59", "0403ca8ad125899996c783f6481c78d432a77106", "d1505c6123c102e53eb19dff312cb25cea840b72", "9653d5c2c7844347343d073bbedd96e05d52f69b", "d5d46991c7e92352865dbf442be7c74d0d560dd8", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "c572e596c3bdd365f43d82ddea1e0d95b8697033", "a9614b05461bb306cc47c8cd645b9b67bb1227ba", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "1da34969c5ba489292cef82bd62206feb016486f", "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da", "68c7c2b9d9a28cc57c9243f2920f2bcfe4e6f498", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "5f176a929d9eaa569b430cb784280802cf8fca79", "60b05f32c32519a809f21642ef1eb3eaf3848008", "4c915c1eecb217c123a36dc6d3ce52d12c742614", "fb56d57d9e64fb2c0af7f19120aae94485df59e2", "30e128568200e6777dc629bc6fb2fb95833aa98c", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "ce9a21b93ba29d4145a8ef6bf401e77f261848de"], "url": "https://www.semanticscholar.org/paper/032274e57f7d8b456bd255fe76b909b2c1d7458e"}, "43428880d75b3a14257c3ee9bda054e61eb869c0": {"id": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning", "authors": [{"authorId": "2401865", "name": "Jonas Gehring", "paperCount": 19, "citationCount": 3583, "hIndex": 11}, {"authorId": "2325985", "name": "Michael Auli", "paperCount": 97, "citationCount": 19553, "hIndex": 45}, {"authorId": "2529182", "name": "David Grangier", "paperCount": 79, "citationCount": 12665, "hIndex": 34}, {"authorId": "13759615", "name": "Denis Yarats", "paperCount": 24, "citationCount": 4242, "hIndex": 18}, {"authorId": "2921469", "name": "Y. Dauphin", "paperCount": 52, "citationCount": 18731, "hIndex": 33}], "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 47, "citation_count": 2675, "influential_paper_citations": 326, "is_open_access": false, "citations": ["ad7ddcc14984caae308c397f1a589aae75d4ab71", "efbd8e7a45cac8f025ba8a4de95b492d8d392c95", "60e69982ef2920596c6f31d6fd3ca5e9591f3db6", "375479213a9982ecf4363669bc36449ca11421a8", "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "003326a15fc4a8833785a47a741d7712474fa256", "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "b79cd3e0e2b154922b5cd16e1fbed9cb44e5a59f", "a824c6e214dd0118f70af8bb05d67d94a858d076", "f0524b3005720bcff886bcb0227f7f0dd924ff07"], "references": ["510e26733aaff585d65701b9f1be7ca9d5afc586", "364f7f7bac907ce326dce84b26eb857f186d3dc2", "88caa4a0253a8b0076176745ebc072864eab66e1", "f958d4921951e394057a1c4ec33bad9a34e5dad1", "2d876ed1dd2c58058d7197b734a8e4d349b8f231", "a3fbd2bd5dc1de28a36da5503030d9c648ce7f6d", "9486f640f90b7c3ddb0d8adff6fa16dd9758746a", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "1a327709cc53ff9e52454e50a643abf4a0ac92af", "0936352b78a52bc5d2b5e3f04233efc56664af51", "b60abe57bc195616063be10638c6437358c81d1e", "1a5ea605111eb3403868d4b679315e944beee8c6", "bba5f2852b1db8a18004eb7328efa5e1d57cc62a", "cd0009c2819f9566930d520da46ca67e4ccf226d", "03ee3c8994edfc3bca62b51fb4d4cc13595b5046", "3d2c6941a9b4608ba52b328369a3352db2092ae0", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "1af68821518f03568f913ab03fc02080247a27ff", "5082a1a13daea5c7026706738f8528391a1e6d59", "25eb839f39507fe6983ad3e692b2f8d93a5cb0cc", "93499a7c7f699b6630a86fad964536f9423bb6d0", "b624504240fa52ab76167acfe3156150ca01cf3b", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "4d376d6978dad0374edfa6709c9556b42d3594d3", "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "34f25a8704614163c4095b3ee2fc969b60de4698", "aa7bfd2304201afbb19971ebde87b17e40242e91", "7b5e31257f01aba987f16e175a3e49e00a5bd3bb", "84069287da0a6b488b8c933f3cb5be759cb6237e", "ed6262b569c0a62c51d941228c54f34e563af022", "3449b65008b27f6e60a73d80c1fd990f0481126b", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "0be949cc24188ef7205bdaaeb7df2508344b8d5a", "60b05f32c32519a809f21642ef1eb3eaf3848008", "3fa4a8191e37b601877716858e6b1026e66e3c5c", "563e821bb5ea825efb56b77484f5287f08cf3753", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "501ffe07846b34c91b3dd6fdb4fc02f22087add2", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "cd62c9976534a6a2096a38244f6cbb03635a127e"], "url": "https://www.semanticscholar.org/paper/43428880d75b3a14257c3ee9bda054e61eb869c0"}, "4550a4c714920ef57d19878e31c9ebae37b049b2": {"id": "4550a4c714920ef57d19878e31c9ebae37b049b2", "title": "Massive Exploration of Neural Machine Translation Architectures", "authors": [{"authorId": "3908643", "name": "D. Britz", "paperCount": 10, "citationCount": 906, "hIndex": 8}, {"authorId": "46684455", "name": "Anna Goldie", "paperCount": 30, "citationCount": 1233, "hIndex": 12}, {"authorId": "1707242", "name": "Minh-Thang Luong", "paperCount": 41, "citationCount": 9690, "hIndex": 24}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}], "abstract": "Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in TensorFlow to make it easy for others to reproduce our results and perform their own experiments.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 33, "citation_count": 438, "influential_paper_citations": 33, "is_open_access": true, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "38643c2926b10f6f74f122a7037e2cd20d77c0f1", "987b2db58fbe0bda771f11a046cd23de1ce92b39", "6a0aaefce8a27a8727d896fa444ba27558b2d381", "0f70913f6f3eac1ffc3efb992005272164094473", "2a2bdf5cf0d73bc333423a8fd246593f4bf65322", "03547cf81db895a448c3d0283bdfa20695ed26ab", "d58fd97d7a8308a20b159bbaf02011e26c323852", "a39398f68ae7e042f2ef5009e31b4e6a20fd5736", "b145ea2049648535d6081407ebd315b072248183"], "references": ["aab5002a22b9b4244a8329b140bd0a86021aa2d1", "a312a573ef81793d56401e932ef6c9498791a3d1", "31fc1b0fd5ec43863f1a502f6fc3df2cc71b6e6f", "f958d4921951e394057a1c4ec33bad9a34e5dad1", "5694e46284460a648fe29117cbc55f6c9be3fa3c", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "b60abe57bc195616063be10638c6437358c81d1e", "1a5ea605111eb3403868d4b679315e944beee8c6", "46200b99c40e8586c8a0f588488ab6414119fb28", "733b821faeebe49b6efcf5369e3b9902b476529e", "acec46ffd3f6046af97529127d98f1d623816ea4", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "33108287fbc8d94160787d7b2c7ef249d3ad6437", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "651e5bcc14f14605a879303e97572a27ea8c7956", "1af68821518f03568f913ab03fc02080247a27ff", "93499a7c7f699b6630a86fad964536f9423bb6d0", "5247a6e3a60ff0381355e66bfc313bf27512ae0c", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "e0945081b5b87187a53d4329cf77cd8bff635795", "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "1956c239b3552e030db1b78951f64781101125ed", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/4550a4c714920ef57d19878e31c9ebae37b049b2"}, "204a4a70428f3938d2c538a4d74c7ae0416306d8": {"id": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding", "authors": [{"authorId": "3146592", "name": "Zhouhan Lin", "paperCount": 33, "citationCount": 7083, "hIndex": 16}, {"authorId": "2521552", "name": "Minwei Feng", "paperCount": 20, "citationCount": 2179, "hIndex": 10}, {"authorId": "1790831", "name": "C. D. Santos", "paperCount": 80, "citationCount": 8351, "hIndex": 27}, {"authorId": "2482533", "name": "Mo Yu", "paperCount": 95, "citationCount": 7440, "hIndex": 36}, {"authorId": "144028698", "name": "Bing Xiang", "paperCount": 118, "citationCount": 10061, "hIndex": 40}, {"authorId": "145218984", "name": "Bowen Zhou", "paperCount": 222, "citationCount": 11970, "hIndex": 40}, {"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}], "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 37, "citation_count": 1676, "influential_paper_citations": 160, "is_open_access": false, "citations": ["a88c914f5a738d38f02790bb5de41453bf17bde1", "5156381d63bb3e873533b08f203cb56c2d79b6c9", "e75638f8dee71ea0439e139f53411a98a9c9f825", "95ed88b46c9dea9d32badc61213e92a86bda9520", "7e3d5b20e5df692deb80d9e100e4f34c1a8f8031", "bc955f7102429f67fddf95940db8574cfdcce76e", "a8a168d53e01b0c35d626cfced103656e22b8343", "1109f787fc8d51feb3bae9bf6e1945dc4a1191e7", "a3be9acba7b1f847e6d091c540536753ec66669c", "4e99406c10b61004826a0428634ad5c7b0f2f731"], "references": ["bcd857d75841aa3e92cd4284a8818aba9f6c0c3f", "705dcc8eadba137834e4b0359e2d696d4b209f5b", "cff79255a94b9b05a4ce893eb403a522e0923f04", "1261fe9bfde319abcc5d011bc70f7e7547b5258f", "bdf28e3cadbabda3261bd904c37edea66ab84766", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "f93a0a3e8a3e6001b4482430254595cf737697fa", "6b570069f14c7588e066f7138e1f21af59d62e61", "f292139b371f5b34e835f4dbff102dc28f972876", "36c097a225a95735271960e2b63a2cb9e98bff83", "a2dc06c8da0ff9344dc558d6df571fc704b81ae7", "26e743d5bd465f49b9538deaf116c15e61b7951f", "13fe71da009484f240c46f14d9330e932f8de210", "ea407573bfcd39f9a478fe33cf6ce0ee1780a5f0", "46b8cbcdff87b842c2c1d4a003c831f845096ba7", "22ae02d81c21cb90b0de071550cfb99e6a623e62", "ce8ec6352d9ed8212de5e2441880b37452030383", "1347bd4f826f72ff561b70e665477edadb2a72be", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "f33e86970c11f9bb6d0abb60acdc9274d5c3f342", "57e562b46338f176e3b20c2dd0b66f17dfbef9e8", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "bd32ebb9fac53a14202fb1a4f76ef96d1ff68c6c", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "1682b8b395c7d7fa30b3cec961ac81fdda53e72d", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "b0aca3e7877c3c20958b0fae5cbf2dd602104859", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "687bac2d3320083eb4530bf18bb8f8f721477600", "f282338fa4cd5516cdcd33cd4b6034f9739c45f4", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "cfa2646776405d50533055ceb1b7f050e9014dcb", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/204a4a70428f3938d2c538a4d74c7ae0416306d8"}, "79baf48bd560060549998d7b61751286de062e2a": {"id": "79baf48bd560060549998d7b61751286de062e2a", "title": "Factorization tricks for LSTM networks", "authors": [{"authorId": "2787022", "name": "O. Kuchaiev", "paperCount": 30, "citationCount": 3035, "hIndex": 17}, {"authorId": "31963005", "name": "Boris Ginsburg", "paperCount": 58, "citationCount": 3037, "hIndex": 21}], "abstract": "We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 18, "citation_count": 98, "influential_paper_citations": 14, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "88caa4a0253a8b0076176745ebc072864eab66e1", "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "901e5f840daa0609a80232ec363d7c77de25b611", "7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7", "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83", "e70d609ce18cd61799b087bf3a5e14c1ce70a41a", "d520ac6248716819f0c6d0c03737d2e738135b1e", "a07609c2ed39d049d3e59b61408fb600c6ab0950"], "references": ["510e26733aaff585d65701b9f1be7ca9d5afc586", "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "579e0077a3810510a7965224a8782ecc01766ea0", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "29316449c7cc52ad326c5d1bd5b0dc5af27c1496", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "f9c990b1b5724e50e5632b94fdb7484ece8a6ce7", "eb42cf88027de515750f230b23b1a057dc782108", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "067e07b725ab012c80aa2f87857f6791c1407f6d", "e8650503ab80ad7299f0845b1843abf3a97f313a", "84069287da0a6b488b8c933f3cb5be759cb6237e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "398c296d0cc7f9d180f84969f8937e6d3a413796", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "d0be39ee052d246ae99c082a565aba25b811be2d"], "url": "https://www.semanticscholar.org/paper/79baf48bd560060549998d7b61751286de062e2a"}, "13d9323a8716131911bfda048a40e2cde1a76a46": {"id": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks", "authors": [{"authorId": "38367242", "name": "Yoon Kim", "paperCount": 26, "citationCount": 14842, "hIndex": 18}, {"authorId": "47472547", "name": "Carl Denton", "paperCount": 1, "citationCount": 354, "hIndex": 1}, {"authorId": "144294755", "name": "Luong Hoang", "paperCount": 8, "citationCount": 489, "hIndex": 4}, {"authorId": "2531268", "name": "Alexander M. Rush", "paperCount": 135, "citationCount": 13993, "hIndex": 45}], "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 66, "citation_count": 354, "influential_paper_citations": 27, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "adc276e6eae7051a027a4c269fb21dae43cadfed", "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f", "013faec0400d315935e71a2bdfeb22cc83752b3e", "027f9695189355d18ec6be8e48f3d23ea25db35d", "cb8f1840a018fddcc7a453dd73cc082ebea82e7b", "d7dc79050f17154e7cf57501cf6cab1b9c18f232", "e9b13731027418ed38103d1dfc8a70f6881bc684", "76faaf292c6d9dc29d3a99300a7fdd7a35d6d107"], "references": ["bcd857d75841aa3e92cd4284a8818aba9f6c0c3f", "d19b712f90cde698cc96ebd5fe291b410e3f0f9c", "83e7654d545fbbaaf2328df365a781fb67b841b4", "705dcc8eadba137834e4b0359e2d696d4b209f5b", "e06a68b26bde368883761c9dceb547914b2ecca8", "222c23cba96dd487eee0722d3d302cc97e70d18e", "784ee73d5363c711118f784428d1ab89f019daa5", "e8135016ff3bd33ace936e50247fd650fcc58a7a", "d9d1def2c3cd2af5705a736534f7682a967d9126", "f61da0efbb38bd3e6b9a9855809f5288b829f1f0", "162db03ef3cb50a07ff54ae4a1d4ea120e4162f2", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "53ca01f8c593b2339f292672b183235da5f6ce70", "4be0dd53aa1c751219fa6f19fed8a6324f6d2766", "eec3a236ecd185712ce65fb336141f8656eea13d", "24158c9fc293c8a998ac552b1188404a877da292", "36c097a225a95735271960e2b63a2cb9e98bff83", "aedffcebea081138a0f2bf2454f872700237fbf6", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "ea407573bfcd39f9a478fe33cf6ce0ee1780a5f0", "bc82b4f9f202062857958f0336fc28327a75563b", "6b904d6e84c98c6ce22ce6923224b205a2a24ee1", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f", "b8f4a8028c31dbc4b9cf3ff61e5399bf7f14f978", "93499a7c7f699b6630a86fad964536f9423bb6d0", "1abe41711155afe82222ac0f99b978b32b1e68b5", "dc555e8156c956f823587ebbff018863e6d2a95e", "654a3e53fb41d8168798ee0ee61dfab73739b1ed", "bb50784deeb9d74106de7639db775ec3bfa43df3", "b624504240fa52ab76167acfe3156150ca01cf3b", "438bb3d46e72b177ed1c9b7cd2c11a045644a1f4", "04d1a26c2516dc14a765112a63ec60dc3cb3de72", "d1505c6123c102e53eb19dff312cb25cea840b72", "9653d5c2c7844347343d073bbedd96e05d52f69b", "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "e2820bffe5b42cb7d88b7f65c12171c62ab4aae2", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "21da448e7c31e1ff6cc3b7155a9c9c49a0138060", "71ae756c75ac89e2d731c9c79649562b5768ff39", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "c66758c1029a463489f26aeb3955f333b37f727a", "c3823aacea60bc1f2cabb9283144690a3d015db5", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "2418131d18173161b48575839d145091c3178a72", "5b0a44014c24f9b584904bf223530a3b9fa9853f", "80f3ec80d1114bcd9a8ad9b56dd7b230ec16c048", "fba7c0a51a6301ca4086a5ce59b1f13af9acad7f", "04e9385a1267d75197f695acf83e314668f6ae52", "413c1142de9d91804d6d11c67ff3fed59c9fc279", "bc1022b031dc6c7019696492e8116598097a8c12", "c3227702dd212965157a615332f3dd78b0f11b5e", "f8a96652a43c2b6bd087ece6c3d67f097d0c7c9e", "394c6c50445ab4ccd1c79fbc2db8f35994ef9f15", "ebbb58b2e616435b6ededbc103acfef6dc79bd51", "f4ba954b0412773d047dc41231c733de0c1f4926", "162d958ff885f1462aeda91cd72582323fd6a1f4", "adfef97814b292a09520d8c78a141e7a4baf8726", "6c79a9bb8f885050cad70b4c69e016b186ffa538"], "url": "https://www.semanticscholar.org/paper/13d9323a8716131911bfda048a40e2cde1a76a46"}, "510e26733aaff585d65701b9f1be7ca9d5afc586": {"id": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "authors": [{"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "1861312", "name": "Azalia Mirhoseini", "paperCount": 76, "citationCount": 2617, "hIndex": 22}, {"authorId": "50351613", "name": "Krzysztof Maziarz", "paperCount": 13, "citationCount": 1111, "hIndex": 5}, {"authorId": "36347083", "name": "Andy Davis", "paperCount": 10, "citationCount": 25095, "hIndex": 7}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}, {"authorId": "1695689", "name": "Geoffrey E. Hinton", "paperCount": 398, "citationCount": 383672, "hIndex": 147}, {"authorId": "48448318", "name": "J. Dean", "paperCount": 35, "citationCount": 6306, "hIndex": 19}], "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 45, "citation_count": 1032, "influential_paper_citations": 112, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "306a2e8ca31fdcc148618d37074785c290f96375", "7cfa5c97164129ce3630511f639040d28db1d4b7", "5b01eaef54a653ba03ddd5a978690380fbc19bfc", "fdacf2a732f55befdc410ea927091cad3b791f13", "f6eee02a2f4c74c2b543bf419f76cef60d5752f8"], "references": ["66b8d34477cf1736f91fd22b27e37ce0b703c86e", "a486e2839291111bb44fa1f07731ada123539f75", "b0a2719ffd8c0cf8fe8b4aeca118c7a71cf94fef", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "b60abe57bc195616063be10638c6437358c81d1e", "f61e9fd5a4878e1493f7a6b03774a61c17b7e9a4", "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "8ff840a40d3f1557c55c19d4d636da77103168ce", "5a5d48986b855b83a7d9df5005bbd155024ce756", "94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f", "fba71eefd060e30f3516fdd46df9a191cd0aaaf7", "93499a7c7f699b6630a86fad964536f9423bb6d0", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "e2e81c568ac0aa067e32fbc9ca9396824fa04d66", "4d376d6978dad0374edfa6709c9556b42d3594d3", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "1956c239b3552e030db1b78951f64781101125ed", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "8829e3873846c6bbad5aca111e64f9d2c1b24299", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "97cedf99252026f58e8154bc61d49cf885d42030", "44ddac48353ead135eef4096859956eaa31be2a5", "cf3229e74f912ef365d67d1954441b32ce2573ee", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "067e07b725ab012c80aa2f87857f6791c1407f6d", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "31868290adf1c000c611dfc966b514d5a34e8d23", "ed6262b569c0a62c51d941228c54f34e563af022", "413c1142de9d91804d6d11c67ff3fed59c9fc279", "61854f305fed48a52b53884c138de8fa90772cfb", "81c9cb2c3c08070a80c7b26c288789084f70b43b", "16174aa9f7b2f72a078b1301244367f40a754502", "480d517574a079d3e0159b978cb19b3f014e59a3", "11540131eae85b2e11d53df7f1360eeb6476e7f4", "421c01f0f005e5bd72e578162d07e6540e3beb09", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "f6d8a7fc2e2d53923832f9404376512068ca2a57", "c8d90974c3f3b40fa05e322df2905fc16204aa56"], "url": "https://www.semanticscholar.org/paper/510e26733aaff585d65701b9f1be7ca9d5afc586"}, "5b6ec746d309b165f9f9def873a2375b6fb40f3d": {"id": "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "title": "Xception: Deep Learning with Depthwise Separable Convolutions", "authors": [{"authorId": "1565641737", "name": "Fran\u00e7ois Chollet", "paperCount": 22, "citationCount": 10847, "hIndex": 12}], "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 27, "citation_count": 8218, "influential_paper_citations": 1409, "is_open_access": true, "citations": ["1320250a833b94c11168ba77ba4a9806e60629a1", "1e33716e8820b867d5a8aaebab44c2d3135ea4ac", "03c637e8cae7395fc5e33c59c53058ae2bc0a11d", "02a615c807a6f0f692f4d89a62f11c00314492fc", "574bcb4aba88cd7a296d584a5bcb99bd769705d8", "84f3290630be8dff726ca43195ed6c82bb65d0d9", "728ddffea17d18cf765a38fe526908716d6f2a0e", "e031687d491994604dcdb4fd663ac646433a9b7e", "73c8338cc4be6f066620925eb9fded4d78b66542", "92a6b3aeb7765fce1943d5109e3942b68418ac20"], "references": ["3647d6d0f151dc05626449ee09cc7bce55be497e", "977bf40fe28f773b95d6802e694505beac78441d", "b5c26ab8767d046cb6e32d959fdf726aee89bb62", "d20da75866f500ee9fbfa859e69556702e1f50a4", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "f63e917638553414526a0cc8550de4ad2d83fe7a", "0c908739fbff75f03469d13d4a1a07de3414ee19", "4d376d6978dad0374edfa6709c9556b42d3594d3", "30ae2bdef407e7a05b83781907ad7fb257dcd7a1", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "528beaa995bf1bd1ae451b18218674af9ecd2b50", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "4dbc68cf2e14155edb6da0def30661aca8c96c22", "842dd6d0f4b72ce0e8f3ac8e6861637c1f4645ea", "6dc61f37ecc552413606d8c89ffbc46ec98ed887"], "url": "https://www.semanticscholar.org/paper/5b6ec746d309b165f9f9def873a2375b6fb40f3d"}, "63e39cdf1ad884da6bc69096bb3413b5b1100559": {"id": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models", "authors": [{"authorId": "40170001", "name": "Ofir Press", "paperCount": 10, "citationCount": 894, "hIndex": 9}, {"authorId": "145128145", "name": "Lior Wolf", "paperCount": 242, "citationCount": 20908, "hIndex": 55}], "abstract": "We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 45, "citation_count": 602, "influential_paper_citations": 33, "is_open_access": true, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "67d968c7450878190e45ac7886746de867bf673d", "921196c32213a229245a9705ee4768bc941e7a26", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "81f5810fbbab9b7203b9556f4ce3c741875407bc", "58c6f890a1ae372958b7decf56132fe258152722", "032274e57f7d8b456bd255fe76b909b2c1d7458e", "3c623c08329e129e784a5d03f7606ec8feba3a28", "6ce1922802169f757bbafc6e087cc274a867c763", "14d8b4fdb0262c30ae9afe20ea8e7227b115c63e"], "references": ["67d968c7450878190e45ac7886746de867bf673d", "424aef7340ee618132cc3314669400e23ad910ba", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "579e0077a3810510a7965224a8782ecc01766ea0", "1a5ea605111eb3403868d4b679315e944beee8c6", "58001259d2f6442b07cc0d716ff99899abbb2bc7", "568374ac9433e29b812008b2a01f81e657bdbd34", "19d6da9fc75ff04116d022d5bb4b6539b741484a", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "1af68821518f03568f913ab03fc02080247a27ff", "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6", "63c490f2a3330e3685e7df50973278296905f63b", "0c908739fbff75f03469d13d4a1a07de3414ee19", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "7a96765c147c9c814803c8c9de28a1dd069271da", "dbd5e38086fce7210bff8b9038b6b41fb2212d93", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "0b544dfe355a5070b60986319a3f51fb45d1348e", "2012f32199adc88747d5a1b47c7b4ba1cb3cb995", "533ee188324b833e059cb59b654e6160776d5812", "83a768a0720d0a1f68792827a422395001291614", "53ca064b9f1b92951c1997e90b776e95b0880e52", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "53ab89807caead278d3deb7b6a4180b277d3cb77", "db734a0e1dc65fe3fe2eef474aefba6d083f54dd", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "8729441d734782c3ed532a7d2d9611b438c0a09a", "1a8c33f9e51ba01e1cdade7029f96892c7c7087b", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "f9a1b3850dfd837793743565a8af95973d395a4e", "396aabd694da04cdb846cb724ca9f866f345cbd5", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "9819b600a828a57e1cde047bbe710d3446b30da5", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "4bc1678b9d2ffc9af22a6c3b786ad79a941f0b86", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8", "0b44fcbeea9415d400c5f5789d6b892b6f98daff"], "url": "https://www.semanticscholar.org/paper/63e39cdf1ad884da6bc69096bb3413b5b1100559"}, "98445f4172659ec5e891e031d8202c102135c644": {"id": "98445f4172659ec5e891e031d8202c102135c644", "title": "Neural Machine Translation in Linear Time", "authors": [{"authorId": "2583391", "name": "Nal Kalchbrenner", "paperCount": 35, "citationCount": 30929, "hIndex": 25}, {"authorId": "2311318", "name": "Lasse Espeholt", "paperCount": 12, "citationCount": 6611, "hIndex": 9}, {"authorId": "34838386", "name": "K. Simonyan", "paperCount": 115, "citationCount": 128670, "hIndex": 55}, {"authorId": "3422336", "name": "A\u00e4ron van den Oord", "paperCount": 57, "citationCount": 23700, "hIndex": 36}, {"authorId": "1753223", "name": "A. Graves", "paperCount": 83, "citationCount": 83500, "hIndex": 54}, {"authorId": "2645384", "name": "K. Kavukcuoglu", "paperCount": 102, "citationCount": 115978, "hIndex": 67}], "abstract": "We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 50, "citation_count": 479, "influential_paper_citations": 42, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "921196c32213a229245a9705ee4768bc941e7a26", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "fae129338c0899576524506008427f64477d3967", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "1193317829bfcc9b9dffa5ae85a2e2114254b37e", "2b954c9750b1d4a96916664ed8ab35d1aa589304", "807370352540f171685998aec7a5701f7110f147", "66386a946a04534275bd466862364d139790f41f"], "references": ["65eee67dee969fdf8b44c87c560d66ad4d78e233", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "df0402517a7338ae28bc54acaac400de6b456a46", "7dded890956b37df5ac4c42b8ffbc142725f2801", "136cf66392f1d6bf42da4cc070888996dc472b91", "b60abe57bc195616063be10638c6437358c81d1e", "733b821faeebe49b6efcf5369e3b9902b476529e", "acec46ffd3f6046af97529127d98f1d623816ea4", "77f0a39b8e02686fd85b01971f8feb7f60971f80", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "7f5fc84819c0cf94b771fe15141f65b123f7b8ec", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "93499a7c7f699b6630a86fad964536f9423bb6d0", "4d376d6978dad0374edfa6709c9556b42d3594d3", "d14c7e5f5cace4c925abc74c88baa474e9f31a28", "39ad6c911f3351a3b390130a6e4265355b4d593b", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "24cf0e72cf1eb6d35c8b3889bb1068a210edc3d2", "56fcf886de43d431590c5617546f75c4c16f3ad4", "dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "a17745f1d7045636577bcd5d513620df5860e9e5", "9819b600a828a57e1cde047bbe710d3446b30da5", "aed054834e2c696807cc8b227ac7a4197196e211", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/98445f4172659ec5e891e031d8202c102135c644"}, "735d547fc75e0772d2a78c46a1cc5fad7da1474c": {"id": "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "title": "Can Active Memory Replace Attention?", "authors": [{"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "1751569", "name": "Samy Bengio", "paperCount": 379, "citationCount": 51213, "hIndex": 85}], "abstract": "Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 27, "citation_count": 50, "influential_paper_citations": 4, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "9c5c89199114858eafbe50b46d77d38ffd03b28a", "9f1e9e56d80146766bc2316efbc54d8b770a23df", "642c1b4a9da95ea4239708afc5929a5007a1870d", "98445f4172659ec5e891e031d8202c102135c644", "1778e32c18bd611169e64c1805a51abff341ca53", "6a0d253053ce8646e49904efe0e062bcc30d8257", "2d08ed53491053d84b6de89aedbf2178b9c8cf84", "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83", "f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751"], "references": ["aaf08e37bcd0f4624d8eb04f301bfa98b0456641", "0811597b0851b7ebe21aadce7cb4daac4664b44f", "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "33108287fbc8d94160787d7b2c7ef249d3ad6437", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "4e8930ae948262a89acf2e43c8e8b6e902c312c4", "d5eadd6f059d742d76441fd0a635a21694dd7392", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "f9c990b1b5724e50e5632b94fdb7484ece8a6ce7", "eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "c3823aacea60bc1f2cabb9283144690a3d015db5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "0b544dfe355a5070b60986319a3f51fb45d1348e", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "6658bbf68995731b2083195054ff45b4eca38b3a", "4c915c1eecb217c123a36dc6d3ce52d12c742614", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/735d547fc75e0772d2a78c46a1cc5fad7da1474c"}, "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd": {"id": "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "authors": [{"authorId": "48607963", "name": "Yonghui Wu", "paperCount": 84, "citationCount": 19126, "hIndex": 49}, {"authorId": "144927151", "name": "M. Schuster", "paperCount": 46, "citationCount": 28668, "hIndex": 20}, {"authorId": "2545358", "name": "Z. Chen", "paperCount": 323, "citationCount": 43170, "hIndex": 49}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}, {"authorId": "144739074", "name": "Mohammad Norouzi", "paperCount": 169, "citationCount": 26441, "hIndex": 53}, {"authorId": "3153147", "name": "Wolfgang Macherey", "paperCount": 45, "citationCount": 8305, "hIndex": 28}, {"authorId": "2048712", "name": "M. Krikun", "paperCount": 33, "citationCount": 8199, "hIndex": 17}, {"authorId": "145144022", "name": "Yuan Cao", "paperCount": 35, "citationCount": 6638, "hIndex": 16}, {"authorId": "145312180", "name": "Qin Gao", "paperCount": 24, "citationCount": 5880, "hIndex": 13}, {"authorId": "113439369", "name": "Klaus Macherey", "paperCount": 20, "citationCount": 6427, "hIndex": 13}, {"authorId": "2367620", "name": "J. Klingner", "paperCount": 20, "citationCount": 6520, "hIndex": 13}, {"authorId": "145825976", "name": "Apurva Shah", "paperCount": 44, "citationCount": 5581, "hIndex": 11}, {"authorId": "145657834", "name": "Melvin Johnson", "paperCount": 36, "citationCount": 9919, "hIndex": 22}, {"authorId": "2109059862", "name": "Xiaobing Liu", "paperCount": 15, "citationCount": 9277, "hIndex": 10}, {"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "2776283", "name": "Stephan Gouws", "paperCount": 22, "citationCount": 7162, "hIndex": 12}, {"authorId": "2739610", "name": "Y. Kato", "paperCount": 36, "citationCount": 5743, "hIndex": 10}, {"authorId": "1765329", "name": "Taku Kudo", "paperCount": 88, "citationCount": 12719, "hIndex": 32}, {"authorId": "1754386", "name": "H. Kazawa", "paperCount": 28, "citationCount": 6178, "hIndex": 14}, {"authorId": "144077726", "name": "K. Stevens", "paperCount": 14, "citationCount": 5854, "hIndex": 8}, {"authorId": "1753079661", "name": "George Kurian", "paperCount": 29, "citationCount": 6955, "hIndex": 14}, {"authorId": "2056800684", "name": "Nishant Patil", "paperCount": 17, "citationCount": 8992, "hIndex": 9}, {"authorId": "49337181", "name": "Wei Wang", "paperCount": 24, "citationCount": 6609, "hIndex": 11}, {"authorId": "39660914", "name": "C. Young", "paperCount": 77, "citationCount": 12886, "hIndex": 29}, {"authorId": "2119125158", "name": "Jason R. Smith", "paperCount": 13, "citationCount": 5922, "hIndex": 7}, {"authorId": "2909504", "name": "Jason Riesa", "paperCount": 20, "citationCount": 5615, "hIndex": 14}, {"authorId": "29951847", "name": "Alex Rudnick", "paperCount": 17, "citationCount": 5321, "hIndex": 6}, {"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "32131713", "name": "G. Corrado", "paperCount": 21, "citationCount": 80305, "hIndex": 16}, {"authorId": "48342565", "name": "Macduff Hughes", "paperCount": 7, "citationCount": 7241, "hIndex": 6}, {"authorId": "49959210", "name": "J. Dean", "paperCount": 70, "citationCount": 121889, "hIndex": 40}], "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 53, "citation_count": 5192, "influential_paper_citations": 391, "is_open_access": false, "citations": ["fcecc4ef2c32dbedda61648febb39a0f905c367e", "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "f55781f7ce6fd31e946f0efe76d5bf89858391d1", "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe", "9653c070724e44f023e8cc3ec79f0b9e6d59480d", "197d5867a45a2988f4dd159063cdfbfe90164962", "969287b8a96e242793b11f0dbb99ec341228106f", "ba9d736006b897d06f75586ad46e28e00a5e566e", "eea16dfc29f0521dd547e67a84af4ff95a9c5529"], "references": ["1d9600229a4cf8ba5e8f5ad4d05b41af9c8f80a6", "b60abe57bc195616063be10638c6437358c81d1e", "46200b99c40e8586c8a0f588488ab6414119fb28", "10a2482088e469dd40f49bdc9978b292b3f7bb1f", "733b821faeebe49b6efcf5369e3b9902b476529e", "aa5b35dcf8b024f5352db73cc3944e8fad4f3793", "acec46ffd3f6046af97529127d98f1d623816ea4", "4d070993cb75407b285e14cb8aac0077624ef4d9", "44a9d4711a5e9fa36bbd56aed8d5291acd9e8876", "d3cb9bad655197b52932978dd8186b36c512bf92", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "9f2a8e923965b23c11066a2ead79658208f1fae1", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "d76c07211479e233f7c6a6f32d5346c983c5598f", "642d0f49b7826adcf986616f4af77e736229990f", "1af68821518f03568f913ab03fc02080247a27ff", "93499a7c7f699b6630a86fad964536f9423bb6d0", "83cf4b2f39bcc802b09fd59b69e23068447b26b7", "b7cf49e30355633af2db19f35189410c8515e91f", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "1956c239b3552e030db1b78951f64781101125ed", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "0b544dfe355a5070b60986319a3f51fb45d1348e", "97cedf99252026f58e8154bc61d49cf885d42030", "0894b06cff1cd0903574acaa7fcf071b144ae775", "8e4fb17fff38a7834af5b4eaafcbbde02bf00975", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "3127190433230b3dc1abd0680bb58dced4bcd90e", "c5145b1d15fea9340840cc8bb6f0e46e8934827f", "ed6262b569c0a62c51d941228c54f34e563af022", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "aed054834e2c696807cc8b227ac7a4197196e211", "11540131eae85b2e11d53df7f1360eeb6476e7f4", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "ab7b5917515c460b90451e67852171a531671ab8", "26affdaceca32cd4a5fde0db61ffef02a59baa13", "a1066659ec1afee9dce586f6f49b7d44527827e1", "995a3b11cc8a4751d8e167abc4aa937abc934df0", "2166fa493a8c6e40f7f8562d15712dd3c75f03df"], "url": "https://www.semanticscholar.org/paper/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd"}, "b60abe57bc195616063be10638c6437358c81d1e": {"id": "b60abe57bc195616063be10638c6437358c81d1e", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "authors": [{"authorId": "49178343", "name": "Jie Zhou", "paperCount": 539, "citationCount": 5678, "hIndex": 37}, {"authorId": "2112866139", "name": "Ying Cao", "paperCount": 4, "citationCount": 668, "hIndex": 4}, {"authorId": "2108084524", "name": "Xuguang Wang", "paperCount": 4, "citationCount": 274, "hIndex": 3}, {"authorId": "144326610", "name": "Peng Li", "paperCount": 45, "citationCount": 2236, "hIndex": 20}, {"authorId": "145738410", "name": "W. Xu", "paperCount": 557, "citationCount": 16808, "hIndex": 57}], "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT\u201914 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT\u201914 English-to-German task.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 33, "citation_count": 186, "influential_paper_citations": 21, "is_open_access": true, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "43428880d75b3a14257c3ee9bda054e61eb869c0", "97faeefa771e8cc8e55159e2bd03e6f5eef249a8", "a486e2839291111bb44fa1f07731ada123539f75", "6ce1922802169f757bbafc6e087cc274a867c763", "510e26733aaff585d65701b9f1be7ca9d5afc586", "c41516420ddbd0f29e010ca259a74c1fc2da0466", "2113af2c71526b211612586daa185d326712cb9c", "563783de03452683a9206e85fe6d661714436686"], "references": ["2c03df8b48bf3fa39054345bafabfeff15bfd11d", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "d3e1c2d8eddbfd01dce03b5d9ff3c6df8259dbbe", "89a6412f009f6ffbdd11399bda8b02161caf56fc", "a739ae988ba0e3ff232f4507627dfc282ba7b3f4", "c34e41312b47f60986458759d5cc546c2b53f748", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "d1505c6123c102e53eb19dff312cb25cea840b72", "e0945081b5b87187a53d4329cf77cd8bff635795", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "1956c239b3552e030db1b78951f64781101125ed", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "97cedf99252026f58e8154bc61d49cf885d42030", "8e4fb17fff38a7834af5b4eaafcbbde02bf00975", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "8729441d734782c3ed532a7d2d9611b438c0a09a", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "522e90b9fccfd3c1c0603359eb04757d770c1ab5", "375214ac340226e23ec428e92ec499fb89f508b8", "f4f6bfacb4cd508df62540f5aa9ba30cd83dd127", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "162d958ff885f1462aeda91cd72582323fd6a1f4", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "9d6c43ad7f4cf7ebcd4245510df2880f3d3b0964", "73c2a58c936ba2d269491548ef32644c5e982199", "d0be39ee052d246ae99c082a565aba25b811be2d"], "url": "https://www.semanticscholar.org/paper/b60abe57bc195616063be10638c6437358c81d1e"}, "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa": {"id": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "title": "Exploring the Limits of Language Modeling", "authors": [{"authorId": "1944541", "name": "R. J\u00f3zefowicz", "paperCount": 29, "citationCount": 17633, "hIndex": 14}, {"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "144927151", "name": "M. Schuster", "paperCount": 46, "citationCount": 28668, "hIndex": 20}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "48607963", "name": "Yonghui Wu", "paperCount": 84, "citationCount": 19126, "hIndex": 49}], "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 54, "citation_count": 991, "influential_paper_citations": 130, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "9405cc0d6169988371b2755e573cc28650d14dfe", "46200b99c40e8586c8a0f588488ab6414119fb28", "3febb2bed8865945e7fddc99efd791887bb7e14f", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "df0402517a7338ae28bc54acaac400de6b456a46", "b227f3e4c0dc96e5ac5426b85485a70f2175a205", "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "09879f7956dddc2a9328f5c1472feeb8402bcbcf"], "references": ["4dabd6182ce2681c758f654561d351739e8df7bf", "12a5b7190b981bf478b4c9c04d3c0d41f13b9023", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "e8e76b1062918624e9904e0073e11794d7594593", "6c5325c2b67bf88f2b846cf5a6df6c2e6362d75b", "5082a1a13daea5c7026706738f8528391a1e6d59", "3566c944bd71468d38872e67e441f493715233de", "b064b714107ffc724e0d477f4083e9e507c22fec", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "96526726f87233fb017f6ea9483090f04e0f0530", "b92aa7024b87f50737b372e5df31ef091ab54e62", "7df22e88a86d7e7e914a9cf3ad5b8fbd62b35cb8", "5b8364c21155d3d2cd38ea4c8b8580beba9a3250", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "ac973bbfd62a902d073a85ca621fd297e8660a82", "757f517f1952addc1716ea56f912f2e4a2803f7a", "1956c239b3552e030db1b78951f64781101125ed", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "0b544dfe355a5070b60986319a3f51fb45d1348e", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "067e07b725ab012c80aa2f87857f6791c1407f6d", "53ca064b9f1b92951c1997e90b776e95b0880e52", "71480da09af638260801af1db8eff6acb4e1122f", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "84069287da0a6b488b8c933f3cb5be759cb6237e", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "d1275b2a2ab53013310e759e5c6878b96df643d4", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "a17745f1d7045636577bcd5d513620df5860e9e5", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "77dfe038a9bdab27c4505444931eaa976e9ec667", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025", "9819b600a828a57e1cde047bbe710d3446b30da5", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "699d5ab38deee78b1fd17cc8ad233c74196d16e9", "5eb1a272f9933a11d113cf63fe659e073942bce5", "2f83f6e1afadf0963153974968af6b8342775d82", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "6b388f0151ab37adb3d57738b8f52a3f943f86c8", "11540131eae85b2e11d53df7f1360eeb6476e7f4", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "26bc0449360d7016f684eafae5b5d2feded32041", "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6"], "url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa"}, "7345843e87c81e24e42264859b214d26042f8d51": {"id": "7345843e87c81e24e42264859b214d26042f8d51", "title": "Recurrent Neural Network Grammars", "authors": [{"authorId": "1745899", "name": "Chris Dyer", "paperCount": 253, "citationCount": 33766, "hIndex": 74}, {"authorId": "3376845", "name": "A. Kuncoro", "paperCount": 24, "citationCount": 1777, "hIndex": 14}, {"authorId": "143668305", "name": "Miguel Ballesteros", "paperCount": 101, "citationCount": 7651, "hIndex": 28}, {"authorId": "144365875", "name": "Noah A. Smith", "paperCount": 342, "citationCount": 30604, "hIndex": 84}], "abstract": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 56, "citation_count": 452, "influential_paper_citations": 95, "is_open_access": true, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "fd17bd9a5dc24a081ad9743570f50dd6750f54b2", "3aa52436575cf6768a0a1a476601825f6a62e58f", "222928303a72d1389b0add8032a31abccbba41b3", "15d6f3d815d0ff176fafb14a3f46e5723ebac723", "f32f16ca3c27ff945198c6551a5d35fae3b1a660", "928f9dccb806a3278d20d82cc53781c5f44e2bb1", "593e4e749bd2dbcaf8dc25298d830b41d435e435", "472a5227279b45f25508017816af34e3cb3ac0d7", "7ea59779ffb392f099d5304680126b4299f43750"], "references": ["36c097a225a95735271960e2b63a2cb9e98bff83", "94960fcdf0ea3b346fca77ae8c63ae7943eb0d28", "4d4b46e545e1a3f6871b49cc69640ef2eb1a4654", "5ba79e88b37a084026ddc9ed2875a9dbe156aed4", "d8ac015407cf68c695043b23d905cddd880e5844", "39af464a4390d442fe3f68b9933e6b1343b21227", "b36b7f7c68923d14ba2859b5d28a1124616a8c89", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "0b544dfe355a5070b60986319a3f51fb45d1348e", "9c8affd69f19c1e3fed565d2b07aa98784b106a1", "5b2537973805684b65da2fa203af6d29220fe136", "58f0d54f2f3f1842c2b8687fc8eb7f8f4c5184e4", "687bac2d3320083eb4530bf18bb8f8f721477600", "174bbdb96252454cbb40a9c4e53335996235a008", "acc4e56c44771ebf69302a06af51498aeb0a6ac8", "2b920fe2d038571693bd96dafd3ed0dbadc4cb67", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "90cad27cd7bbe5180a23ef2184e984b9e2a86f19", "f24fb33e739b0181fc171b515f957190e3bb6430", "305930bab3aa329d01c990494a494cbc1b7db3be", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "9762f5d2ca8c2132942d10ef6b064bf370262585", "9819b600a828a57e1cde047bbe710d3446b30da5", "5bfd8d40bc071fffaf93685a46974b122ee4239d", "1cd44dd4f1b0b1c0499b1dc9637010c6d63d445c", "6e1b12dd7ca3443ba6e0fb38dfa3587a4eb5d539", "d01737b617acc555153f4660417908bf3971b1a5", "78a9513e70f596077179101f6cb6eadc51602039", "a350dc635ea3d0fd6dc8c0224fd8d6b55cc03271", "ffea7f0fd89dc940107cdf94f7decfcc42315c67", "2c72257ae7a4a32dc60569f4e1fe4504b2678112", "1174297ddcf08937a94d8efe4c1efb65f3b92fd8", "1a9c68f68217be7e5608780596fa5744a404a2ca", "7bd0a04b0f0687896e765b48619a27ee545e86db", "eb42a490cf4f186d3383c92963817d100afd81e2", "2ae8397c07bd3c76f84c7cdac7897e8b7dec9029", "a9d59174cc50b119ee4be19b3e65177431e37003", "e30d29fdf23e14623a2024d4fe0f7f3d5dc889d3", "4af41f4d838daa7ca6995aeb4918b61989d1ed80", "435245be302b3dc8ed244b1e6b2dba0b92baacf8", "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "5968067c696653fec60325633fb676f9134bd1db", "a879da675c6133c23beb96c110c351bb4f9154dc", "a49498e51840165d55b6badd4b52e34d17860bc0", "3de5d40b60742e3dfa86b19e7f660962298492af", "b23944bc96f26c1bec20312aea07b0acd3eb41f5", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b", "6c79a9bb8f885050cad70b4c69e016b186ffa538"], "url": "https://www.semanticscholar.org/paper/7345843e87c81e24e42264859b214d26042f8d51"}, "13fe71da009484f240c46f14d9330e932f8de210": {"id": "13fe71da009484f240c46f14d9330e932f8de210", "title": "Long Short-Term Memory-Networks for Machine Reading", "authors": [{"authorId": "1941442", "name": "Jianpeng Cheng", "paperCount": 22, "citationCount": 1955, "hIndex": 12}, {"authorId": "145307652", "name": "Li Dong", "paperCount": 87, "citationCount": 8769, "hIndex": 35}, {"authorId": "1747893", "name": "Mirella Lapata", "paperCount": 320, "citationCount": 22747, "hIndex": 80}], "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 74, "citation_count": 837, "influential_paper_citations": 63, "is_open_access": true, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "33998aff64ce51df8dee45989cdca4b6b1329ec4", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "83e7654d545fbbaaf2328df365a781fb67b841b4", "032274e57f7d8b456bd255fe76b909b2c1d7458e", "204a4a70428f3938d2c538a4d74c7ae0416306d8", "2cd8e8f510c89c7c18268e8ad51c061e459ad321"], "references": ["f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "36c097a225a95735271960e2b63a2cb9e98bff83", "75ddc7ee15be14013a3462c01b38b0548486fbcb", "889e57259a1d6017701fb2c2ceece82f9f4eff4c", "4a197ce36461849bcaee565b510a8ef71b7dcae3", "596c882de006e4bb4a93f1fa08a5dd467bee060a", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "452059171226626718eb677358836328f884298e", "5082a1a13daea5c7026706738f8528391a1e6d59", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "a79ac27b270772c79b80d2235ca5ff2df2d2d370", "b777a55505ee2ffb4f8f9ada916e4e4a5f13a4ed", "a739ae988ba0e3ff232f4507627dfc282ba7b3f4", "17de95a8ec3fe5917d91110b410ab64df33414bf", "d1505c6123c102e53eb19dff312cb25cea840b72", "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "b36b7f7c68923d14ba2859b5d28a1124616a8c89", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "d14c7e5f5cace4c925abc74c88baa474e9f31a28", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "71ae756c75ac89e2d731c9c79649562b5768ff39", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "822f1ed9a76a57cc19d8fda7745365b97130b97a", "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "0b544dfe355a5070b60986319a3f51fb45d1348e", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "c5b31a837ca6dd26b63d814659234cac5e35b5ee", "687bac2d3320083eb4530bf18bb8f8f721477600", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "84069287da0a6b488b8c933f3cb5be759cb6237e", "e14a4b1b4d49b3af54a399d71c4b6f97c5e3aa27", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "1f4a4769e4d2fb846e59c2f185e0377190739f18", "d4b651d6a904f69f8fa1dcad4ebe972296af3a9a", "906901b15c93d0cbfdf6c9b6587c6a1b389ec386", "77d2698e8efadda698b0edb457cd8de75224bfa0", "2b49b8e6f1ecc30198c9db9a8612114ec2045a16", "c077b3791a7ea01ef487f48ebac943af2e8e79c4", "f7312b8568d63bbbb239583ed282f46cdc40978d", "db328685d00ec35fe35f9350f884c7b4b8db3f4c", "9819b600a828a57e1cde047bbe710d3446b30da5", "4b9bdeb0d7225548f5b0319c8bf3878ffeb2d696", "f9cbf54dd6b1adc699328b555c4bc03b42ce5851", "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "ca2858b2040724ae9f29ba601df12aae2e539596", "50449f7d4810c79cc3f89a161df2244420a5b48f", "b4fd3bad83d0e64c5679dfa00b037db98342eec3", "87c8a7be8d5e2e2209e766c3e28a3e8ee5babb64", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "765491c9cfc746275000297fc0ae919b5c4440e0", "a49498e51840165d55b6badd4b52e34d17860bc0", "d0be39ee052d246ae99c082a565aba25b811be2d", "30110856f45fde473f1903f686aa365cf70ed4c7", "f4330cf49857d6a4aed2965d43bc1c5ef2328a3b", "3f3d13e95c25a8f6a753e38dfce88885097cbd43", "62f4d33e21cd808bc481ea0bc5c56d1935bfedc4", "687b413ddccafd37379b5d7f66764091133f1b89"], "url": "https://www.semanticscholar.org/paper/13fe71da009484f240c46f14d9330e932f8de210"}, "2c03df8b48bf3fa39054345bafabfeff15bfd11d": {"id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition", "authors": [{"authorId": "39353098", "name": "Kaiming He", "paperCount": 86, "citationCount": 286664, "hIndex": 65}, {"authorId": "1771551", "name": "X. Zhang", "paperCount": 673, "citationCount": 158139, "hIndex": 63}, {"authorId": "3080683", "name": "Shaoqing Ren", "paperCount": 18, "citationCount": 183730, "hIndex": 13}, {"authorId": null, "name": "Jian Sun", "paperCount": null, "citationCount": null, "hIndex": null}], "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 54, "citation_count": 114972, "influential_paper_citations": 25403, "is_open_access": true, "citations": ["70d3f037dd66f0126b6acda066276f66e204f76b", "370b680057a6e324e67576a6bf1bf580af9fdd74", "2435ffb8ed3212156d6b6f19f633a861399cf30e", "6901507cbaaa2fca80ac1ced76c40217a6850568", "89f86a915245043e1ecd7a3c462945fdde171e7b", "9112ebb8b8aff412751bc0f3ce03aba82b903e36", "44732fe24025c2f17777028bb49bb4a09c31af2a", "65e40b0c9327b9e1cfe30cfec61a8708b334b8eb", "18b0ca3448bd9703bff86c375362b6ad34f797f7", "868d14ba0e41c2f912bd4548ebee22c172aa78e4"], "references": ["f075f89b4f4026748cbf2fb9f989a9934c42ee8f", "58b09127761bb83f4761de108eb4d88e92b4f451", "b92aa7024b87f50737b372e5df31ef091ab54e62", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "e3ea1c7a3c6cc15590ab5e62a171536fe01b9b1c", "e0945081b5b87187a53d4329cf77cd8bff635795", "7ffdbc358b63378f07311e883dddacc9faeeaf4b", "4d376d6978dad0374edfa6709c9556b42d3594d3", "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "cd85a549add0c7c7def36aca29837efd24b24080", "8ad35df17ae4064dd174690efb04d347428f1117", "317aee7fc081f2b137a85c4f20129007fd8e717e", "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "cbb19236820a96038d000dc629225d36e0b6294a", "6bdb186ec4726e00a8051119636d4df3b94043b5", "71b7178df5d2b112d07e45038cb5637208659ff7", "b034b5769ab94acf9fb8ae48c7edb560a300bb63", "1109b663453e78a59e4f66446d71720ac58cec25", "99c970348b8f70ce23d6641e201904ea49266b6e", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "fc26b9c1afe81e1b20195123fe6f3ced9520abb6", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "f72c079d9179cfaada1135a7e4c77d48b6309a30", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5183230b706b72f6f6c19415c423d93c79ddde53", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14", "bc6dff14a130c57a91d5a21339c23471faf1d46f", "b87274e6d9aa4e6ba5148898aa92941617d2b6ed", "7b7908f71188b89adf62ce9126a0466e1a34338f", "4748d22348e72e6e06c2476486afddbc76e5eca7", "d720a95e1501922ea17ee31f299f43b2db5e15ef", "a538b05ebb01a40323997629e171c91aa28b8e2f", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "82635fb63640ae95f90ee9bdc07832eb461ca881", "5d90f06bb70a0a3dced62413346235c02b1aa086", "23694b6d61668e62bb11f17c1d75dde3b4951948", "4f04da90218f8ddfa3a758188edade8c7bd95ac1", "1bd875676fe49f83d431500cea908da1bdf068da", "5763bd6b3f24a01c3bc7cd15d3c916b4840b759d", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "1f711685bfe9e67d6afe0d5a3cb6675c310237d6", "75a026ddfdd9c219d69fe8af816f085ea1b3877d", "877a887e7af7daebcb685e4d7b5e80f764035581", "d0be39ee052d246ae99c082a565aba25b811be2d", "dbc0a468ab103ae29717703d4aa9f682f6a2b664", "3f3d13e95c25a8f6a753e38dfce88885097cbd43", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "1efc5a54a4b3f4675bee194ee5842978e45a5bc2", "8a2384ff41dc2c337e6aaacbcfd13f74e0e2f1ea"], "url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d"}, "23ffaa0fe06eae05817f527a47ac3291077f9e58": {"id": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision", "authors": [{"authorId": "2574060", "name": "Christian Szegedy", "paperCount": 48, "citationCount": 140382, "hIndex": 23}, {"authorId": "2657155", "name": "Vincent Vanhoucke", "paperCount": 53, "citationCount": 85691, "hIndex": 26}, {"authorId": "2054165706", "name": "Sergey Ioffe", "paperCount": 30, "citationCount": 63492, "hIndex": 19}, {"authorId": "1789737", "name": "Jonathon Shlens", "paperCount": 108, "citationCount": 67183, "hIndex": 50}, {"authorId": "3282833", "name": "Z. Wojna", "paperCount": 19, "citationCount": 21225, "hIndex": 9}], "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 25, "citation_count": 18281, "influential_paper_citations": 2689, "is_open_access": true, "citations": ["2435ffb8ed3212156d6b6f19f633a861399cf30e", "a57d47b762341340656d5b5caa84f370d9d31063", "09df3792e5d516bbc64240a92364e85b50fd3301", "44732fe24025c2f17777028bb49bb4a09c31af2a", "77e8c2d78638056cd7a347d7c6e36406ca176dbd", "a815b0a955db2163617baf308020c3d770da099d", "2d01d191956d81c950221db4323004687f037b32", "8baa03bc0a7c3aa2716dfb12d2ccccdba758815c", "e7559868430aa50748d38c4c9edaeeae6907a57f", "aff9fe07dbe63a85cf520470e61360f9aec4ea8d"], "references": ["d5eadd6f059d742d76441fd0a635a21694dd7392", "382bebe4df1ce90b2093f2167fee6da4d3c5dfed", "efb5032e6199c80f83309fd866b25be9545831fd", "5aa26299435bdf7db874ef1640a6c3b5a4a2c394", "4d376d6978dad0374edfa6709c9556b42d3594d3", "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "317aee7fc081f2b137a85c4f20129007fd8e717e", "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "0504945cc2d03550fecb6ff02e637f9421107c25", "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "2a002ce457f7ab3088fbd2691734f1ce79f750c4", "67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6", "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "b2180fc4f5cb46b5b5394487842399c501381d67", "aa7bfd2304201afbb19971ebde87b17e40242e91", "84069287da0a6b488b8c933f3cb5be759cb6237e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "8f9f992fddfccd5cd85d3254c0b5f54334d7fa63"], "url": "https://www.semanticscholar.org/paper/23ffaa0fe06eae05817f527a47ac3291077f9e58"}, "5e4eb58d5b47ac1c73f4cf189497170e75ae6237": {"id": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "title": "Neural GPUs Learn Algorithms", "authors": [{"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}], "abstract": "Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. \nWe present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. \nAn essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. \nTo achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 48, "citation_count": 313, "influential_paper_citations": 34, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "8a53f2344de0620ed26861a350c1d109fcbc53de", "0936352b78a52bc5d2b5e3f04233efc56664af51", "21c99706bb26e9012bfb4d8d48009a3d45af59b2", "452059171226626718eb677358836328f884298e", "9c5c89199114858eafbe50b46d77d38ffd03b28a", "2e17cf6a339fd071ad222062f868e882ef4120a4", "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "8a25c9403d8a0e2fb8ca362a1b26262afd57417f", "50295c19e177480ba3599300de1ab837cc62b08c"], "references": ["bcd857d75841aa3e92cd4284a8818aba9f6c0c3f", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "4e8930ae948262a89acf2e43c8e8b6e902c312c4", "d5eadd6f059d742d76441fd0a635a21694dd7392", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f", "dc555e8156c956f823587ebbff018863e6d2a95e", "f9c990b1b5724e50e5632b94fdb7484ece8a6ce7", "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "f10e071292d593fef939e6ef4a59baf0bb3a6c2b", "e0945081b5b87187a53d4329cf77cd8bff635795", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "c3823aacea60bc1f2cabb9283144690a3d015db5", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "c0b624c46b51920dfec5aa02cc86323c0beb0df5", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "2a84e524a859450d0816e22ea6b981d448a7b5cc", "6658bbf68995731b2083195054ff45b4eca38b3a", "aeed631d6a84100b5e9a021ec1914095c66de415", "18b8ef71bc01b8658b4ef2c8b9a9e4e6e5c2a07b", "72d531724d9305263f2c844daa80e5581fb2d36b", "a19a5e29b3eaf0223d0611451ce17b1a07c89aae", "ed9215b146bfd3a1db72a23fe5de8e2a3ae4c6c8", "4b030084ce9df0666fe7c63222039a0dc7aa4be5", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "225331d1700a9544545cc7c54a63c1b485269ce7"], "url": "https://www.semanticscholar.org/paper/5e4eb58d5b47ac1c73f4cf189497170e75ae6237"}, "d76c07211479e233f7c6a6f32d5346c983c5598f": {"id": "d76c07211479e233f7c6a6f32d5346c983c5598f", "title": "Multi-task Sequence to Sequence Learning", "authors": [{"authorId": "1707242", "name": "Minh-Thang Luong", "paperCount": 41, "citationCount": 9690, "hIndex": 24}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}, {"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}, {"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}], "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 34, "citation_count": 717, "influential_paper_citations": 72, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "a486e2839291111bb44fa1f07731ada123539f75", "75acc731bdd2b626edc74672a30da3bc51010ae8", "a33a06ddc762fb855b6954c08d5aca603080b011", "9151f229e7b4e318b0b12afe99993da0ee5e0e34", "2b0d7e51efd004fe3847f54863540c79312f3546", "c3a3c163f25b9181f1fb7e71a32482a7393d2088"], "references": ["bcd857d75841aa3e92cd4284a8818aba9f6c0c3f", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "25eb839f39507fe6983ad3e692b2f8d93a5cb0cc", "feb420a4ac7c5719d51480053cd3e8669d5f2062", "93499a7c7f699b6630a86fad964536f9423bb6d0", "83cf4b2f39bcc802b09fd59b69e23068447b26b7", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "c3b8367a80181e28c95630b9b63060d895de08ff", "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "1956c239b3552e030db1b78951f64781101125ed", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "2826f9dccdcceb113b33ccf2841d488f1419bb30", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "c0b624c46b51920dfec5aa02cc86323c0beb0df5", "b8de958fead0d8a9619b55c7299df3257c624a96", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "a41b826d23957d6ad4e9e794d20a583a9b567c5d", "0a7c4cec908ca18f76f5101578a2496a2dceb5e7", "d3c04a424fff21d3d12ff8b0543734cf244d5f67", "dbb3342599c9b431a3152a0d5c813d3e56967a27", "944e1a7b2c5c62e952418d7684e3cade89c76f87", "e219a61354d972a28954e655a7c53373508a08b6", "161ffb54a3fdf0715b198bb57bd22f910242eb49", "d7da009f457917aa381619facfa5ffae9329a6e9", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "371c9dc680e916f79d9c78fcf6c894a2dd299095", "0b44fcbeea9415d400c5f5789d6b892b6f98daff"], "url": "https://www.semanticscholar.org/paper/d76c07211479e233f7c6a6f32d5346c983c5598f"}, "1af68821518f03568f913ab03fc02080247a27ff": {"id": "1af68821518f03568f913ab03fc02080247a27ff", "title": "Neural Machine Translation of Rare Words with Subword Units", "authors": [{"authorId": "2082372", "name": "Rico Sennrich", "paperCount": 145, "citationCount": 14775, "hIndex": 41}, {"authorId": "2259100", "name": "B. Haddow", "paperCount": 149, "citationCount": 14693, "hIndex": 39}, {"authorId": "2539211", "name": "Alexandra Birch", "paperCount": 100, "citationCount": 17238, "hIndex": 26}], "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 41, "citation_count": 5386, "influential_paper_citations": 940, "is_open_access": true, "citations": ["722ad6ac92286507437b31486f47987d6ece05c9", "54523ff961a1ac57a86696ef9a53b3a630b482c0", "b896b846ae180d804c7290d8b9ae9ffc55325866", "8f2bca9d684005675e294b33c26481e36f528cdb", "96ea07447d2f9adefe03852a878517a2a6d45b96", "13a0d8bb38f739990c8cd65a44061c6534f17221", "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "9933a5af7895354087baf6c96b64dc8a8973eaed", "1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe"], "references": ["891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "8c3cf30db12d17638b01e0e464e09d6b58a88187", "ff1577528a34a11c2a81d2451d346c412c674c02", "93a9694b6a4149e815c30a360347593b75860761", "25eb839f39507fe6983ad3e692b2f8d93a5cb0cc", "2da338d8972e473df62a566290c9de95a52209e5", "d8c5e6adf7023def3be0bee91799e18607cf588f", "285c165c81fc9275955147a892b9a039ec8b1052", "726244c312dc1145e9e9ee32ce641ab8dd9c6e74", "93499a7c7f699b6630a86fad964536f9423bb6d0", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "1956c239b3552e030db1b78951f64781101125ed", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "46f418bf6fab132f193661226c5c27d67f870ea5", "fa144b01862baa5de61d22fd3f922a3ddd54ac4d", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "53ab89807caead278d3deb7b6a4180b277d3cb77", "7b5e31257f01aba987f16e175a3e49e00a5bd3bb", "db734a0e1dc65fe3fe2eef474aefba6d083f54dd", "84069287da0a6b488b8c933f3cb5be759cb6237e", "8729441d734782c3ed532a7d2d9611b438c0a09a", "88b66f705a329da8292e7b8aa4bfe26de4759cfa", "5ab5cc1c135a1af68fdea604474b70f4121db623", "b0b3c2e5e924621b234a24037fa4f4410b478b49", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "71088c81d1fb157844c61ac24fb1dd2a70d0e59f", "36ffcc1cc218ca36de384a107fb48e5abe2e6359", "e9ad27106dd487893bcc1cc12bbf645168c60f87", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "dca029eafe302034f0e7784b9266403938c55263", "cdaae7a8f0db8b280266606004f1c6f164a13f6d", "0c5043108eda7d2fa467fe91e3c47d4ba08e0b48", "ec5f929b57cf12b4d624ab125f337c14ad642ab1", "566eb7be43b8a2b2daff82b03711098a84859b2a", "3dffe5ebf00f10dd137beff00d94952f1af658c3", "1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8", "6c0f01c67f43e35859025d3424e0268b4d1ee2f1"], "url": "https://www.semanticscholar.org/paper/1af68821518f03568f913ab03fc02080247a27ff"}, "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e": {"id": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "title": "End-To-End Memory Networks", "authors": [{"authorId": "2265067", "name": "Sainbayar Sukhbaatar", "paperCount": 30, "citationCount": 4950, "hIndex": 16}, {"authorId": "3149531", "name": "Arthur D. Szlam", "paperCount": 121, "citationCount": 17372, "hIndex": 44}, {"authorId": "145183709", "name": "J. Weston", "paperCount": 249, "citationCount": 75937, "hIndex": 97}, {"authorId": "2276554", "name": "R. Fergus", "paperCount": 130, "citationCount": 84678, "hIndex": 70}], "abstract": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 27, "citation_count": 2147, "influential_paper_citations": 249, "is_open_access": false, "citations": ["832fff14d2ed50eb7969c4c4b976c35776548f56", "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "1f58e8d4c827037d4c2a1afc695a88704e088beb", "18f207d8dab7357f4f674211ec4f150de1c93a0e", "dd20f00b0121075d2373e60025d59d9bddf89ea1", "19adf1af8daa9551328226fc6c0140e955bf5689", "9c7a455b1e48d01a99e58e884a6c1acb75074ad0", "202c79bbb45ab6524141feacc81caacc4ba00401", "bf658a0ffd83b283656a38b25c99a7edd99020cd", "70557ea6b65846fc30729ceed224acd4ac64ca5d"], "references": ["abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "4d05ab884a6c1645b80ce5d02b09c7e5ff499790", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "71ae756c75ac89e2d731c9c79649562b5768ff39", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "c3823aacea60bc1f2cabb9283144690a3d015db5", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "f9a1b3850dfd837793743565a8af95973d395a4e", "b185742930fd959aaccdfdecdb31641839a787c4", "09c76da2361d46689825c4efc37ad862347ca577", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "3ffd1ae2362c3d74f2cfc0f6ad7c5d1954e3d46f", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "ef873b940a5acfeb45796fb6d98163300f8903e6", "30110856f45fde473f1903f686aa365cf70ed4c7", "6ca3455b28e165ab5c772bdb6048c312d06d1d1e", "ceb8a7e9342360801300e7dd753f79bd4ee2e5c6", "6433ab51853564593af4dd080b02bf9d8fb27e68"], "url": "https://www.semanticscholar.org/paper/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e"}, "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40": {"id": "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "title": "Grammar as a Foreign Language", "authors": [{"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "2060101052", "name": "Terry Koo", "paperCount": 13, "citationCount": 3195, "hIndex": 11}, {"authorId": "1754497", "name": "Slav Petrov", "paperCount": 86, "citationCount": 12214, "hIndex": 40}, {"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}, {"authorId": "1695689", "name": "Geoffrey E. Hinton", "paperCount": 398, "citationCount": 383672, "hIndex": 147}], "abstract": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 36, "citation_count": 872, "influential_paper_citations": 99, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "46200b99c40e8586c8a0f588488ab6414119fb28", "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "e9a986c8ff6c2f381d026fe014f6aaa865f34da7", "455afd748e8834ef521e4b67c7c056d3c33429e2", "651e5bcc14f14605a879303e97572a27ea8c7956", "a072c2a400f62f720b68dc54a662fb1ae115bf06", "9653d5c2c7844347343d073bbedd96e05d52f69b", "1db6e3078597386ac4222ba6c3f4f61b61f53539"], "references": ["94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "1956c239b3552e030db1b78951f64781101125ed", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "47d2dc34e1d02a8109f5c04bb6939725de23716d", "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a", "cea967b59209c6be22829699f05b8b1ac4dc092d", "016842482d3d733d8b999f6b9735a15906a12b53", "1e9febd1a88c44843e5f865c3b679174688b1d27", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "174bbdb96252454cbb40a9c4e53335996235a008", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "a0bf3a18ea98f259c029827a7c1f7d5822642db8", "9c0ddf74f87d154db88d79c640578c1610451eec", "d6d931fe9bfe7223165b14ec85d79d56e406e38a", "e441126a8dd0cb8363272b7b54207ae92e155bc0", "9dccaf6ea0fa19772cf8067295b16df3eb7b4dda", "266185b76db9143a4f1d695c8f1fd5cc48a2c829", "5bfd8d40bc071fffaf93685a46974b122ee4239d", "053f1cf10ced2321c1853f307075f0a6a83b6840", "2fd9983d42eba7f25f27d437df8c5d8bdb2b778e", "f52de7242e574b70410ca6fb70b79c811919fc00", "956d4b97f5814dd9e53abf286cc8cfe1283a01eb", "78a9513e70f596077179101f6cb6eadc51602039", "e54d8b07ef659f9ee2671441c4355e414e408836", "1174297ddcf08937a94d8efe4c1efb65f3b92fd8", "41828fc3dab24784f95e6976e8aaa73f68e1840e", "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "2ae8397c07bd3c76f84c7cdac7897e8b7dec9029", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "0ffa423a5283396c88ff3d4033d541796bd039cc", "54c846ee00c6132d70429cc279e8577f63ed05e4", "0b44fcbeea9415d400c5f5789d6b892b6f98daff"], "url": "https://www.semanticscholar.org/paper/47570e7f63e296f224a0e7f9a0d08b0de3cbaf40"}, "a6cb366736791bcccc5c8639de5a8f9636bf87e8": {"id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization", "authors": [{"authorId": "1726807", "name": "Diederik P. Kingma", "paperCount": 33, "citationCount": 136875, "hIndex": 27}, {"authorId": "2503659", "name": "Jimmy Ba", "paperCount": 80, "citationCount": 125590, "hIndex": 32}], "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 29, "citation_count": 105505, "influential_paper_citations": 18096, "is_open_access": false, "citations": ["6901507cbaaa2fca80ac1ced76c40217a6850568", "719b251a5ddc83d4ec35e939c908b0ca3dcae194", "9112ebb8b8aff412751bc0f3ce03aba82b903e36", "44732fe24025c2f17777028bb49bb4a09c31af2a", "264d4b803e93d933eaa6691836609522b38ba03e", "ec59f787848a17862321a934f1a077f847142d41", "43e08ca5d63fb3517d33ae42373ddf8f35758cdf", "868d14ba0e41c2f912bd4548ebee22c172aa78e4", "e13d7450dfbfe28d2954207b61d6d7e3d4409ef7", "2f10d9d3aeaf7449b9dd9b2616d4653a4f6e73a7"], "references": ["769ef3d5021cd71c37d2c403f231a53d1accf786", "981ce6b655cc06416ff6bf7fac8c6c2076fd7fac", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "af0ee019dcc1fe7eab918e3c670a6c47e48d17f6", "e8f95ccfd13689f672c39dca3eccf1c484533bcc", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "ec92efde21707ddf4b81f301cd58e2051c1a2443", "aa7bfd2304201afbb19971ebde87b17e40242e91", "6bdccfe195bc49d218acc5be750aa49e41f408e4", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "e5a685f40338f9c2f3e68e142efa217aad16dd56", "8729441d734782c3ed532a7d2d9611b438c0a09a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "31868290adf1c000c611dfc966b514d5a34e8d23", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "7658cecad68afc970f18cadbf6390439b17def87", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "413c1142de9d91804d6d11c67ff3fed59c9fc279", "f7cc843c318d8862357485488971b26527ef1a8e", "c50dca78e97e335d362d6b991ae0e1448914e9a3", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "e1f153c6df86d1ca8ecb9561daddfe7a54f901e7", "735d4220d5579cc6afe956d9f6ea501a96ae99e2", "5a767a341364de1f75bea85e0b12ba7d3586a461", "6dc61f37ecc552413606d8c89ffbc46ec98ed887", "2991f9bb677b71c33945e89ac0c7dcf7a36fa198"], "url": "https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8"}, "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5": {"id": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "authors": [{"authorId": "3335364", "name": "Dzmitry Bahdanau", "paperCount": 47, "citationCount": 50418, "hIndex": 23}, {"authorId": "1979489", "name": "Kyunghyun Cho", "paperCount": 326, "citationCount": 88663, "hIndex": 80}, {"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}], "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 32, "citation_count": 21448, "influential_paper_citations": 2708, "is_open_access": false, "citations": ["264d4b803e93d933eaa6691836609522b38ba03e", "e3bb2f3d9296badad67daf7a00cf4fa3ada0f814", "26133033149afb4b45e5d0a4bd1dc712a236810e", "1c2e1c73e3f43688e4f22d29ae5a6463721690f9", "1c83f3f9789df43bf937ae2618721e2da83dcc06", "cb1945f48906b4b618f645198c11ae3dcaa1836a", "9961a5ff509943045103dac06b7d15432e26d25f", "574bcb4aba88cd7a296d584a5bcb99bd769705d8", "4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c", "2198ec31e64e7ff62e14ac18651cd404aae789ce"], "references": ["cea967b59209c6be22829699f05b8b1ac4dc092d", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "6122c95ac6475e965bf4e120f7a588d29bb00ecc", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0894b06cff1cd0903574acaa7fcf071b144ae775", "533ee188324b833e059cb59b654e6160776d5812", "c20ed3a1600122e6cf03b8ed74d3d2920ad0a8c6", "1149888d75af4ed5dffc25731b875651c3ccdeb2", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "84069287da0a6b488b8c933f3cb5be759cb6237e", "e27d81521dc4e8b6ea93947c05ffccf06784f569", "8729441d734782c3ed532a7d2d9611b438c0a09a", "5f08df805f14baa826dbddcb002277b15d3f1556", "855d0f722d75cc56a66a00ede18ace96bafee6bd", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "396aabd694da04cdb846cb724ca9f866f345cbd5", "b3e89f05876d47b9bd6ece225aaeee457a6824e8", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "9d6c43ad7f4cf7ebcd4245510df2880f3d3b0964", "73c2a58c936ba2d269491548ef32644c5e982199", "d0be39ee052d246ae99c082a565aba25b811be2d", "3f3d13e95c25a8f6a753e38dfce88885097cbd43"], "url": "https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"}, "adfcf065e15fd3bc9badf6145034c84dfb08f204": {"id": "adfcf065e15fd3bc9badf6145034c84dfb08f204", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "authors": [{"authorId": "8270717", "name": "Junyoung Chung", "paperCount": 20, "citationCount": 13295, "hIndex": 11}, {"authorId": "1854385", "name": "\u00c7aglar G\u00fcl\u00e7ehre", "paperCount": 38, "citationCount": 38103, "hIndex": 30}, {"authorId": "1979489", "name": "Kyunghyun Cho", "paperCount": 326, "citationCount": 88663, "hIndex": 80}, {"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}], "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 20, "citation_count": 8490, "influential_paper_citations": 1504, "is_open_access": false, "citations": ["c435ecd0321dcec1f25e458bf930311f9e1d04b6", "6623be9157da6019b75827bd0a29987b69a43c5a", "e64914f9d4d2eabf3fa1352e2d132364a5539aaa", "7843af5b0ebe7b30fdcea90fb6822f8c9efa6f83", "44d1b81911e35e2aa2c03a5347b88ae479602837", "a96b482cab45823bc1a095baecb5d5b05ffd9284", "9364677436053cf8e86688eb89e0a97565592eab", "cd471b5ef162906ef3d9a84398b3f98e9ee4bf56", "0ac9e357f2599dc4f66bf74f511c717fdc9e3a20", "add12473900be92c3ff36d07585011ec33e0a736"], "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5df0a0e9ceec70a9321b0555288222bf53216342", "cea967b59209c6be22829699f05b8b1ac4dc092d", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "1b3aab4ff8f77b81501f271877321609cc1a1a2b", "836acf6fc99ebf81d219e2b67f7ab25efc29a6a4", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "ded103d0613e1a8f51f586cc1678aee3ff26e811", "84069287da0a6b488b8c933f3cb5be759cb6237e", "855d0f722d75cc56a66a00ede18ace96bafee6bd", "18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8", "188e247506ad992b8bc62d6c74789e89891a984f", "5a9ef216bf11f222438fff130c778267d39a9564", "0d6203718c15f137fda2f295c96269bc2b254644", "a97b5db17acc731ef67321832dbbaf5766153135", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "d0be39ee052d246ae99c082a565aba25b811be2d", "3f3d13e95c25a8f6a753e38dfce88885097cbd43"], "url": "https://www.semanticscholar.org/paper/adfcf065e15fd3bc9badf6145034c84dfb08f204"}, "cea967b59209c6be22829699f05b8b1ac4dc092d": {"id": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks", "authors": [{"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}, {"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}], "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 38, "citation_count": 16249, "influential_paper_citations": 1452, "is_open_access": false, "citations": ["f3a1cd0baf0ffffe0c269feb15f4cc3b159a0613", "89f86a915245043e1ecd7a3c462945fdde171e7b", "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "372f60a267239c285f5f0bdf3d6605843787108c", "055cd2faeebc7a9df43923d554a61ae924a4af6b", "618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7", "574bcb4aba88cd7a296d584a5bcb99bd769705d8", "aff7e2bbc89ad8294956688a724dc29a603a724d", "66de7ac94dd13ec041659b06041c6ff27b1e55f2", "b955b76c0e66cae466bf9a710ea85c2ad77f0d3a"], "references": ["fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "6122c95ac6475e965bf4e120f7a588d29bb00ecc", "0b544dfe355a5070b60986319a3f51fb45d1348e", "97cedf99252026f58e8154bc61d49cf885d42030", "0894b06cff1cd0903574acaa7fcf071b144ae775", "9f2efadf66817f1b38f58b3f50c7c8f34c69d89a", "c20ed3a1600122e6cf03b8ed74d3d2920ad0a8c6", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "167ad306d84cca2455bc50eb833454de9f2dcd02", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "84069287da0a6b488b8c933f3cb5be759cb6237e", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "e33cbb25a8c7390aec6a398e36381f4f7770c283", "398c296d0cc7f9d180f84969f8937e6d3a413796", "6658bbf68995731b2083195054ff45b4eca38b3a", "3d2218b17e7898a222e5fc2079a3f1531990708f", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "f9a1b3850dfd837793743565a8af95973d395a4e", "77b73697c8b5295a718ffef0d63a1afd489de915", "1a4b6fa371f78a4bf209b8d4cdea328522f0f465", "9819b600a828a57e1cde047bbe710d3446b30da5", "96494e722f58705fa20302fe6179d483f52705b4", "d7da009f457917aa381619facfa5ffae9329a6e9", "aed054834e2c696807cc8b227ac7a4197196e211", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "162d958ff885f1462aeda91cd72582323fd6a1f4", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "b158a006bebb619e2ea7bf0a22c27d45c5d19004", "d0be39ee052d246ae99c082a565aba25b811be2d", "06342ba4480526574a8d864a3c3c1719450e2463", "3f3d13e95c25a8f6a753e38dfce88885097cbd43", "1a3d22599028a05669e884f3eaf19a342e190a87", "052b1d8ce63b07fec3de9dbb583772d860b7c769"], "url": "https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d"}, "0b544dfe355a5070b60986319a3f51fb45d1348e": {"id": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "authors": [{"authorId": "1979489", "name": "Kyunghyun Cho", "paperCount": 326, "citationCount": 88663, "hIndex": 80}, {"authorId": "3158246", "name": "Bart van Merrienboer", "paperCount": 14, "citationCount": 24278, "hIndex": 11}, {"authorId": "1854385", "name": "\u00c7aglar G\u00fcl\u00e7ehre", "paperCount": 38, "citationCount": 38103, "hIndex": 30}, {"authorId": "3335364", "name": "Dzmitry Bahdanau", "paperCount": 47, "citationCount": 50418, "hIndex": 23}, {"authorId": "2076086", "name": "Fethi Bougares", "paperCount": 54, "citationCount": 18503, "hIndex": 17}, {"authorId": "144518416", "name": "Holger Schwenk", "paperCount": 165, "citationCount": 27840, "hIndex": 46}, {"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}], "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 33, "citation_count": 16992, "influential_paper_citations": 3109, "is_open_access": true, "citations": ["15736f7c205d961c00378a938daffaacb5a0718d", "89f86a915245043e1ecd7a3c462945fdde171e7b", "6255cbb71a19d5cefd8e1f847922be9e5bb3a786", "c6d2d950b64973f1bec3b0dd0ed46fbf0319351d", "6783e2c405e94264246b73f99d526174efe48453", "688465f03f131c258a9242240c95253d4ea22953", "a282ee69e7eedb7e327ddb8c54582a62e8155fbf", "f899e1c18b0eaf4614d0e675023031e9b253c01b", "1982ec5b84cbf14d0e1c0d95581125a1b8c2bdc2", "8b49d43c4e1621af2a04faeb226f7fe214b1ef58"], "references": ["0894b06cff1cd0903574acaa7fcf071b144ae775", "70600593f870f460624c56c2a57b9a03b94f94a5", "533ee188324b833e059cb59b654e6160776d5812", "99c970348b8f70ce23d6641e201904ea49266b6e", "d49fc0b584012532e4fd7725149a29e25ac835bc", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "0d3233d858660aff451a6c2561a05378ed09725a", "71480da09af638260801af1db8eff6acb4e1122f", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "167ad306d84cca2455bc50eb833454de9f2dcd02", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "4aa77b42231b111a695aea35bfdbb9a82026daa3", "ded103d0613e1a8f51f586cc1678aee3ff26e811", "8729441d734782c3ed532a7d2d9611b438c0a09a", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "5f08df805f14baa826dbddcb002277b15d3f1556", "855d0f722d75cc56a66a00ede18ace96bafee6bd", "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "6658bbf68995731b2083195054ff45b4eca38b3a", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "396aabd694da04cdb846cb724ca9f866f345cbd5", "67107f78a84bdb2411053cb54e94fa226eea6d8e", "0482eeefa01c4cbf585f572c1fd4ed930ee222d2", "a97b5db17acc731ef67321832dbbaf5766153135", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "028a35d6835b2717c02a7acd5be1c71e0749df26", "694b3c58712deefb59502847ba1b52b192c413e5", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "32a9ba4a76d1e9948c1cb980800ad117531753f8", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e"}, "34f25a8704614163c4095b3ee2fc969b60de4698": {"id": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting", "authors": [{"authorId": "2897313", "name": "Nitish Srivastava", "paperCount": 43, "citationCount": 43762, "hIndex": 22}, {"authorId": "1695689", "name": "Geoffrey E. Hinton", "paperCount": 398, "citationCount": 383672, "hIndex": 147}, {"authorId": "2064160", "name": "A. Krizhevsky", "paperCount": 21, "citationCount": 154741, "hIndex": 16}, {"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}, {"authorId": "145124475", "name": "R. Salakhutdinov", "paperCount": 332, "citationCount": 123720, "hIndex": 101}], "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 39, "citation_count": 31105, "influential_paper_citations": 2544, "is_open_access": false, "citations": ["44732fe24025c2f17777028bb49bb4a09c31af2a", "264d4b803e93d933eaa6691836609522b38ba03e", "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "4084bbccc5418fad23a370d90c841a59fd5dc7d7", "c1ba680d6a8adaa4f19df6f730fda2b344924e8a", "aef9a4ebb7832361249c41a728aedfa8a1d796cc", "21118365dd2abf2cb4ab5f7e841bcf852f5a3b00", "2198ec31e64e7ff62e14ac18651cd404aae789ce", "e031687d491994604dcdb4fd663ac646433a9b7e", "e7559868430aa50748d38c4c9edaeeae6907a57f"], "references": ["d124a098cdc6f99b9a152fcf8afa9327dac583be", "3c20df69865df6a627cc45c524869ccc0297048f", "ec92efde21707ddf4b81f301cd58e2051c1a2443", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "0abb49fe138e8fb7332c26b148a48d0db39724fc", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "8db26a22942404bd435909a16bb3a50cd67b4318", "2e2089ae76fe914706e6fa90081a79c8fe01611e", "9f7f9aba0a6a966ce04e29e401ea28f9eae82f02", "d2b62f77cb2864e465aa60bca6c26bb1d2f84963", "0805faaedd3d8a693fca38cf0a5e0d12d8ffc65e", "eefcc7bcc05436dac9881acb4ff4e4a0b730e175", "02227c94dd41fe0b439e050d377b0beb5d427cda", "3a1a2cff2b70fb84a7ca7d97f8adcc5855851795", "90b63e917d5737b06357d50aa729619e933d9614", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "116b678d7a155845956de2204f6ee5151f8dd98b", "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "85c1c2d3a7bddfc3f276db479c60b3ec246bdf24", "0ea90fac0958d84bcf4a2875c2b169478358b480", "5d90f06bb70a0a3dced62413346235c02b1aa086", "843959ffdccf31c6694d135fad07425924f785b1", "c0d05cac959f07f924786d68f32f7a3c0d19f22b", "5262fe8369992259be27165ccd09d1d31c7a4def", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "6dd9bb6b38e5b84616e207f00a181dbadce06937", "8423a5782a1acda21a6f68c307ce5376ebef13c7", "5562a56da3a96dae82add7de705e2bd841eb00fc", "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "ca0382b2e2aff0249ae94833641108f281d13b0d", "db869fa192a3222ae4f2d766674a378e47013b1b", "de75e4e15e22d4376300e5c968e2db44be29ac9e", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "1578e606cad01df459162404ed9edfcb17d787aa"], "url": "https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698"}, "89b1f4740ae37fd04f6ac007577bdd34621f0861": {"id": "89b1f4740ae37fd04f6ac007577bdd34621f0861", "title": "Generating Sequences With Recurrent Neural Networks", "authors": [{"authorId": "1753223", "name": "A. Graves", "paperCount": 83, "citationCount": 83500, "hIndex": 54}], "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2013, "reference_count": 34, "citation_count": 3387, "influential_paper_citations": 348, "is_open_access": false, "citations": ["acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "501c02c7caa7fc2c7077405299b4cbe7d294b170", "1e04ca1998c04040c9c10685fc0daa4ecc13855b", "62d1a3137b01a69443bebf4d92c1990ec512a6a1", "ba91490d9775474e0e7e53f3dd43005ca547240e", "92bf1c069747374fbc3efb55e7a916a3e2d736da", "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "9933a5af7895354087baf6c96b64dc8a8973eaed", "5b64710efe5a1ccc0cd13ff8b4d817383290ff83"], "references": ["5cea23330c76994cb626df20bed31cc2588033df", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8", "76f9f7f03a660306c3e351e3b7ab6c7df048b411", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "e95d3934e51107da7610acd0b1bcb6551671f9f1", "5a9ef216bf11f222438fff130c778267d39a9564", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "346fbcffe4237aa60e8bcb3d4294a8b99436f1d0", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "0228810a988f6b8f06337e14f564e2fd3f6e1056", "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22", "ed14f8d87e84b4ee2698d97477414f3a472007da", "9eb7daa88879f283ae05e359d6c502a320b833c9", "2f83f6e1afadf0963153974968af6b8342775d82", "047655e733a9eed9a500afd916efa566915b9110", "c135eede17f30b901275057553d37c07c0b90198", "aed054834e2c696807cc8b227ac7a4197196e211", "bdc53fb58689cfc12cc88d774f179b3f0baae472", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "030a977bf32e81fb694117d78ac84a3fbe2a1d81", "10dae7fca6b65b61d155a622f0c6ca2bc3922251", "d0be39ee052d246ae99c082a565aba25b811be2d", "d4ca18249446328c86d9da295a21c679aea1ed77", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "dbc0a468ab103ae29717703d4aa9f682f6a2b664", "9c2dff1c9b04da49f50fd85ba010d2a041fc5c39", "50e2733d2a8a9b3929bda278d382c37711f3fa8e"], "url": "https://www.semanticscholar.org/paper/89b1f4740ae37fd04f6ac007577bdd34621f0861"}, "174bbdb96252454cbb40a9c4e53335996235a008": {"id": "174bbdb96252454cbb40a9c4e53335996235a008", "title": "Fast and Accurate Shift-Reduce Constituent Parsing", "authors": [{"authorId": "145490067", "name": "Muhua Zhu", "paperCount": 48, "citationCount": 1098, "hIndex": 13}, {"authorId": "48378565", "name": "Yue Zhang", "paperCount": 618, "citationCount": 10945, "hIndex": 54}, {"authorId": "2463750", "name": "Wenliang Chen", "paperCount": 136, "citationCount": 1724, "hIndex": 22}, {"authorId": "1390813134", "name": "Min Zhang", "paperCount": 116, "citationCount": 1312, "hIndex": 21}, {"authorId": "1728004", "name": "Jingbo Zhu", "paperCount": 153, "citationCount": 3049, "hIndex": 24}], "abstract": "Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2013, "reference_count": 40, "citation_count": 158, "influential_paper_citations": 25, "is_open_access": false, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "7345843e87c81e24e42264859b214d26042f8d51", "a396a6febdacb84340d139096455e67049ac1e22", "ce2d5b5856bb6c9ab5c2390eb8b180c75a162055", "ce264a4e1490e959d84ddd60edbb0edcbfb3af38", "6d6697d412cee05e3b23687e5db09ced5bf2b3b1", "6d2a987f0020433813ba96d1e1399db46bfbe694", "46fe2ae301aeb75b25ebca0bdc26132ca46f5101", "715dde17239c52b3f9924a5a35edc32b0f27830b"], "references": ["d30eced0a4a779a46a6c9e68a667172ceafd9a8c", "281507601d3d57f087547016a6863a7643cf2417", "8b9e6a39096807ff1de6b287c1e4b97b32a1c159", "c1d96c0f421f8b519cafdbb4e499faa1c797ed9b", "e441126a8dd0cb8363272b7b54207ae92e155bc0", "311a30c808ee16e13e8d1a9ba5c12213de5ddf60", "865081156ed7506d21de25d7b865f278b6b8191d", "5bfd8d40bc071fffaf93685a46974b122ee4239d", "5e80938dedf6c8b8fe6d05278ef47eb957ca931e", "060435976a20b551b659cba4335725d48cbce6b1", "e9c7169aba6c8ff772aba06a19a8570b1e01071f", "248d32911670e551db4835a5a5279d2d9673ee37", "3594af2ebf510609651bf282dfea65c8e837b1a7", "790ecefeaf2b471b439743a772ccce026131bef5", "6e1b12dd7ca3443ba6e0fb38dfa3587a4eb5d539", "78a9513e70f596077179101f6cb6eadc51602039", "6ffbfc01fd25546093d1f1953522d782d4e7fa8d", "ecf3a4cd2b2d721e823fd06b5c467c64e0fde5a8", "a350dc635ea3d0fd6dc8c0224fd8d6b55cc03271", "d3b27746f7a53f2dc5d9b8c2f3d343313622ec36", "0ecb33ced5b0976accdf13817151f80568b6fdcb", "2c72257ae7a4a32dc60569f4e1fe4504b2678112", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "41828fc3dab24784f95e6976e8aaa73f68e1840e", "7bd0a04b0f0687896e765b48619a27ee545e86db", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "f0e1883cf9d1b3c911125f46359f908557fc5827", "f4ba954b0412773d047dc41231c733de0c1f4926", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "0ffa423a5283396c88ff3d4033d541796bd039cc", "54c846ee00c6132d70429cc279e8577f63ed05e4", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "7846dc6537009a18b96a5292422cac7e63a44eb0"], "url": "https://www.semanticscholar.org/paper/174bbdb96252454cbb40a9c4e53335996235a008"}, "5bfd8d40bc071fffaf93685a46974b122ee4239d": {"id": "5bfd8d40bc071fffaf93685a46974b122ee4239d", "title": "Self-Training PCFG Grammars with Latent Annotations Across Languages", "authors": [{"authorId": "34227637", "name": "Zhongqiang Huang", "paperCount": 65, "citationCount": 1783, "hIndex": 21}, {"authorId": "1999488", "name": "M. Harper", "paperCount": 139, "citationCount": 3216, "hIndex": 32}], "abstract": "We investigate the effectiveness of self-training PCFG grammars with latent annotations (PCFG-LA) for parsing languages with different amounts of labeled training data. Compared to Charniak's lexicalized parser, the PCFG-LA parser was more effectively adapted to a language for which parsing has been less well developed (i.e., Chinese) and benefited more from self-training. We show for the first time that self-training is able to significantly improve the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data. Our approach achieves state-of-the-art parsing accuracies for a single parser on both English (91.5%) and Chinese (85.2%).", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2009, "reference_count": 26, "citation_count": 103, "influential_paper_citations": 9, "is_open_access": true, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "7345843e87c81e24e42264859b214d26042f8d51", "8ae9a17c87a4518b513e860683a0ef7824be994d", "0fac3a87cf6f214a10f4e39d73e119ea81b52e46", "174bbdb96252454cbb40a9c4e53335996235a008", "12442420adf1c36887fafd108f4b7f4fc822ae60", "70c2d71a68df279d057471db5173fab0a8aa19ae", "9dccaf6ea0fa19772cf8067295b16df3eb7b4dda", "045f37c5b6de14392d59deae2045ce1837b10460"], "references": ["b84db61bf5ffba650f40de655b8174283be107b2", "5ff4dcc8b52096aa89cd96a791f3c1f69789a8cd", "13392a6b2d27a7c8c79b3dbe9be7b3f42e42c24c", "5b2beddeb063bd5988da70ae58f3e0a6564e647a", "e9c7169aba6c8ff772aba06a19a8570b1e01071f", "7a8475e88bef685273bfc0795f6ac68adf210a2a", "790ecefeaf2b471b439743a772ccce026131bef5", "9e71fb31ee25a4117509f79e21b4a2518dc9137d", "8067a93316a34181fbb4b765310f15e11925d5f3", "6e1b12dd7ca3443ba6e0fb38dfa3587a4eb5d539", "f52de7242e574b70410ca6fb70b79c811919fc00", "78a9513e70f596077179101f6cb6eadc51602039", "4e751a9017dbd825418d35afba6442df958a973c", "713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "0ecb33ced5b0976accdf13817151f80568b6fdcb", "2c72257ae7a4a32dc60569f4e1fe4504b2678112", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "2f6070f88ecca9a4d20167571a856d6884d6cb1a", "b16fc0643c715813c2eafc3f6d0e2bd59354b236", "b5b0ab2d445a393341d292b26c8a9a6d01c62051", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "2a5e619f2c5f4220438b1357e596db5b1578398d"], "url": "https://www.semanticscholar.org/paper/5bfd8d40bc071fffaf93685a46974b122ee4239d"}, "f52de7242e574b70410ca6fb70b79c811919fc00": {"id": "f52de7242e574b70410ca6fb70b79c811919fc00", "title": "Learning Accurate, Compact, and Interpretable Tree Annotation", "authors": [{"authorId": "1754497", "name": "Slav Petrov", "paperCount": 86, "citationCount": 12214, "hIndex": 40}, {"authorId": "37157794", "name": "Leon Barrett", "paperCount": 18, "citationCount": 1249, "hIndex": 7}, {"authorId": "3323298", "name": "R. Thibaux", "paperCount": 9, "citationCount": 2967, "hIndex": 8}, {"authorId": "38666915", "name": "D. Klein", "paperCount": 235, "citationCount": 27438, "hIndex": 75}], "abstract": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2006, "reference_count": 24, "citation_count": 983, "influential_paper_citations": 163, "is_open_access": true, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "eba36ac75bf22edf9a1bfd33244d459c75b98305", "cfa2646776405d50533055ceb1b7f050e9014dcb", "acc4e56c44771ebf69302a06af51498aeb0a6ac8", "9c0ddf74f87d154db88d79c640578c1610451eec", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "6e1b12dd7ca3443ba6e0fb38dfa3587a4eb5d539", "790ecefeaf2b471b439743a772ccce026131bef5", "ce2d5b5856bb6c9ab5c2390eb8b180c75a162055", "6d5ff1e0d334e0236ba6eaa905bd681695051a83"], "references": ["93231398214275e4316aa19ced49a508ace56ffa", "713a4825ea09801ebc24ce207ca9ae5fbc97ac65", "0ecb33ced5b0976accdf13817151f80568b6fdcb", "1174297ddcf08937a94d8efe4c1efb65f3b92fd8", "3fc44ff7f37ec5585310666c183c65e0a0bb2446", "a600850ac0120cb09a0b7de7da80bb6a7a76de06", "b8f896caa1226713ed2731101cb8de21195dbf0b", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "6c9f553e723a40a6713453b734b552c1928bf52b", "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c", "693fe8c8c1e3b2ae2bd63ab9e2a3b5e86b74344d", "aad8c5ae265e8f645101245afb9d9c9cdf40b4ca", "dd85cca2c133835ea29069a6a4438c70185bd427", "0bf69a49c2baed67fa9a044daa24b9e199e73093", "0c0eab87d4855c42ae6395bf2e27eefe55003b4a", "f7d04596a7a0e6e6b8c9199ce4aba2d2a6f3dbc6"], "url": "https://www.semanticscholar.org/paper/f52de7242e574b70410ca6fb70b79c811919fc00"}, "78a9513e70f596077179101f6cb6eadc51602039": {"id": "78a9513e70f596077179101f6cb6eadc51602039", "title": "Effective Self-Training for Parsing", "authors": [{"authorId": "2240597", "name": "David McClosky", "paperCount": 33, "citationCount": 8886, "hIndex": 19}, {"authorId": "1749837", "name": "Eugene Charniak", "paperCount": 181, "citationCount": 17813, "hIndex": 60}, {"authorId": "145177220", "name": "Mark Johnson", "paperCount": 204, "citationCount": 14285, "hIndex": 51}], "abstract": "We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2006, "reference_count": 23, "citation_count": 623, "influential_paper_citations": 72, "is_open_access": true, "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "bc1022b031dc6c7019696492e8116598097a8c12", "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb", "299847adf3ee558a760475ffa364facac3ebbb16", "57458bc1cffe5caa45a885af986d70f723f406b4", "c92970286c535992a86539b761357761e97a37ee", "acc4e56c44771ebf69302a06af51498aeb0a6ac8", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "7345843e87c81e24e42264859b214d26042f8d51", "ede8ba65c4db10d357d9c3bf8e75b092f536fc84"], "references": ["8a62631f3782d0f6440745eea3b041ce917af4aa", "8dfdedcbeb3b67ecaa4d85ca4b9d1aff368b8e2a", "0ecb33ced5b0976accdf13817151f80568b6fdcb", "844db702be4bc149b06b822b47247e15f5894cc3", "1174297ddcf08937a94d8efe4c1efb65f3b92fd8", "906765f7ac46011123cca59de775216f1ee9b451", "b16fc0643c715813c2eafc3f6d0e2bd59354b236", "77021fb48704b860fa850dd103b79db4dcf920ee", "f0c90a5bc53027d76b24854209a4cdb1bd75dd2f", "a48abd949978432b8a9498f94b029e440bc1b4f3", "566eb7be43b8a2b2daff82b03711098a84859b2a", "ee7e21dd09949a5a53b39c13fca9cd3d55e2bc50", "48b915e2d54e93c3a66e9ddc35244ebdef22c101", "76d5e3fa888bee872b7adb7fa810089aa8ab1d58", "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "463dbd690d912b23d29b7581fb6b253b36f50394", "3fa4a8191e37b601877716858e6b1026e66e3c5c", "278841ab0cb24c1abcb75e363aeed1fa741c8cc4", "2a5e619f2c5f4220438b1357e596db5b1578398d", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "bdaf232c561f1f50e88b1d24097e214890b37e8b"], "url": "https://www.semanticscholar.org/paper/78a9513e70f596077179101f6cb6eadc51602039"}, "aed054834e2c696807cc8b227ac7a4197196e211": {"id": "aed054834e2c696807cc8b227ac7a4197196e211", "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies", "authors": [{"authorId": "3308557", "name": "S. Hochreiter", "paperCount": 224, "citationCount": 82818, "hIndex": 45}, {"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}], "abstract": "D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6\\M[ X N@]_^O\\`JaNcb V RcQ W d EGKeL(^(QgfhKeLOE?i)^(QSj ETNPfPQkRl[ V R)m\"[ X ^(KeLOEG^ npo qarpo m\"[ X ^(KeLOEG^tsAu EGNPb V ^ v wyx zlwO{(|(}<~O\u007fC}\u0081\u0080(\u0082(xp{a\u0083y\u0084.~A}\u0086\u0085\u0088\u0087_~ \u0089C\u008al\u00833\u0089#|<\u0080Az\u0086w#|l\u00806\u0087 \u008b(| \u008c JpfhL X\u008dV\u008f\u008e EG^O\u0090 QgJ \u0091 ETFOR\u0086\u0092\u0093] ^O\\\u0094J\u0095NPb V RcQ\u0097\u0096 X E)ETR \u00986EGKeLOETNcKMLOE\u009a\u0099 F\u0088\u009b ETN V RcQgJp^(^OE ZgZ E i ^(Qkj EGNPfhQSRO\u009b E \u009cOE2m1Jp^ RcNY\u009b E V\u0095Z sO\u009d\u009f\u009e! \u008d\u00a1 q.n sCD X KGKa\u00928\u009d\u00a2EG^ RPNhE\u00a4\u00a3 \u00a5\u00a6Q ZgZ E\u0095s m\u00a7J\u0095^ RPNO\u009b E V\u0095Z s( \u0308 X \u009b EG\u00a9#E\u0081Kas#\u009d V ^ V \u009c V s(H a \u009d\u00aba\u0095\u00ac3\u00ad \u00ae#|.\u0080Y \u0304y} xa\u00b0O\u007fC}l{\u008dx\u0093\u0087 \u0089 \u0083yxl\u0080Y~3{\u008d| \u0084 \u00b12\u0087Pz \u0084 \u009e V J Z J U N V fhKTJp^(Q \u0091 ETFOR\u0086\u0092 J\u0095\\ D vYf3RPEGb \u0301f V ^(\u009c\u00a7\u009d\u0088Jpb\u008fF X RPETN@D KTQ\u0097EG^(KTE i ^(QSjpEGNPfhQSR4v\u03bcJ\u0095\\ U\u00b6Z JaNPEG^(K\u00b7E jYQ V \u009c(Q \u0327D V ^ R V m V N3R V aOs#1 o \u00a1Ga r U Q\u0097NhE\u0081^OoTE1\u20444\u00bb,] R V\u0095Z vC1\u20442 3\u20444 \u0084 x \u00b1 x \u007f \u008b#\u00bf }\u00c0\u0087 \u00893\u0080t}l\u0082C}2\u0087P}<~ \u00act[ X NP\u0090\u0095E\u0081^\u00a7D KeL(b \u0301Qg\u009c(L X \u00a9yETN ] \u0091 DY]_\u00c1 \u009d\u0088J\u0095NPfhJ\u00c3\u00c2 Z j ETo\u0081Q V a\u0095 rpopo2\u00c4 X \u0090 V ^(J(sCD \u00c5)QSRPoTEGN ZgV ^(\u009c \u00c6 \u0089#|\u0095{3 \u0304\u008d|.\u0080(\u007fC}.\u008bC\u00bfY}p\u0084 \u0087Pz\u0086w", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2001, "reference_count": 30, "citation_count": 1687, "influential_paper_citations": 59, "is_open_access": false, "citations": ["30e85b1651b1d2b726e71e2747d09b187bbd045a", "97faeefa771e8cc8e55159e2bd03e6f5eef249a8", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "2113af2c71526b211612586daa185d326712cb9c", "483c85d517ed7a7aa9553c54dae4c58b2ca304d4", "18a93dc1558bf9d7534d0b416633cebaf75c1145", "189ef6bea81fe8ee3ed3214b9394f7e2027eff98", "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "c6dd45c1986f3577336bc9aa596cbd23d610d904", "fb2edf25484c9e9e5f94b719c55dc1faf7591bfa"], "references": ["11540131eae85b2e11d53df7f1360eeb6476e7f4", "2a9ce6eeada970ec7a494a3efc2b333e3b943ab2", "4e2b855834df1f800414aa4950a22deae63577d6", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "b158a006bebb619e2ea7bf0a22c27d45c5d19004", "f6e91c9e7e8f8a577a98ecfcfa998212a683195a", "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "914fa99500eb779e33d22609f8a0fdf3fd2799e1", "10dae7fca6b65b61d155a622f0c6ca2bc3922251", "d0be39ee052d246ae99c082a565aba25b811be2d", "fba6007d482db5fbe8cd6c3af90fe0922453e1d2", "13369d124474b5f8dcbc70d12296a185832192b2", "a4a9af3c0418bceef7e057db0f08fdfeab4cdff5", "b1ae0fb208fd389d2ff723e5442f9ca7896cb0a4", "d0dd604b2b29bbc0adee2b71bbabca5d5ad3cd54", "f55e5107f756e29f1e6ac6109dc44d698d6301fb", "50c770b425a5bb25c77387f687a9910a9d130722", "e141d68065ce638f9fc4f006eab2f66711e89768", "9b7861d28f653ead3e02f1ae5c07540b2d07346d", "3f3d13e95c25a8f6a753e38dfce88885097cbd43", "c115f0d793225c515ebce6be91521fcb8374ad6b", "e08d090d1e586610d636a46004876e9f3ded8209", "5146d7902132bcb0b2e6fe5f607358768fc47323", "266e07d0dd9a75b61e3632e9469993dbaf063f1c", "111fd833a4ae576cfdbb27d87d2f8fc0640af355"], "url": "https://www.semanticscholar.org/paper/aed054834e2c696807cc8b227ac7a4197196e211"}, "44d2abe2175df8153f465f6c39b68b76a0d40ab9": {"id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "title": "Long Short-Term Memory", "authors": [{"authorId": "3308557", "name": "S. Hochreiter", "paperCount": 224, "citationCount": 82818, "hIndex": 45}, {"authorId": "145341374", "name": "J. Schmidhuber", "paperCount": 475, "citationCount": 132959, "hIndex": 94}], "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 1997, "reference_count": 49, "citation_count": 58711, "influential_paper_citations": 10342, "is_open_access": false, "citations": ["220b915c3f86925c7dc05a59e613f53e485ad87d", "719b251a5ddc83d4ec35e939c908b0ca3dcae194", "89f86a915245043e1ecd7a3c462945fdde171e7b", "1c2e1c73e3f43688e4f22d29ae5a6463721690f9", "1c83f3f9789df43bf937ae2618721e2da83dcc06", "242a1bac69575aefa43f48f36990b53af486638b", "75c4f57e8028efa26c0739253459a2d6c2ff55bf", "c6d2d950b64973f1bec3b0dd0ed46fbf0319351d", "4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c", "2198ec31e64e7ff62e14ac18651cd404aae789ce"], "references": ["b158a006bebb619e2ea7bf0a22c27d45c5d19004", "f6e91c9e7e8f8a577a98ecfcfa998212a683195a", "762031682309e0124b2811ee05a798860dde82d1", "030ba5a03666bf4c3a17c64699f8de8ec13d623b", "32e97eef94beacace020e79322cef0e1e5a76ee0", "063fe6ed19c0204d55bde174483c5a93eb4819c0", "10dae7fca6b65b61d155a622f0c6ca2bc3922251", "d0be39ee052d246ae99c082a565aba25b811be2d", "0e63335010c6d3a56ffba62595118447ff9e8734", "13369d124474b5f8dcbc70d12296a185832192b2", "34f8c5769899dfd9450bb13c3f52c18c88444515", "a4a9af3c0418bceef7e057db0f08fdfeab4cdff5", "63b6835c8fb31d91f503b8e08dff4bac8966c8cf", "b1ae0fb208fd389d2ff723e5442f9ca7896cb0a4", "ebf62950d733a4a8f9ecd8d3752dee8d13fc8e6d", "d0dd604b2b29bbc0adee2b71bbabca5d5ad3cd54", "f55e5107f756e29f1e6ac6109dc44d698d6301fb", "a64ca771a733d58dcbf8f7a3fe65a09310424bf8", "50c770b425a5bb25c77387f687a9910a9d130722", "89b9a181801f32bf62c4237c4265ba036a79f9dc", "e141d68065ce638f9fc4f006eab2f66711e89768", "2f7c4048a03281e976f28d35c2f9fef3a58346e6", "9b7861d28f653ead3e02f1ae5c07540b2d07346d", "3f3d13e95c25a8f6a753e38dfce88885097cbd43", "26bc0449360d7016f684eafae5b5d2feded32041", "415dca031402b5186c0c8bf00ca7bb60bfedb986", "9e8cf03655d224b0994d0f9d4f5aa80bca07021a", "c115f0d793225c515ebce6be91521fcb8374ad6b", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "8be3f21ab796bd9811382b560507c1c679fae37f", "e08d090d1e586610d636a46004876e9f3ded8209", "86dee86ea1b2eb5651e9ef9a4962460718d2ebd4", "bd46c1b5948abe04e565a8bae6454da63a1b021e", "fd0da2f1d2b95e5b62221a00ff132219d0c853b7", "34468c0aa95a7aea212d8738ab899a69b2fc14c6", "ae254bfe3533558980cb2902f469cc3606807998", "6d72a0e83e772468c6084ae7c79e43a4f5989feb", "f19ca2336b8a7cc9344ffef5dbe3d3ff17954ab4", "5146d7902132bcb0b2e6fe5f607358768fc47323", "266e07d0dd9a75b61e3632e9469993dbaf063f1c", "6602985bd326d9996c68627b56ed389e2c90fd08", "c304dd3c2df242308702a88aaa6002f5345aaa1b"], "url": "https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9"}, "0b44fcbeea9415d400c5f5789d6b892b6f98daff": {"id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank", "authors": [{"authorId": "1734174", "name": "M. Marcus", "paperCount": 109, "citationCount": 17912, "hIndex": 35}, {"authorId": "2424234", "name": "Beatrice Santorini", "paperCount": 34, "citationCount": 10975, "hIndex": 16}, {"authorId": "2063206", "name": "Mary Ann Marcinkiewicz", "paperCount": 3, "citationCount": 9418, "hIndex": 3}], "abstract": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 1993, "reference_count": 33, "citation_count": 8408, "influential_paper_citations": 1405, "is_open_access": true, "citations": ["72cdd6ebe0221fb568ef20534f44ba5b35190a56", "40848b41ed8c9c255ecd8a920006877691b52d03", "1e04ca1998c04040c9c10685fc0daa4ecc13855b", "a84d359922a69916a05de7c91204b79d02c36cda", "00696ba295d66f049d70272219f7fea4266171be", "e210f4b0a9b00b73f5f353ca38a60776fab443af", "d1206ccabd1980848f14472d6548251c2fab7963", "73e0f38ab49b19b86321016b773e15f1d02e3a72", "427973cbf535187c95cd174adce64c20292a0c78", "67b1f8e48118bb1aa250f400d475425317bf4117"], "references": ["15e4843e2c55843b5c5b429f89dad3d99e801f02", "2a029f49255919bd77b874436868ad24867850f3", "0c0eab87d4855c42ae6395bf2e27eefe55003b4a", "820214a49dd8cae98506f08ac953074e6f66c715", "f89268038d043ab1bc3aff0034202ba46e7acd84", "73cb6c929026bf5475602bad3fdde033da3c86c9", "8cf9b7c08655dadad0cad00771f3c9670181004e", "098de23f08e080bed8a224bf1ad2e504688d3db3", "a145854ede2f62098bf4e92de1584ab270b676c9", "c2fbd6cead3815b8e7038fda6f0f0254a2218ca7", "856f8ba3913a6733e5720f47e4f95b1f81fb6fd7", "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "91b52959c7731830f1518ed35e5ab1dabadf79ec", "729316fbded86763104f3412cadf98f00a9a3993", "05e2b18474f4f6abd6b722c946b41c1221557123", "c89cb43217197c64513c8fb1d46f2047e3e67900"], "url": "https://www.semanticscholar.org/paper/0b44fcbeea9415d400c5f5789d6b892b6f98daff"}, "370b680057a6e324e67576a6bf1bf580af9fdd74": {"id": "370b680057a6e324e67576a6bf1bf580af9fdd74", "title": "Self-Supervised Learning: Generative or Contrastive", "authors": [{"authorId": "2111312892", "name": "Xiao Liu", "paperCount": 25, "citationCount": 1192, "hIndex": 11}, {"authorId": "40898720", "name": "Fanjin Zhang", "paperCount": 8, "citationCount": 608, "hIndex": 5}, {"authorId": "2068251467", "name": "Zhenyu Hou", "paperCount": 11, "citationCount": 508, "hIndex": 6}, {"authorId": "2144718801", "name": "Zhaoyu Wang", "paperCount": 3, "citationCount": 440, "hIndex": 2}, {"authorId": "2066299001", "name": "Li Mian", "paperCount": 2, "citationCount": 438, "hIndex": 2}, {"authorId": "1519070643", "name": "Jing Zhang", "paperCount": 140, "citationCount": 3673, "hIndex": 15}, {"authorId": "2109541439", "name": "Jie Tang", "paperCount": 52, "citationCount": 5571, "hIndex": 20}], "abstract": "Deep supervised learning has achieved great success in the last decade. However, its defects of heavy dependence on manual labels and vulnerability to attacks have driven people to find other paradigms. As an alternative, self-supervised learning (SSL) attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further collect related theoretical analysis on self-supervised learning to provide deeper thoughts on why self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided<inline-formula><tex-math notation=\"LaTeX\">$^1$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>1</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"liu-ieq1-3090866.gif\"/></alternatives></inline-formula>.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 185, "citation_count": 440, "influential_paper_citations": 34, "is_open_access": true, "citations": ["2a9fbca9dc6badbeedc591ad829c5c6e0f950fd6", "128917425601a541c93c600a2f67d654512928bb", "f3a332ff1b73acda482e5d83696b2c701f487819", "9653c070724e44f023e8cc3ec79f0b9e6d59480d", "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa", "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "e70930b8a8c7ba7af5f3df13ecf7b60364a4c189", "0c635829365f8639f699892764669ad143bff88a", "3e38f4b4055abecbac2e618df2ecb33554073e08", "00969b4dcf8f9b21895bd038a51a038018da84f0"], "references": ["0f8aa47ff8c6c49a347e192debe20ce4e5a4caea", "0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d", "57835c5ad5424f94ee75901c3113730f3900e656", "6b989b8327db3a7212141c59c1569f0219775058", "4c94ee7df6bc2bfcac76703be4f059a79010f7e5", "04faf433934486c41d082e8d75ccfe5dc2f69fef", "10161d83d29fc968c4612c9e9e2b61a2fc25842e", "91fb815361fdbf80ff15ce4d783a41846bd99232", "3e7f5f4382ac6f9c4fef6197dd21abf74456acd1", "ac2e6afbadc3428eb5dff167b52025e64525441e", "38f93092ece8eee9771e61c1edaf11b1293cae1b", "368c72c2298e5f8276398b2cb198702281eac4f8", "1f3c381eedfe8914b81e93070bfdb00cf86ac943", "9a56ab8b1aba50dc2fea3cf4b531d30891a88ba9", "7f768fa192a76ab097ccfda0a68523bc36425423", "79cd9f77e5258f62c0e15d11534aea6393ef73fe", "2c7606eef27a9603d0cc94b37b9ad1690c0955dc", "756810258e3419af76aff38c895c20343b0602d0", "3bcb17559ce96eb20fa79af8194f4af0380d194a", "a1b8a8df281bbaec148a897927a49ea47ea31515", "b905a4452b9eb53ec5bdec44381dcb1c458e186f", "0ca7d8c3250d43d14fdde46bf6fc299654d861ef", "34733eaf66007516347a40ad5d9bbe1cc9dacb6b", "832fff14d2ed50eb7969c4c4b976c35776548f56", "036d743c7ca1e513adf0a91594fc8111e03dc30c", "c7fc1cac162c0e2a934704184c7554fd6b6253f0", "4906a09839dcb6ac96a52e06bc7bd613f0482967", "0170bb0b524df2c81b5adc3062c6001a2eb34c96", "521b4e26df0f1cf5763dece14cbb218df152dc59", "add2f205338d70e10ce5e686df4a690e2851bdfc", "20ba55ee3229db5cb190a00e788c59f08d2a767d", "b04889922aae7f799affb2ae6508bc5f5c989567", "7a064df1aeada7e69e5173f7d4c8606f4470365b", "41fcef711faca9013fd0980a9f6ec1d23c9c76c8", "2fb59ebe271d6b007bb0429c1701fd1004782d1b", "81f5810fbbab9b7203b9556f4ce3c741875407bc", "97f4d09175705be4677d675fa27e55defac44800", "789a7069d1a2d02d784e4821685b216cc63e6ec8", "e49dca82bb4096dcd9a5560597f8ca64643b0090", "fff34334f4a1a7afe8ddbb27911af52decbac987", "ed3ed6458abdfb54fa1e3ce4434dfd9adb3b55ad", "93d63ec754f29fa22572615320afe0521f7ec66d", "f19b884089a3215da5715779bff11dc70fe47ad5", "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "64e87487a87532778352a358209935fef48300d7", "cde35c87aaabbc617d38f9cfaa2721a2e166d750", "db787640c9b42416ff8d7015546e667e58267177", "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "64690e9d0fd467be2cfb4fa904823f0c5a857839", "2673354bc246e65962a6dca32d5f41cc8f11a249", "37f675bf4329cd9e0811e232e81eef2b814c703d", "9b09d296059909490096e34e9df2d95314787ad5", "6be216d93421bf19c1659e7721241ae73d483baf", "3ccd291c8848c73ca34152e27c3ec296cfc838d0", "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "ad8f43bfdb041fd97b3fea2eea23fff6e2221476", "031e4e43aaffd7a479738dcea69a2d5be7957aa3", "982b9c948c7e1f23e14a014002e451f9fd673252", "403227333329b36183004f04db72362b604adef3", "c8b25a128f4bfd0c79de82c174dd403b2ef6eeb1", "9576b234b49a11f2e9977794ab18cd1f1bcf4707", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "ceb2ebef0b41e31c1a21b28c2734123900c005e2", "37ef97bd03a03d34a40ba5d6bbe4d3889c2b30f0", "967a21a111757d6af7f7a25ca7ea2bdf6d505098", "df7ad8eeb595da5f7774e91dae06075be952acff", "22aab110058ebbd198edb1f1e7b4f69fb13c0613", "eae7d5b15423a148e6bb32d24bbabedfacd0e2df", "81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2", "eaf5b3a28606da5014d5c3d106b2fc4306933808", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "9405cc0d6169988371b2755e573cc28650d14dfe", "03fffbaca597aaa740ec910acae727976f9bc19c", "22655979df781d222eaf812b0d325fa9adf11594", "b8da4337c92acda632e8138be1b525a3aef54b85", "fa0fa6c799e7ebb8b2432f0ea5314c57540abefa", "1d033b30f38642e4b6dd146bb8b464bfb58aad96", "9fa3e53b5937a0ec92499ed415e339ede6c92010", "b227f3e4c0dc96e5ac5426b85485a70f2175a205", "21b786b3f870fc7fa247c143aa41de88b1fc6141", "d9f836a2062864e4808e12224e2286a353498202", "155b7782dbd713982a4133df3aee7adfd0b6b304", "561ede166947a8bedb8be9acff182913156e06c6", "2a1f38e4451e826e01c9874954ba7c6f32ff79f4", "d81fc968196e06ccafd7ea4c008b13e1cad1be64", "0814336ef1be7eac6fc8f26ea19751fddee412db", "e1cef464322243feb12ac3f81873c912e071a1a6", "3febb2bed8865945e7fddc99efd791887bb7e14f", "aab368284210c1bb917ec2d31b84588e3d2d7eb4", "84de7d27e2f6160f634a483e8548c499a2cda7fa", "e22979cdf147a63be74f3816ef59ef11f3508919", "51a2bc2e8fb8ed47a085df33dd965e57335080a0", "f7b1453416d3d95af19ff465104e78968920354d", "4a8e4431fae6bc8f5b2a9f527d81b593c8c1e40a", "73047a0f0192a35d3b5c6f5ebeadf3706b17e4dc", "8976e91ccae57a20c29f3c9d88bf45b19973c952", "33998aff64ce51df8dee45989cdca4b6b1329ec4", "4657c06d57cea792eab4f14e3a12f50d88109680", "562c09c112df56c5696c010d90a815d6018a86c8", "908272f8e6340971600148d4e73f50e1e8843aaf", "ee9cc8e663d650ae96405ad680d6447066e6fb23", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "f466157848d1a7772fb6d02cdac9a7a5e7ef982e", "d6eda0c16d226976506396653d14044c185eaf3e", "8d2b706dd0aaae1bec2c74f0b66c3e4b1ede57ab", "ecf6c42d84351f34e1625a6a2e4cc6526da45c74", "d21ebaab3f715dc7178966ff146711882e6a6fee", "acd87843a451d18b4dc6474ddce1ae946429eaf1", "c0b61dce79e540160ef43f8763609e64e0841902", "c0c0990b84a350d5efde8d3b2cb2636b6b57c21c", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "6b7d6e6416343b2a122f8416e69059ce919026ef", "66721162f712690bb10928132d402d9bd4460c1b", "0f7f5679615effcc4c9b98cf2deb17c30744a6d7", "164b8826b9a30adf69630e634b308046b8cec619", "3bd6e4f596d99a47e1e70504e0fc51267ab213e9", "62f3d3015cee122bd147d7d878c85f70cc15680d", "8acbe90d5b852dadea7810345451a99608ee54c7", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "67d968c7450878190e45ac7886746de867bf673d", "df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3", "36eff562f65125511b5dfab68ce7f7a943c27478", "5694e46284460a648fe29117cbc55f6c9be3fa3c", "e2dba792360873aef125572812f3673b1a85d850", "fcf43325529c8b1cc26aeb52fd5d7e532abb0a40", "1db6e3078597386ac4222ba6c3f4f61b61f53539", "09879f7956dddc2a9328f5c1472feeb8402bcbcf", "54906484f42e871f7c47bbfe784a358b1448231f", "e50a3a4994a26777def2c0c4323e03f6282d7dac", "df0402517a7338ae28bc54acaac400de6b456a46", "07627bf7eb649220ffbcdf6bf233e3a4a76e8590", "36ee2c8bd605afd48035d15fdc6b8c8842363376", "0936352b78a52bc5d2b5e3f04233efc56664af51", "05dd7254b632376973f3a1b4d39485da17814df5", "35da0a2001eea88486a5de677ab97868c93d0824", "ffdcad14d2f6a12f607b59f88da4a939f4821691", "1c4e9156ca07705531e45960b7a919dc473abb51", "7d0effebfa4bed19b6ba41f3af5b7e5b6890de87", "2c37826357c18148f6f0773d22f7a4488831c1c4", "2ec8f7e0257a07d3914322b36072d1bbcd58a1e0", "8201e6e687f2de477258e9be53ba7b73ee30d7de", "8e63784bd5a24d5e3035e2a11753e65e6e56625d", "5091316bb1c6db6c6a813f4391911a5c311fdfe0", "1a37f07606d60df365d74752857e8ce909f700b3", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "8388f1be26329fa45e5807e968a641ce170ea078", "1d5972b32a9b5a455a6eef389de5b7fca25771ad", "4f8d648c52edf74e41b0996128aa536e13cc7e82", "c8c04ed972d38e2326a53d322a6f2d7e0f8218c1", "c2fd72cb2a77941e655b5d949d0d59b01e173c3b", "fc1b1c9364c58ec406f494dd944b609a6a038ba6", "8ebc4145aef6a575cbaffcfeec56b20586db573a", "0834e74304b547c9354b6d7da6fa78ef47a48fa8", "4d376d6978dad0374edfa6709c9556b42d3594d3", "317aee7fc081f2b137a85c4f20129007fd8e717e", "dc8301b67f98accbb331190dd7bd987952a692af", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "c3b38c2fd30adb316d0bdb32e983804be5595c30", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "fff114cbba4f3ba900f33da574283e3de7f26c83", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "99c970348b8f70ce23d6641e201904ea49266b6e", "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "26435746f1b7ed2563cad2f5ac9f650f70909ff1", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "33c95f4d13958e65e676a9cd64bd4d8df0fdb44d", "b87274e6d9aa4e6ba5148898aa92941617d2b6ed", "a538b05ebb01a40323997629e171c91aa28b8e2f", "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "f020b61789112fe7241b871907268f0bdc5c84fa", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "d0be39ee052d246ae99c082a565aba25b811be2d", "5bef9f0d1e74409ca0e67f79f9547c5f4e4e4257", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "aa109a5c8440332a05ac538d98c4f93d25500c81", "4f7476037408ac3d993f5088544aab427bc319c1", "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd"], "url": "https://www.semanticscholar.org/paper/370b680057a6e324e67576a6bf1bf580af9fdd74"}, "659597b1699ba5b73da9a8628bf7e4ad9bebd242": {"id": "659597b1699ba5b73da9a8628bf7e4ad9bebd242", "title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting", "authors": [{"authorId": "7311172", "name": "Giorgos Bouritsas", "paperCount": 16, "citationCount": 315, "hIndex": 7}, {"authorId": "51484149", "name": "F. Frasca", "paperCount": 14, "citationCount": 901, "hIndex": 7}, {"authorId": "1776444", "name": "S. Zafeiriou", "paperCount": 384, "citationCount": 20446, "hIndex": 65}, {"authorId": "1732570", "name": "M. Bronstein", "paperCount": 314, "citationCount": 23782, "hIndex": 69}], "abstract": "While Graph Neural Networks (GNNs) have achieved remarkable results in a variety of applications, recent studies exposed important shortcomings in their ability to capture the structure of the underlying graph. It has been shown that the expressive power of standard GNNs is bounded by the Weisfeiler-Leman (WL) graph isomorphism test, from which they inherit proven limitations such as the inability to detect and count graph substructures. On the other hand, there is significant empirical evidence, e.g. in network science and bioinformatics, that substructures are often intimately related to downstream tasks. To this end, we propose \u201dGraph Substructure Networks\u201d (GSN), a topologically-aware message passing scheme based on substructure encoding. We theoretically analyse the expressive power of our architecture, showing that it is strictly more expressive than the WL test, and provide sufficient conditions for universality. Importantly, we do not attempt to adhere to the WL hierarchy; this allows us to retain multiple attractive properties of standard GNNs such as locality and linear network complexity, while being able to disambiguate even hard instances of graph isomorphism. We perform an extensive experimental evaluation on graph classification and regression tasks and obtain state-of-the-art results in diverse real-world settings including molecular graphs and social networks.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 154, "citation_count": 122, "influential_paper_citations": 20, "is_open_access": true, "citations": ["3576cbc79bb552c6847ef5ff5f6cef217a4bee86", "536da0e76290aea9cbe75c29bac096aeb45ef875", "454304628bf10f02aba1c2cfc95891e94d09208e", "aafe1338caef4682069e92378f1190785ec24c2c", "44b9f16ba417b90e2e7c42f9074378dd06415809", "758294483c308d9d6028321f0741b6e4617c9ab9", "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb", "093bd7a7dd31a8eb4da371d012da5fd272bb96ca", "6667a57c8ffa3f3c0d724b1e8e986758995df2b8", "c7ac16a01c0ba9f6833e834973f50c92a7260d35"], "references": ["b6499e3dd7c18bf7ef7cf233d3f4003c649ad7ac", "aafe1338caef4682069e92378f1190785ec24c2c", "92d229609b33717ec6e0e97591def3c086913858", "c36571ac50808c75fa8a5d37f1041af22e89e6ee", "463f490d3bded6e527b0838da8495ed6441da25a", "b626e35f2d30ac54a77e6d80b6fa8d01fa68971f", "f7b550cba2d61b3fb676e72c92c245134363fda1", "33e31195ab6853dfb8b1d90b07da5755f9bf5de0", "67f473caaa52a97e65bb1bcb9029a580c4f8d10f", "3514bd316ad14c71863c58ccd9af9f1bc5dc4766", "43934016318c1c0727ead97886fef8bd052e6c66", "3209f553d3e67a34e7dafc1973e1ef9a93dd5822", "e9ef895b56e5dee045af31e737d46c255194f4f2", "467a90e1315ad10be812e420f89551d818b15a83", "cb42d038ee5c13c2db26d46149b1688d76289291", "4b1193ded00ff3d882eb7ed31db586406c9acd70", "597bd2e45427563cdf025e53a3239006aa364cfc", "19c0d004bd0e42a6449d8b7717cbda4431a67e65", "6fb8b967dad742eb390a3090f094a12f2d909538", "6bc4004347cdf76d84597210796f38fcf7a01a80", "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5", "536da0e76290aea9cbe75c29bac096aeb45ef875", "07a213517dbb8bfe67360a70b532b56bf502ca28", "3db5fcb595492dcd64663c00d56f004dfafa689c", "583d9083d2fcc4678427e597cd41adaba9910c26", "7f42da4abfadf9d5a464affe22d0bd4bf21c0edb", "520005eba3f1b8e852ed9508ead726e15629c78a", "3576cbc79bb552c6847ef5ff5f6cef217a4bee86", "fadfffdbb3588bb8ddcc480613336f1d71694d55", "e4f2c471d27ced746f26cc6e8337ea5cb7c8faf3", "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "e1fd81af050dbdc4232ff8b1ab71cf0973d530b6", "19af82973b785c66c3033377eebca4513e106879", "b9b886967fb0e5f3ed7538d370a9f27b630b95ac", "1ec06d7591ceb91343648cc151546862447f1c9d", "381411d740562de1e766dc8cc833844eb99dde01", "fd075bcdf2d7e13d23f7c249a8eded343d5bbe3b", "589490604f143ee347b628a6d496eceb092bc5fa", "9d17e02bddbf7668ee1a47f012afa1f8de11209f", "f338528c156f275fcff3e310e157b23980e1bd19", "54e6a214c9dca4ba6bb8c5d52e5b219c5ea50e12", "057b594c9e6ed45fc445a4dc45384117bdd15a3e", "63870f553def3c656290255216512829803d426f", "d52961a91f03061c6732a69e292bd1e403e7f8b8", "840bdc32643b434556a800ae20caf63e25d1e3af", "c7fdb033ff15771e9bcea4336135eec171a595f3", "a9ec03dbe702f6909acd1f1f14a3395d0141043b", "63a513832f56addb67be81a2fa399b233f3030fc", "7f5ea861a57e14796f033fd0f5580dbc34ff88f2", "b7a6b7adafd01e939c9266083dfba9edea88846c", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "03e7e8663c69e691be6b6403b1eb1bbf593d31f2", "1ca9bce5d61e7b5b8c9347cdc0e57dc045c80911", "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da", "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9", "6541afa4b4061a7d5c8387514bedea9dc249fd80", "42ec3db12a2e4628885451b13035c2e975220a25", "875883ccc49269bb3fc6ba5fcf824fad8ba084ad", "6d5b65deaa0b517ac3276f5009a43bab890aa102", "d18b48f77eb5c517a6d2c1fa434d2952a1b0a825", "fda0021390fc4fe9b168700a332e3f634f162dc5", "5aea95e1ae78a66474051a330ded374e199b658c", "6c96c2d4a3fbd572fef2d59cb856521ee1746789", "e51e557cf4701510916bb1444074f62e6a04f889", "d81fc968196e06ccafd7ea4c008b13e1cad1be64", "2de0f24f91d86725a26a983776ed9f27c370fa4c", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "f3a9749c602968b19c21a83aab11f35ce2fb86d6", "fd17bd9a5dc24a081ad9743570f50dd6750f54b2", "e9842f649578886801a3b4ce40a89b9125f51daf", "d1ab38e2a4d197b5a86162e65e849193d0f5cfe3", "836daad5381d39b5dea00c32e075f2009c0da59a", "36652428740cd30d245d55889f01a7fb04a91c93", "4e926648026c73bb5a7af2833d7f5edef83827c5", "33998aff64ce51df8dee45989cdca4b6b1329ec4", "cd8a9914d50b0ac63315872530274d158d6aff09", "88427d2143d4cff357c3b393ae7580a7b6e19940", "6d1d91a413af1212fea8791e266282019b62c37d", "a5fe578a6b9f51ce19263676e6395421fedc6d2d", "c751ab01aedc2888a7fe6e8b4f77ab1afa94072f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "6b7d6e6416343b2a122f8416e69059ce919026ef", "3af93b525963cbf434cd8c2644f065cf190fa1b7", "5e83a4d033e1aa9d7c81a6d3f0d3fd5a1be01a1c", "43428880d75b3a14257c3ee9bda054e61eb869c0", "1a39bb2caa151d15efd6718f3a80d9f4bff95af2", "e24cdf73b3e7e590c2fe5ecac9ae8aa983801367", "680a48d5c74824d7e13c87db20de968b59e7f297", "a456265138c088a894301c0433dae938705a9bec", "222928303a72d1389b0add8032a31abccbba41b3", "d6b5baeaee6b5266bc2a3918f400fe831837376d", "f09f7888aa5aeaf88a2a44aea768d9a8747e97d2", "36eff562f65125511b5dfab68ce7f7a943c27478", "bfbabf38618501614b73cb09c5ddfe10c0250998", "ae42c0cff384495683192b06bd985cdd7a54632a", "f6e90c5209f486468b2a080de530b98a30e3ff3c", "dc96f46f400eae1881a5ec13aa6afc37ee6910a7", "7c6de5a9e02a779e24504619050c6118f4eac181", "18b47b83a373f33d6b902a3615f42c10f7600d72", "1845a560ef99f127418aa42acdd737a1975875b6", "5d1bfeed240709725c78bc72ea40e55410b373dc", "ede7748b66214f615ed8dc97ffd455d4342ca7e2", "00d736c540f80582279093cfc5ffe454a3226da9", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "a743115795375666593919cbd48590213c229ae9", "db80c210e25c47e2eeb89ec24a4578ebfd791666", "5255dab44692c7778685e7e5d7c6727a63b0c798", "b02aba65fb2792cee4fb0cbae12858195f383764", "20460b11db27bb41b3504044c334cb5b1e42c9b5", "2c846c5ac7e8bd020b89f6c4767a01731114ee4d", "7e1874986cf6433fabf96fff93ef42b60bdc49f8", "9b44a53069dcf4dc10c34e456bf3a5ff160d3ede", "7a10f6a406b664d1159e7c4fefbdd6ac275aee53", "d1b796ff0c1895426de11a1eaafc5443be29645d", "18d49975ff47f2ca499d7535a3dbae87ff46544f", "eaf320bb262c2e07662716cb8c9dc9a95b483ce1", "a7fc751cd95bd1a409a26daaef69fc3aa8a35e0e", "86775225af9de81c2797de86856cdf356faeb500", "3b2d59bc24e999d5800d63ae5e51dc40a8905aeb", "f655d41dcec06ab99c6377c6d426e486141caaa5", "12e70b3321051335e0ea9dfd6c7af7ad77986701", "87507a498558ed6ed23115a42f42376c0884f7f2", "0e90a9ae528f1a6d813126739c4227912d545643", "84ea9e676647daa5eaa5fbc0ed80d8cd5463d6dc", "4023f78148820ed636d755f9b5efd67c5aa79b5d", "2626eb1d24c1b9a4089afe02f325a8494d2c2a3a", "a13050d0a10ea6c1a536e3ceec069639123fea3d", "647fdaacfcea319677175464b7db39e3b22c7808", "894b7897dc2143c5e8db666766cf59637a7c7b82", "ad40b25e38314f39a82f193dc4806e6a1c2c6b69", "2a005868b79511cf8c924cd5990e2497527a0527", "110fc89934aa58f59580adecd1f317c4a3e75f7e", "7a395ecfbef85ecd9592d4ab4659fab8b45c18ac", "82811474ca5f77fa68d79804c2fde5053a407c63", "f1c11a6c7fa456c33d16b958841d7d34a039c379", "52288861e285861a707782e07b265e8fbdcfd2b6", "a4b650d68fa2a15db5307b70d5f5fb7dc3e29569", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "11cf4bea8ffde8225eaea72e71b4c8497f240d72", "f33372a2ddd265425ea60a2889f8f6eb6edfce13", "114ea414b025a68d641efad9b74295a5625b9e7e", "ed9b4ff3d634a448747b7d51f56abc81ef9d0054", "065066a94860279587ecc7c7caaa65303008940f", "75e5b98d3b53ae0bd6b76709841aa6d0f05ecbfe", "a2e593cf07b58d074fd672348905c84d53b7dcab", "85bc28db35038e5f7313767b22ad301bccad4470", "2b55d6997133251c0aa6807bfd665ea9d388cc73"], "url": "https://www.semanticscholar.org/paper/659597b1699ba5b73da9a8628bf7e4ad9bebd242"}, "2435ffb8ed3212156d6b6f19f633a861399cf30e": {"id": "2435ffb8ed3212156d6b6f19f633a861399cf30e", "title": "Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition", "authors": [{"authorId": "3298532", "name": "Qibin Hou", "paperCount": 61, "citationCount": 4514, "hIndex": 29}, {"authorId": "145062296", "name": "Zihang Jiang", "paperCount": 14, "citationCount": 369, "hIndex": 7}, {"authorId": "2087091296", "name": "Li Yuan", "paperCount": 18, "citationCount": 1784, "hIndex": 13}, {"authorId": "1557350184", "name": "Mingg-Ming Cheng", "paperCount": 29, "citationCount": 323, "hIndex": 10}, {"authorId": "143653681", "name": "Shuicheng Yan", "paperCount": 782, "citationCount": 66630, "hIndex": 121}, {"authorId": "1698982", "name": "Jiashi Feng", "paperCount": 77, "citationCount": 1386, "hIndex": 20}], "abstract": "In this paper, we present Vision Permutator, a conceptually simple and data efficient MLP-like architecture for visual recognition. By realizing the importance of the positional information carried by 2D feature representations, unlike recent MLP-like models that encode the spatial information along the flattened spatial dimensions, Vision Permutator separately encodes the feature representations along the height and width dimensions with linear projections. This allows Vision Permutator to capture long-range dependencies and meanwhile avoid the attention building process in transformers. The outputs are then aggregated in a mutually complementing manner to form expressive representations. We show that our Vision Permutators are formidable competitors to convolutional neural networks (CNNs) and vision transformers. Without the dependence on spatial convolutions or attention mechanisms, Vision Permutator achieves 81.5% top-1 accuracy on ImageNet without extra large-scale training data (e.g., ImageNet-22k) using only 25M learnable parameters, which is much better than most CNNs and vision transformers under the same model size constraint. When scaling up to 88M, it attains 83.2% top-1 accuracy, greatly improving the performance of recent state-of-the-art MLP-like networks for visual recognition. We hope this work could encourage research on rethinking the way of encoding spatial information and facilitate the development of MLP-like models. PyTorch/MindSpore/Jittor code is available at https://github.com/Andrew-Qibin/VisionPermutator.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 53, "citation_count": 64, "influential_paper_citations": 19, "is_open_access": true, "citations": ["1fb10189c500e4902cd1b5afd406f57323d21be8", "f75cddf2d42ed01b34686704eb3504becef67442", "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "3425495ee3b6ead009f35aeb70edeac4e6eb2d10", "71363797140647ebb3f540584de0a8758d2f7aa2", "60707f6d2bffeab09e8f1d073fce4fc06ab89ec1", "fd547648ded5dd4c45a3594b398844876d93c339", "b476c932e959cfe645911786f1a070c70b5375c6", "58970a426b687bb080b7fed3b4b78ab1ebaa56f4", "fa717a2e31f0cef4e26921f3b147a98644d2e64c"], "references": ["1fb10189c500e4902cd1b5afd406f57323d21be8", "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "a7266bc2229ef8da4baf769eab925646601dfaad", "0fc190033ec2832ed65dfac7a19bdb8a270fb6eb", "8602fd5b0ac73bb422f238b265479f363c0ffe61", "e3a3e85c5a32af29e13b3561f6cf070de70651de", "2def61f556f9a5576ace08911496b7c7e4f970a4", "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "e775e649d815a02373eac840cf5e33a04ff85c95", "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "96da196d6f8c947db03d13759f030642f8234abf", "70cf7c785952375e8061c92235aa20e94b02ecd4", "0ae67202f0584afccefa770865d14a46655d2975", "c16835c8e535ebd9c10a550ca9455fe384a14449", "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "ad7ddcc14984caae308c397f1a589aae75d4ab71", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "279696f90d13b1327ec7adb73e711e7d8f5db761", "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "f5c8464032a936451b222be1984cabf42d6adfa8", "d920e0710d2a1ae966661d0513b817b6b81dc2b2", "2709167f1c3a03fa5b970a665ea48ed243aab582", "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "6bbd51697c25493ea15f4cac830e28eeac143898", "df67d46e78aae0d2fccfb6212d101a342259c01b", "2788a2461ed0067e2f7aaa63c449a24a237ec341", "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "366244acdd930e488ae224ab6e2a92dc24aa7e06", "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "27ac832ee83d8b5386917998a171a0257e2151e2", "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "29309743870c825f9645a4803af727402462e513", "d07284a6811f1b2745d91bdb06b040b57f226882", "b7339c1deeb617c894cc08c92ed8c2d4ab14b4b5", "8899094797e82c5c185a0893896320ef77f60e64", "4feef0fd284feb1233399b400eb897f59ec92755", "05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "1c4e9156ca07705531e45960b7a919dc473abb51", "51db1f3c8dfc7d4077da39c96bb90a6358128111", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "d2c733e34d48784a37d717fe43d9e93277a8c53e"], "url": "https://www.semanticscholar.org/paper/2435ffb8ed3212156d6b6f19f633a861399cf30e"}, "44732fe24025c2f17777028bb49bb4a09c31af2a": {"id": "44732fe24025c2f17777028bb49bb4a09c31af2a", "title": "Split Computing and Early Exiting for Deep Learning Applications: Survey and Research Challenges", "authors": [{"authorId": "8837664", "name": "Yoshitomo Matsubara", "paperCount": 21, "citationCount": 335, "hIndex": 8}, {"authorId": "1737640", "name": "M. Levorato", "paperCount": 165, "citationCount": 2427, "hIndex": 26}, {"authorId": "145825175", "name": "Francesco Restuccia", "paperCount": 88, "citationCount": 1486, "hIndex": 20}], "abstract": "Mobile devices such as smartphones and autonomous vehicles increasingly rely on deep neural networks (DNNs) to execute complex inference tasks such as image classification and speech recognition, among others. However, continuously executing the entire DNN on mobile devices can quickly deplete their battery. Although task offloading to cloud/edge servers may decrease the mobile device\u2019s computational burden, erratic patterns in channel quality, network, and edge server load can lead to a significant delay in task execution. Recently, approaches based on split computing (SC) have been proposed, where the DNN is split into a head and a tail model, executed respectively on the mobile device and on the edge server. Ultimately, this may reduce bandwidth usage as well as energy consumption. Another approach, called early exiting (EE), trains models to embed multiple \u201cexits\u201d earlier in the architecture, each providing increasingly higher target accuracy. Therefore, the tradeoff between accuracy and delay can be tuned according to the current conditions or application demands. In this article, we provide a comprehensive survey of the state of the art in SC and EE strategies by presenting a comparison of the most relevant approaches. We conclude the article by providing a set of compelling research challenges.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 182, "citation_count": 28, "influential_paper_citations": 3, "is_open_access": true, "citations": ["83bc8dee18605d0abf981496e785a261b7e1a34f", "38b901714bc97009f7e4f1078967efc6ac2c176a", "be73eb7d3da8ff36621f9f86c0a96b5dfa129cc3", "13f07349ae44483e7f1177ab3939a5f0ee82503e", "015411a9eb69cfc860c5aaf7ca6de955cf8d21cb", "f84ca210279922d3578b579e79a3905699004244", "47993d0dcca76d194cf1c2382f5f4e3c9207f823", "d6b914715c5a88c92fd112107a7cf1654b11e40f", "8bb1104b4c87c627197f0988b160bcaaa9dddf6f", "5169a986f8f30418f239372e6f9b59de832aaac2"], "references": ["9e7e1d962ab25ae00e6fb5713116967e9b64be50", "94a212a595f1ae16a18539fcb7d1e6d4225cffdb", "db1f7dfd849d962da510b2557e5d21f1b66a67dc", "937d868b52cb5a113cbe7f0f60e95c92ac663b5d", "0ffa39275a7680f86586d053bed326d8bcf27ec3", "999cf469e6795160894a97bb868363c5dd3e081a", "ed0c8f60dca1ac99a7ea94955b2096a177613d12", "8cd2eec36ab5d2d761ef86404181f7b9f18bec3f", "857a1beabfcec53fa6f9903d032c07e3b61de09c", "212e2272a662e1edcddb77462a0a8fdeff49c03b", "1c6c090601afa26f0552884bee0f02bae70a53a7", "883e8419203c5763e2eff0f2d4d4663adf2ef25d", "cf5e6e3c50a798d87033e0e108e88b3647738bbe", "4f5948dc8eeffa43e5af9d37ba0d9a238bfd9f07", "bfe00983e4566c7195a7e6bd1bf41fffb98c9f37", "5630b9dd7dd0539d105e0983862bae6592be9078", "3db9031ce4465fd8cab23910099d99f483718a81", "d7e786739735f3107f4817b4382248e80a89ef1d", "cbab055052c6098b21c210e945ca17df143273da", "cbde5598c1a78285adfcfd77fb3636f5498987a0", "dc4e3147e4455c0387f66f7938c0703b67b6074d", "b046cabb3e74439fff3118f078120675434d26bc", "f84bb7872052b0cc094d0750501635b466268bea", "f0c962a28e9b988140048c25a912a4fc02f38cdb", "f5f3c514f2f5be4604c1b3536c9c5dde00a081e6", "0792978229f7ad7145c40defdd2e9f13a0a77b43", "070f7d56bdec6bd31f87dfb28a6a3bc557a5a082", "5badfb6880f219cf9973ed52b63af604faf12ac9", "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8", "4ad9b582450d564be2a69f2f812b5c44306846ae", "afbb3550da652f4eabaf88e171c68c46ac14a98b", "c25ba10bd770c44b0411699507f94b704885bef6", "90a1491ac32e732c93773354e4e665794ed4d490", "6dc940a670d5fb818ca9afaad4b477a2977a7783", "5d34881ff68bd203ff790187e7e5c9e034389cfa", "fb22ef53086d236345a5690a6cfd89dce009afc8", "2709167f1c3a03fa5b970a665ea48ed243aab582", "756810258e3419af76aff38c895c20343b0602d0", "77dd380d7f7f1eb7b8b11624a89f774eb3cb1ddc", "a7e22174bd50390ccddc1fbebbe2fa90b4c56962", "2356781b8a98bf94e6fc73798c6cb65ac35e5f97", "60402c13d83ebcf80fca70895ffb7f217cd22804", "9c321af4fab3f40d8d9d5254dc9c26f21a188e88", "41c67d04be2d1632c0d3b0880c21c9fe797cdab8", "c12e6c65e1de5d3993c5b65d0e234ae1f60c85ae", "4b49329c48e78a0acb9c0e79e9a9df976088e680", "028e2e896eaf6f8b8fb9bcba2490dfd58be21221", "af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2", "0087d72c72fe3098393f7bbb92032c38a78c43ae", "7a064df1aeada7e69e5173f7d4c8606f4470365b", "ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2", "53e36eb4ead1999146601db500d85faed219e8a9", "bc6dfc6bda2d929fec91042dce1831fd07999b39", "72564a69bf339ff1d16a639c86a764db2321caab", "022dd244f2e25525eb37e9dda51abb9cd8ca8c30", "dca73526dcb51428af31c950414a34b1754ba260", "db87b4cd5e5fcb050dce07531f177434a79a848f", "544e39cb689e937d8954f139366346c45ac08dd9", "b0911b52515445b689ce0d7a517dd22b1d89e675", "671108c0a8c468ea5e8605f00a3572e8c6db183d", "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "4f2d4e821dd03ac5df7d5448948bc738aefdd6db", "e192d74e8713adc74aefcf6da3349d553caf50ae", "83d074cc5051ade0c08d66180e4a04d2c112fa97", "2cdb923930e70136084397ce7891f584685912fd", "d287718c31c30433857358f0073acf3138276125", "08f06907ea1a810d0acd2affaa539fc60b361ff3", "65c47c35495f472bad1c088dfd1e15c304786634", "693c97ecedb0a84539b7162c95e89fa3cd84ca73", "0ea07f0b14550cfbb9f55ab5f00ac652a4efc8f5", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "7d668cf7cabcca5ddeb33cb40d52953f50d63a18", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "0b99b67a0b9dead453db1c1c917188801854b7f5", "638a7d74ea0d76440db2c6bf4c59fc8ce31f29c0", "3e804ac49b99c6509a65ba81d11ba911bac81596", "fb1fefa1c9a84b86b43af090598266db7f2c47c4", "8563b6545a8ff8d17a74da1f70f57c4a7d9a38bc", "4939fdf171ceacb8fb85df2595d3de26c1d397ee", "151c7cb45c75ac5573a6c03b2ea089b34512898c", "93b8da28d006415866bf48f9a6e06b5242129195", "d16b21f3e99171c86365679435f9f03766750639", "e4845fb1e624965d4f036d7fd32e8dcdd2408148", "0a255e716a89b787336ab956f0aa74424629c950", "1a11de12480356b25b92c956db7ec126f2d3da9e", "678c5b1771e7c15e86b454662dec8cd45d3d30bf", "00c4632d2d926acabc18574c9c5b870709ae9450", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "f257e3ac714cd8fcd3b22d7d27ac6fab2db34097", "59d0d7ccec2db66cad20cac5721ce54a8a058294", "d0611891b9e8a7c5731146097b6f201578f47b2f", "c67fd991fdcea8c89be2ad632ea8476e07a24edc", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "125ccd810f43f1cba83c6681836d000f83d1886d", "39e8d4e695dac335c4c4c75fd2354568726aeafd", "d68006c3761563ed6382b3b092da0b2f79a68c73", "c27db32efa8137cbf654902f8f728f338e55cd1c", "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "ee53c9480132fc0d09b1192226cb2c460462fd6d", "ee4a012a4b12d11d7ab8c0e79c61e807927a163c", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "da773af7e4f1248f47e6057eabcb595b3997eac9", "3647d6d0f151dc05626449ee09cc7bce55be497e", "51871d01c26acb651c81adaf073c32c3d9ec0f0b", "c269858a7bb34e8350f2442ccf37797856ae9bca", "8c5293da3ad1a463cb9694edfbf1bf19b8cbd698", "7d39d69b23424446f0400ef603b2e3e22d0309d6", "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36", "3ac1df952ffb63abb4231a4410f6f8375ccdfe79", "232148b97bd0543613ffd98fb4edcff79434ce1a", "67d968c7450878190e45ac7886746de867bf673d", "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "5694e46284460a648fe29117cbc55f6c9be3fa3c", "4eb2e27ee90d53d86a5cf4c90bfb444cf7bdc833", "b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b", "896de8418884f4aab1ae4a60027500c9e8baffc3", "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "dc594e93ad936172e28f1e776228bb50406efa06", "57a10537978600fd33dcdd48922c791609a4851a", "05dd7254b632376973f3a1b4d39485da17814df5", "1c4e9156ca07705531e45960b7a919dc473abb51", "e0cb07ef9086985c6a8bfec377f1ce1ff1eac5af", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "642d0f49b7826adcf986616f4af77e736229990f", "4f8d648c52edf74e41b0996128aa536e13cc7e82", "f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "ee21abcd21e010b53f2c6c21ba2bdd53abdcc40a", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "34038d9424ce602d7ac917a4e582d977725d4393", "753bb416c4ec90031d3eaf238e4b6f53645defac", "0c908739fbff75f03469d13d4a1a07de3414ee19", "415229903f91a1f3fc7404f5e5997fde025c221d", "4d376d6978dad0374edfa6709c9556b42d3594d3", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "24741d280869ad9c60321f5ab6e5f01b7852507d", "9f2efadf66817f1b38f58b3f50c7c8f34c69d89a", "71b7178df5d2b112d07e45038cb5637208659ff7", "6a80eca61a2f4282998c78ff33cb9dfb159296b8", "1109b663453e78a59e4f66446d71720ac58cec25", "d770060812fb646b3846a7d398a3066145b5e3c8", "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "8d25d04051074be7590cbe5e4e34c45bb26674e1", "34f25a8704614163c4095b3ee2fc969b60de4698", "2319a491378867c7049b3da055c5df60e1671158", "687bac2d3320083eb4530bf18bb8f8f721477600", "eb9243a3b98a819539ad57b7b4f05b969510d075", "ef4261a7fb3f3017e6de97057ae4f651d9863b5e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "31868290adf1c000c611dfc966b514d5a34e8d23", "7b8ace072475a9a42d6ceb293c8b4a8c9b573284", "128cb6b891aee1b5df099acb48e2efecfcff689f", "bc1022b031dc6c7019696492e8116598097a8c12", "3a1a2cff2b70fb84a7ca7d97f8adcc5855851795", "02227c94dd41fe0b439e050d377b0beb5d427cda", "a538b05ebb01a40323997629e171c91aa28b8e2f", "5d90f06bb70a0a3dced62413346235c02b1aa086", "db8885a0037fe47d973ade79d696586453710233", "5ab6ddd1d45302bf635cce5cb93fbaf4ea79458a", "b2815bc4c9e4260227cd7ca0c9d68d41c4c2f58b", "30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9", "812355cec91fa30bb50e9e992a3549af39e4f6eb", "0ec48ac86456cea3d6d6172ca81ef68e98b21a61", "136326377c122560768db674e35f5bcd6de3bc40", "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "cd950dad6521e8eb32a7636c70b306a04bae750a", "475354f10798f110d34792b6d88f31d6d5cb099e", "c76c62c5ab6c076a80f925d277ef04dd36f6bf9c", "162d958ff885f1462aeda91cd72582323fd6a1f4"], "url": "https://www.semanticscholar.org/paper/44732fe24025c2f17777028bb49bb4a09c31af2a"}, "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec": {"id": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "title": "Robust Speech Recognition via Large-Scale Weak Supervision", "authors": [{"authorId": "38909097", "name": "Alec Radford", "paperCount": 34, "citationCount": 52296, "hIndex": 24}, {"authorId": "2110935237", "name": "Jong Wook Kim", "paperCount": 13, "citationCount": 3545, "hIndex": 10}, {"authorId": "2118717067", "name": "Tao Xu", "paperCount": 7, "citationCount": 126, "hIndex": 6}, {"authorId": "2065151121", "name": "Greg Brockman", "paperCount": 5, "citationCount": 4305, "hIndex": 4}, {"authorId": "3028785", "name": "C. McLeavey", "paperCount": 9, "citationCount": 1003, "hIndex": 5}, {"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}], "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing. the decoder, allowing for a single model to replace many different stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets, as further explained in Section 2.3.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2022, "reference_count": 96, "citation_count": 18, "influential_paper_citations": 3, "is_open_access": false, "citations": ["16de2006e2960ba410772c6b6d460b83c0a5cc4b", "4dbf7779825bb92083eed0c66096dde0003269fa", "43ea2737ab4e4843e0bbc66b81ff1d7d00da664e", "82788cdad52d1cf7603879ad9d60d6afdfee5fbd", "89beaeacb16e0ee4bbd8fe38a6c3b86de0d3373a", "ef4fb433642567a8b07c99b8edc98b708ba79797", "1884ebac2db0711bc42c9eb45dd35e6ae7dcb2f0", "f621a37afde95fcc9afe600a49e90adcfbc2d6fe", "50b67709507cbaef315ce6c6085aa62c386d1f28", "60b63e694bce313c423c5abc661fa204f02e969e"], "references": ["7c73fe9266db0b35f551645ca37c6e7ce0980c4b", "1a65b38aaa72da05023d7d88fe1b88608b37b5fc", "1ce67b5555c123ae1efb710965567f51bf764423", "19dbb57ad106137553bff4282149ac2800b5c176", "e871d035824f24f2f82be58f636451214d5b5a71", "6fe21b01d2202defb8fcd75c40f306a88bd385dc", "de1fdaf92488f2f33ddc0272628c8543778d0da9", "69f082216612b20fc701cb9e203ee8717c195f3b", "85a71f9a7e916ab5a53acff9bc9b547434f6c838", "6f1ca0249eafa36a5762ac53f6ba2a4ee2133456", "3a223a5174717a193bb7e22c825d801273ad7cb2", "5f4d69a483afe38a44ce53909a24a5535dcbf638", "2a8fa407e074bebeaf1e254be37fae7fc54610e3", "5f769c5df8de29d0a2cd9c020f78047013a87b34", "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "6da74c5b1b3cffc236c4b2d75ac46b767f327e62", "b5044990de96ae16ab842bb0f404408643a55986", "a7fa3497481c400bf2e7856cfe284717e07ef66a", "bb227021ad3891c088197d0a81421b44fc72cdc8", "62ce4d65335c32844247e0ecf3be6de1ccb924b2", "fef7e29b43e838a0e4bce39add6361c7bcfeb456", "2fb4522a8560c8778c2a4e8a94690a24b9c4450a", "3fb85a349adc758a0c72a96fb0065765ac3b0945", "4affd37c7749b2b6f2e52f49e4dce00f24dc6d8d", "569ef4e3c9f5ae968fc94000c12d212a2b679907", "49a049dc85e2380dde80501a984878341dd8efdf", "024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb", "a8026ad09d1b31987b66ecd35efbb72676fb9cb4", "2fe6759b0e9757df70ca3db1e1dd9bd1c5a5bda5", "ead6323f137c2f99ef0ffcfa34fa6eb1c6eca3c6", "1b04936c2599e59b120f743fbb30df2eed3fd782", "97f08c1ae8ca5ddf5948c66bfbbc0546ac154807", "219b7266ae848937da170c5510b2bfc66d17859a", "0495d9df8eb84dcdab4e5536179823cd26279949", "63a71de0dafc90910e37a2b07169ff486d9b5fe5", "d023e4c652dc21b2068a8527203a70d9eaf195d9", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "f0d35b37fec26c3f1ed09253cbb9304fb62208d1", "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "3f5f0899751f4222217e1b0c39c5ee8eac527f5c", "21da617a0f79aabf94272107184606cefe90ab75", "b0fae9fbb4e580d92395eabafe73e317ae6510e3", "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "207c073e427ff50b72a3f53975f5c6251551c4cb", "d07284a6811f1b2745d91bdb06b040b57f226882", "639174f32a71ecfe9041ad05ff30eb39bd4977bf", "9405cc0d6169988371b2755e573cc28650d14dfe", "eedef63cc160fe9138b5be900243d5907537d55c", "9784fbf77295860b2e412137b86356d70b25e3c0", "c1f39fe16ca72d73170e74fe6920cb553d580766", "0f885fd46064d271d4404cf9bb3d758e1a6f8d55", "2561e613038865e8904e34f021c8dd9760e7dfb4", "ffb949d3493c3b2f3c9acf9c75cb03938933ddf0", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "a486e2839291111bb44fa1f07731ada123539f75", "63e39cdf1ad884da6bc69096bb3413b5b1100559", "15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7", "942deb7d865b7782c03176d95e3a0d56cb71009e", "7260c0692f8d265e11c4e9c4c8ef4c185bd587ad", "51db1f3c8dfc7d4077da39c96bb90a6358128111", "8ff840a40d3f1557c55c19d4d636da77103168ce", "d76c07211479e233f7c6a6f32d5346c983c5598f", "1af68821518f03568f913ab03fc02080247a27ff", "34038d9424ce602d7ac917a4e582d977725d4393", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "cea967b59209c6be22829699f05b8b1ac4dc092d", "34f25a8704614163c4095b3ee2fc969b60de4698", "1bc084e123794ca0c89798dfccd3a6b02b0af3e0", "84069287da0a6b488b8c933f3cb5be759cb6237e", "6de74028d2af14b09879abfdb3824f64cc24bed2", "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9", "0302bb2d5476540cfb21467473f5eca843caf90b", "168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "bc1022b031dc6c7019696492e8116598097a8c12", "f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "c0d3d9a7b1a69428d71c15a3d5f6866fb312fdac", "161ffb54a3fdf0715b198bb57bd22f910242eb49", "766cd91c0d8650495529cab7d4eeed482729cf89", "6dc61f37ecc552413606d8c89ffbc46ec98ed887"], "url": "https://www.semanticscholar.org/paper/a02fbaf22237a1aedacb1320b6007cd70c1fe6ec"}, "f83382307aa3e55243da15ea17d7c39720fa1708": {"id": "f83382307aa3e55243da15ea17d7c39720fa1708", "title": "PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-Step Point Moving Paths", "authors": [{"authorId": "2069706514", "name": "Xin Wen", "paperCount": 19, "citationCount": 471, "hIndex": 10}, {"authorId": "2066322275", "name": "Peng Xiang", "paperCount": 8, "citationCount": 167, "hIndex": 4}, {"authorId": "2157429937", "name": "Yaru Cao", "paperCount": 7, "citationCount": 65, "hIndex": 5}, {"authorId": "37124370", "name": "Pengfei Wan", "paperCount": 46, "citationCount": 593, "hIndex": 13}, {"authorId": "2152934280", "name": "Wen Zheng", "paperCount": 20, "citationCount": 478, "hIndex": 9}, {"authorId": "46399275", "name": "Yu-Shen Liu", "paperCount": 72, "citationCount": 2005, "hIndex": 27}], "abstract": "Point cloud completion concerns to predict missing part for incomplete 3D shapes. A common strategy is to generate complete shape according to incomplete input. However, unordered nature of point clouds will degrade generation of high-quality 3D shapes, as detailed topology and structure of unordered points are hard to be captured during the generative process using an extracted latent code. We address this problem by formulating completion as point cloud deformation process. Specifically, we design a novel neural network, named PMP-Net++, to mimic behavior of an earth mover. It moves each point of incomplete input to obtain a complete point cloud, where total distance of point moving paths (PMPs) should be the shortest. Therefore, PMP-Net++ predicts unique PMP for each point according to constraint of point moving distances. The network learns a strict and unique correspondence on point-level, and thus improves quality of predicted complete shape. Moreover, since moving points heavily relies on per-point features learned by network, we further introduce a transformer-enhanced representation learning network, which significantly improves completion performance of PMP-Net++. We conduct comprehensive experiments in shape completion, and further explore application on point cloud up-sampling, which demonstrate non-trivial improvement of PMP-Net++ over state-of-the-art point cloud completion/up-sampling methods.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2022, "reference_count": 61, "citation_count": 16, "influential_paper_citations": 2, "is_open_access": true, "citations": ["022b3d5f684dd6e1b74e0455b5b78a3986c8b69f", "b38a2bcbf8d3b06668ca4b11f42f4950f51c0293", "cb4114cb59acbabd78a2ee0a381e4fc4fa178133", "ede618392b52947f4103415c97e444c20697e550", "2d595a9faec9ee81d48c1bef4dca10f86cbff8ea", "b7fc576ba36575c091b072026e4b1bea5d9b2f8a", "4cb9f3171506dee629fdf0293e76aa937d6f8a84", "9bc64c4518722b994e814dda0c96642955e899ef", "23e0e88c5abcda4b363f96e3d639ac43a85f5e7e", "4c6e378a93a8277666ad02e6caa67be4eecd96f8"], "references": ["34e170f868551d35788099ce02c3c2d03e06f0d5", "5e9a9710674de21b00ea7ca32efe9a82c7ce1b88", "b8ea0e14177d018b0a5cfd525235afc451bd072e", "e1d082562981a9f51649c60663aa484ee623dbb0", "deeb1258396d281bdcdaded26f1f1a19973f2ad2", "ce7b2ebd5406d890a351b7b4a3ab3e73a381facd", "a1cfa05b3d492d004f65ef7951fe58e6e21d51f8", "9437a7cd9bcaaafe2bc6b8ee7592f0748f3769c2", "03c322125804e670e82fff4be561561976c07515", "b91b7b20f910a97ea4dbd1ea7e78e5e115bc5f31", "ed28a3b3e4fe4b02512a3612186d9759d5197b1b", "c41a89d744afcdc91cc7b9f05d2abb438ed200c1", "477a1de90ba8f22e4f4068ff8d6233afe74db936", "3ad9bb934a144ab66f4c89fe17b9606b3e6cb2b7", "0cc5c14c3636acab524b330668799a55a8421b9f", "9712cc6fc96f463842f9d41c565e3a8781bfba40", "b624bc846e9a2228f611633cd1818d1d9039a0b8", "00e0adb2e5a09588d779bd887db800d7cea87e0f", "0b9476177e70d281d4a52aa60809b6a15d2a7523", "f591c4869999f7d14095e176cb6ef72d2b904149", "d70de096619ca477a211e2db93e3f1ff66a9812c", "3cdca7b8b18c936f07954e77838e60de72ee668a", "3f2c6e5073a79a72a82c598bdd31bec670bd756a", "d602759a888f23494dd4bcc4c3573fc68bce6c4a", "119bb19d6a6fc34eaf4e517d0acc102a7cbb4d65", "b6cf10713451191de8f0d30211f85a1080249a74", "35aca5a69e6368843eeffd3fd0de1d15cfdf7cd0", "1255adda844a83e072c561e573988b828ca9d4f5", "d8ba2965be9cc93439c5bb52961b504c39f16e80", "5b0a5d51e5d35b63ef0dc9e135cacba21a5916cd", "5be337b3715e619075c2855900707b42f9f61c28", "5a358f077d6346fa22f464c80bfb876ab439d495", "c92c70dbfc2907350a2c547d6126da783aa5e1b6", "d33949030c7be933ee83768e4dfa1ed387cdc12b", "8f9b1c2264834f7ec6c6913c68dca7fd67ea9859", "41edd975efbbc5af28971bc2669d9a1ae6ee8ace", "1335190de78e0b58fbf7a7c7c52fa1a15f14858a", "2a417a16473e2bcb1c98cd7814bc106760925e60", "840e804bb5ed1944e494959c2980a90bea0675c4", "6b0bbf3e7df725cc3b781d2648e41782cb3d8539", "c82ca047a80dbdf10ec6a92f860c0c40035c6517", "572a5aa00f0569887469ffb7554699c21156ba0b", "316c5f0f260b25ee3db5667eec7ec4605e0c6754", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "8674494bd7a076286b905912d26d47f7501c4046", "d997beefc0922d97202789d2ac307c55c2c52fba", "db20b07036ef078aaa4a0068dd01f1fa5605b002", "8a3bf4d403a39ed33f0fa8cf78dc906d6130595f", "6295a9bfc52275a4fdb6037b2f67d4d9cccb8c12", "097a216d6e1e4f67327ad6d9004b81036cbf5186", "089a5be16cdf1bdc0f26c73cb0ec9494d9e4fe3f", "61bd7a2a7b1083e3bc106a5b0408f46da1cd1204", "9aa2abf7829de849628ffaad322153794ac99c3f", "b23f6d72614e85c9bb76329853fa7d0952437c59", "c15d9bb03260c30b5c0fc4ed58abab5fef0e1614", "0e870469f332b3f787559b1ecc54909e41307d73", "d13a04844e4a781e5180987118f732d93aa9f398"], "url": "https://www.semanticscholar.org/paper/f83382307aa3e55243da15ea17d7c39720fa1708"}, "1c83f3f9789df43bf937ae2618721e2da83dcc06": {"id": "1c83f3f9789df43bf937ae2618721e2da83dcc06", "title": "From Show to Tell: A Survey on Deep Learning-Based Image Captioning", "authors": [{"authorId": "2054511289", "name": "Matteo Stefanini", "paperCount": 14, "citationCount": 507, "hIndex": 6}, {"authorId": "3468983", "name": "Marcella Cornia", "paperCount": 60, "citationCount": 1769, "hIndex": 17}, {"authorId": "1843795", "name": "L. Baraldi", "paperCount": 99, "citationCount": 2584, "hIndex": 23}, {"authorId": "3492481", "name": "S. Cascianelli", "paperCount": 47, "citationCount": 334, "hIndex": 10}, {"authorId": "3144258", "name": "G. Fiameni", "paperCount": 43, "citationCount": 240, "hIndex": 7}, {"authorId": "1741922", "name": "R. Cucchiara", "paperCount": 538, "citationCount": 18012, "hIndex": 55}], "abstract": "Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 254, "citation_count": 22, "influential_paper_citations": 2, "is_open_access": true, "citations": ["959e1ff94f7775f02a53a54a9939b28006882084", "b105bb0a55715160ef699f365bef0c7df69a1189", "4de94949daf9bc8dd0e5161d20dfe83198d20ec1", "3882d8c04a7ac5fed8b7979cad17c6c8400506b0", "ab22defea0cfa99d1e40e9ba406426884a2f846a", "74d8a4bf39ec57505f1780a616bb4f82d79bcf23", "5e6f3782d32ab0a54deed567315b9a627f750c6f", "5797921b076b73b1932efc45191e911e2f7a764f", "027431caa701b27fb2d2326aca181ef867dd6e68", "235ef5a58ee7e2bf75660883d3c51798aa859339"], "references": ["3ea60cbce6c9065661d207fccf021c5d58a83f01", "59600d21fc9d5e71bde2e9deeb23620f8bf15c47", "5e00596fa946670d894b1bdaeff5a98e3867ef13", "8f167ec1149921fac63b1ea855443de109bb013a", "003f4387c5afacec9a30b0c68d8fc84daf6c6334", "ba341f992ceb10d9a7f032ac6027f18ef5e5f895", "d0134f63879cedf3cdfe795bd2fd7c48d9554e4a", "83e094f4661d12fbf598268838984df09e7a2d0d", "a7aa150b55d64d339b1c154d6d88455fc3cbc44f", "3a8637c0d93bab1f651003a895335b8a71149c0c", "c27fc330b8ec901eda436ac00bd70fec78907b64", "9b10c49575c5aa07b5076d2f5b27c67b68b09c9b", "389b98518980f218cdc0869fd852428686eef6dd", "75decd34aeed7057ab3962be5310ec2faa7ec9ba", "63c74d15940af1af9b386b5762e4445e54c73719", "8888c82729866b4fd9692461f86ddace104cebc1", "afa76fedf8701e057b2bf7a228bf41980ac2d1c9", "0d806dcbb1c14bea8139f21290358c483f208b18", "c1d41e75bd795e21984452d276af4721fa2beb21", "279925dbe2ebe4e38d6cfa646f726aa8d6d9a122", "84bab90970575f07eb2380453b51191f3aff6fcd", "7ba268c6d5489dd3b3c08e3642f1385c6235118e", "2d7e4b2c3d69e9d3af12f4e1c90cab1dab4e1776", "17e695d7b00600e0fd6599e1d7703d9f76e796e8", "38b0567e83386ddc294d6c81b541deacbd8e3c2a", "ada35e2c099fbde9d07a279311f4abe698341cd8", "86a0f09ca46e40a763bf94e5c405d3f09e48c953", "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "394be105b87e9bfe72c20efe6338de10604e1a11", "141a5033d9994242b18bb3b217e79582f1ee9306", "cc03c6a4160ad133faeb1b6c49d24432c7a896aa", "49e78a41450af8b03576eb3478e009b8e4de062c", "ae7e5a4de962ca4face3bb52b36dfd09db5451d8", "ad7ddcc14984caae308c397f1a589aae75d4ab71", "7217b5d8d0fb753532026cc36b0aaa056960c6f8", "952edddbb3438072312756762be1bfde287e1497", "8deceb13cb3afcfbaab06a2c655f1935445635fe", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "60b508a92f73f7e7f14380b45f75d5f0df9548fe", "9ffc4fa1db8372d763f990a1f0a6985260d693b0", "3e86f5a0e2a97894de1cf1f1587799ac79bad0f2", "1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0", "45c66df9952c147d2c4929453e825cb414d51097", "277f50c4c1c49d9df459f432f0ac38c6aeff8610", "7d805e35f17dab7382748130a2ca1bda629cdceb", "2be4e374800a0db69695eb4c558a6653dd258fcd", "2e4ca3d95ffb83870661dd66deee143e782f0706", "021d50ba5ae1c66e9175428f546976798126dd9f", "0bcec12a99cfcd649ea36d6b7215d025bad12974", "b41882903384ef849688a325d747fdaad8ecee82", "81be56a5783552d5b32463b392ff0499dd86a5ab", "eb5f7e1244bde88aa45559a3ac9b5274ba57b78a", "66ed8795eb6de5d2a6b204baac9378d6d28136cc", "2320f853059c29ce7e70409fa559074d727da5a2", "1c12bd0779d4cc5148cb8b9f82e9b1469866785c", "0b4d5b7cef06b66182db80803f783d077e3637b6", "9c3f13969af79ce26d584935e8e39a4423a9d63f", "7e0f91e51ee372939c96714c7919dde6dc756849", "474952c4ceeec59d2677c60e92ebbf6d34140b2d", "b9637ae1fbb8aebc8deb6e3c15dd93839e327e1c", "464d2c88185abdb6c17727000f247b39eddf3de4", "818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57", "9421fa26257e6a8d59bb874cf3b376c6d4c4118b", "076b02b481a41f1e07b8a2bdbe0ac8d946f9872e", "657cce51f80e272373ab4fc0dabf5dc8b30c0070", "4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0", "33eadd4e666a894306a22ba0839c5e0cef77280e", "833560cd68a3e3d1be1bc650756dd6c679798551", "e9419436682726232e1b37a04c53bba919b12025", "b4916e339caef2d2a98e633e1f0b2144e2b0c9e2", "8b2c80788f789d4ce7849c13943fa920d9e3c95f", "fc50a1620b0210c9e9a21b6f62d3338010187032", "85079d64d6fdd0ba5318fda119d152f2d2946391", "3e92f6ad7b1a0d59d5ffa30fc6e617e885f7be1a", "d5caec8107da41ec71fc0bb36d60fc2d8834846e", "439369de9514e41e0f03fed552d8f6e5aebf51b2", "2953617763c8a4a5784acf5a4c233f1d80c730db", "d8a305b9366608d54452ac30459ee57b4f5cf1c9", "6648b4db5f12c30941ea78c695e77aded19672bb", "295065d942abca0711300b2b4c39829551060578", "d966dbc12c3c647f8839b19863fc7bf876cf212f", "8885ae1cbdac1d27443167689ed872dfe46c9e3f", "a3afabdc3650ef93c771262f31db8ee144d0ff44", "38e48d6b39ce94ddc2a0bf20320598739187bfef", "7c4530882cfcef1d2b4aa2996f494dfac626b5d9", "b774d6cd89cb27e62f03f183b89b7cacc412e131", "4b5231149f566fc8a78797b6fb448f9bca416380", "fcf74c4a3042eca2994aac122f1ed045be098902", "05106b86ec45914d1136719d311078182d437872", "94c6e5ccd67be60a5ced11d0a5c59e0ab0f749d4", "0ed9efc8bc8b5c8af907479cbf0c8a184530ffbc", "ce01073130ff984eb43cbf43f6bdcbe4d5a09df9", "42165fdef8d03bb1e2f29dab6cd3ce56d64d4bd4", "a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be", "49d46b0245475067bb7192d9bb1538701ae1c014", "79c93274429d6355959f1e4374c2147bb81ea649", "4c163d4942117179d3e97182e1b280027d7d60a9", "c1056e6e84d52cf45017aad544fa0406441abda0", "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "133921bb5e559de464c0078f5fa67409aca27917", "d4c8f6c4a2b744fcfd82a7d7c8041d87d2b5c250", "e9ea5e8e018fa2d1f508aa0310469ba11a61345a", "b499228aa74b59be32711c3926e44de208d6b636", "e0c0ac3bb66203c32be81193fabeee44c3585582", "4fef1313fd4948fed09dee318e2e231216c4fb3b", "844286473f9afba35b46b960c706e5ecea190b0d", "6dc67482ee0530e9ff535775891481ed9fd5f6ad", "45f9856d418527d23dc7c89197627fa1f3b215f9", "b6b3d6a37e7e77f5d5c763a4abeade256324268c", "908c6b1577a1f5309ae183daf2e24363039f22a8", "eba62fe8050e475ffe533b9f70db538074d8d0d1", "eb28671c43e6f3bfeee3b5bec8023887d8c07bc7", "90579a68e46b772a8e9aaca8ecbd06942d0b9b35", "f4c60c3ee4904d61ff84c9d4c15c9aecdcf04cdc", "79de42d6ca8d1bf2952c46eb74e6e0561f979257", "5abe916562fad8306e3f4e571f83015047f0be1d", "f6feb1af1809dfd872d868dfcc13021cc42f496c", "580fd9a601314ea32dc85ec98267b411dd3465cf", "8e59cf8c3becbedced0089028a1cddac8b19b251", "23e943809c131c50dc90c1d308373febc60b9029", "c677000c9078fdff8622be15a37db7d4945f36c2", "7e27d44e3fac723ccb703e0a83b22711bd42efe8", "e7fe886600399448f3282c8da8fd98ab7e50eae3", "b795675a6228abb68f8ed1b8abaf8630309fd764", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "2eca0afa23ee2b9b0999c445b8dba44ad3039bd1", "8b55402ffee2734bfc7d5d7595500916e1ef04e8", "345a222fef6f5c1415056319ae7e87a369940d3f", "fe66fc9e3d9a52497119bab89ac54fc8b5f8859a", "5cab3ce511ec8345d16a28c00094a2800b3919ce", "fa9189749a4c95c45ec7d98db49e5f736c51760e", "0000fcfd467a19cf0e59169c2f07d730a0f3a8b9", "4921243268c81d0d6db99053a9d004852225a622", "a27973d90c1427369cb10aa0202d671f0422e21e", "7062b5de5fddb298823cf8969c7dfa6165ea933e", "fe400b814cfea5538887c92040f1ab0d6fb45bfe", "04cd9168dcf1a0d2cf01db95a1af53d0900bc346", "94e3b75a6732b5918c4c2b87d127a9216ff07efc", "887027f0aaa2644c29bb9f4c42dcb19ea94c2763", "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "555e65623326de1b9c32bd22d482071920a6e4f1", "2115fe369b3a6b859c6992ba023d5c11b1689801", "02588064dd2b4b56d1dad214c1af3a7518832a59", "6d3d61ef9b5ff6d41badbc3d40ea23acbbc9c3fe", "beeebd2af0d8f130dcf234231de4569d584cb7fd", "1b1e3f7218f1c0f0db56bf2bd9475521454693a1", "d64f52b94977b71976327eeb3db702b246ee39ce", "85e2b2c35b916b1ee4926c155065d01b21c80c60", "3bf09b2e2639add154a9fe6ff98cc373d3e90e4e", "45dd2a3cd7c27f2e9509b023d702408f5ac11c9d", "05544876b7bc58b59b9ec5a8ee09e3ce9b4791ce", "9fb5e3db385588f671b11cfc8bf18efb90ee7b19", "5d45cc9a3a2fc064eccc0c915dbdf73cce559ce7", "7f14e73dade94b8b1f276dcd91257aa7de5f19d7", "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8", "82d9b588eabeb6bf4baa945d5c71b3bf89dd1e69", "f77a604410d88307ec5c6331c8b6133272fbaa10", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "82247c9e74ddebb4dce65560ee69620579358f2d", "159d16cdc48135632c2d5790e5baaf8d0631f510", "83d66c1f808962536a68418587b691f30221c5a1", "39f3f9d22a072d0ccf423aa31bacbb4071ac0644", "561ed7e47524fb3218e6a38f41cd877a9c33d3b9", "10480a42957a8e08e4c543185e135d7c254583a5", "58ee208dce1f06724bb443b4cfe0aa30d6cc9d30", "6aee8323be7ad5e568d62ba368bc7123f750515f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "828a7b3122ebd5b8b0c617902bc04ac5a6c60240", "e4a11cc697c94fdda9eedbac7d83a865646bd4cc", "1de837d76d6513aa664f2d032d49ef12c938bbbb", "9d842c2fb5a0c54f15bc4e45c7c93bdb8b0c44de", "c689f73f8ea65c6e81c628f2b37feae09b29e46b", "1c0a6854b793ca8ad281513c184318b73d4868c4", "428818a9edfb547431be6d7ec165c6af576c83d5", "24dc571a49d3431e8cb1f1008f86d5dd5b7a1613", "f302e136c41db5de1d624412f68c9174cf7ae8be", "5f56b1043c59727ebac5b6f7c31b5c30a0b84a6f", "f75e7a025bf0eaba3de4efb53d6c2b0e4b3669fd", "d85704f4814e9fa5ff0b68b1e5cad9e6527d0bbf", "76f83380fe193ae8475e660c1c6b12b60521a29f", "9f4d7d622d1f7319cc511bfef661cd973e881a4c", "6d86f0e22fed5e065ecf54b273d540b2430f014d", "086fa2fe3ee2a5b805aeaf9fbfe59ee8157dad5c", "163a474747fd63ab62ae586711fa5e5a2ac91bd8", "778ce81457383bd5e3fdb11b145ded202ebb4970", "21fa67345e49642b8ebb22a59c4b2799a56e996f", "3a7011346ce939e3251915e92ae2f252e4c7f777", "88513e738a95840de05a62f0e43d30a67b3c542e", "5785466bc14529e94e54baa4ed051f7037f3b1d3", "36eff562f65125511b5dfab68ce7f7a943c27478", "b9aa3bafa9e8e21bb92908ae23b468fa248239b3", "6e9aebe54f76d85c6df7e80faa761ef0aec3d54c", "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "c70ad19c90491e2de8de686b6a49f9bbe44692c0", "ecbaa92c289f4f5ff9a57b19a2725036a92311f5", "558c587373e2ea44898f70de7858da71aa217b8d", "1c54acd7d9ed8017acdc5674c9b7faac738fd651", "61d2dda8d96a10a714636475c7589bd149bda053", "6c7f040a150abf21dbcefe1f22e0f98fa184f41a", "90368e1751b34f22492ed18cc3b1ab19ae546afa", "bc9db6117d0026bc5b11eeba2303d2bddc96c306", "ecf551d532d0e9cfb252a1bea04d14db620bc488", "bf55591e09b58ea9ce8d66110d6d3000ee804bdd", "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "d7ce5665a72c0b607f484c1b448875f02ddfac3b", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17", "1af68821518f03568f913ab03fc02080247a27ff", "00fe3d95d0fd5f1433d81405bee772c4fe9af9c6", "354c029c88be2bbc27dfd2e2e729c0ae622511e6", "c3640aae13e344ad70a926510221dada626a44de", "a5af227e3e3158158163fb0715ae3971be2e1df4", "66021a920001bc3e6258bffe7076d647614147b7", "a72b8bbd039989db39769da836cdb287737deb92", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "1cbf8a2177ea1f4171d0656435164928da095a54", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "258986132bf17755fe8263e42429fe73218c1534", "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "f01fc808592ea7c473a69a6e7484040a435f36d9", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "59927ded86ab4f7253fc32efb351e5a13e746ead", "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "71b7178df5d2b112d07e45038cb5637208659ff7", "44040913380206991b1991daf1192942e038fe31", "4aa4069693bee00d1b0759ca3df35e59284e9845", "5cb6700d94c6118ee13f4f4fecac99f111189812", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "194e9a6f02fd5f39226dc9848213479fec5f1821", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "0ba87571341beaf6a5c9a30e049be7b1fc9a4c60", "355de7460120ddc1150d9ce3756f9848983f7ff4", "8e080b98efbe65c02a116439205ca2344b9f7cd4", "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11", "fbdbe747c6aa8b35b981d21e475ff1506a1bae66", "a48a56b0727d09f599676524fe190308d9e88bf1", "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141", "e8dbc756ea246f599250c09e3efd9bba9909a842", "05e074abddd3fe987b9bebd46f6cf4bf8465c37e", "b3e89f05876d47b9bd6ece225aaeee457a6824e8", "02b28f3b71138a06e40dbd614abf8568420ae183", "0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7", "60b05f32c32519a809f21642ef1eb3eaf3848008", "cc58597d33d5ff5a51d7810512e2b90b056840ca", "4c915c1eecb217c123a36dc6d3ce52d12c742614", "d7da009f457917aa381619facfa5ffae9329a6e9", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/1c83f3f9789df43bf937ae2618721e2da83dcc06"}, "b9915d00a7f46e51e587aff4365e1c48ab8ef496": {"id": "b9915d00a7f46e51e587aff4365e1c48ab8ef496", "title": "Graph-Based Embedding Smoothing for Sequential Recommendation", "authors": [{"authorId": "2118258182", "name": "Tianyu Zhu", "paperCount": 4, "citationCount": 11, "hIndex": 3}, {"authorId": "1845784191", "name": "Leilei Sun", "paperCount": 74, "citationCount": 1068, "hIndex": 19}, {"authorId": "1805182", "name": "Guoqing Chen", "paperCount": 242, "citationCount": 3629, "hIndex": 34}], "abstract": "In real-world scenarios, a user's interactions with items could be formalized as a behavior sequence, indicating his/her dynamic and evolutionary preferences. To this end, a series of recent efforts in recommender systems aim at improving recommendation performance by considering the sequential information. However, impacts of sequential behavior on future interactions may vary greatly in different scenarios. Additionally, semantic item relations underlying item attributes have not been well exploited in sequential recommendation models, which could be crucial for measuring item similarities in recommendation. To deal with the above problems, this paper provides a general embedding smoothing framework for sequential recommendation models. Specifically, we first construct a hybrid item graph by fusing sequential item relations derived from user-item interactions with semantic item relations built upon item attributes. Second, we perform graph convolutions on the hybrid item graph to generate smoothed item embeddings. Finally, we equip sequential recommendation models with the smoothed item representations to enhance their performances. Experimental results demonstrate that with our embedding smoothing framework, the state-of-the-art sequential recommendation model, SASRec, achieves superior performance to most baseline methods on three real-world datasets. Moreover, the results show that most mainstream sequential recommendation models could benefit from our framework.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2023, "reference_count": 54, "citation_count": 4, "influential_paper_citations": 1, "is_open_access": false, "citations": ["01700c79b56f6893e1985f0c3a7f9127b08cda7b", "1e9920010ddd6e33040806c06653022e83e6e91a", "246c00d9598487991ac5e270e6b02d1fbd26bb3b", "dd5450729b466236f45216bb8b8734f97f986c70"], "references": ["3024f58826a5bce3378af94f677e8fb90cbb49e0", "c9e432023a240b22f385cf1cc76ab75fbe084ee7", "de02ec03f6a71246e505862a7195894601fbab99", "c5f5f179d80a3bf9b4f29750283a87eaca42e91b", "594dc362b4332ae661e3d71da17d097bb4a357dd", "c79b2c43454793cbde45476780de60a66ee3778b", "690edf44e8739fd80bdfb76f40c9a4a222f3bba8", "03ed44b85886a7a95d1533fb1d1a142e60ae292c", "7e71eedb078181873a56f2adcfef9dddaeb95602", "398d6f4432e6aa7acf21c0bbaaebac48998faad3", "901a6ad54f3bfc0ca6671f4e492703c671475288", "f6c985149798760da88f871cf71148bbeca69b2a", "4e810d6b04de4e657a4cda2f58296b01e91eae53", "38b06821977b5024dd51145bbcfdd0eac42d439c", "81a4fd3004df0eb05d6c1cef96ad33d5407820df", "cf933ca85f4505cebe874be2b9256748c75697be", "f778186c42aac4822385ce43ebaeb79ac2ea5c1f", "fb698241a9a6dec0ddd3e0bf85485ee0e31cbb6a", "97faeefa771e8cc8e55159e2bd03e6f5eef249a8", "0b42168f22713c48d8b0ce3bc8f680d347184d88", "97f1d08c306040401112ff0564f37e6c6a312522", "5aea95e1ae78a66474051a330ded374e199b658c", "6c96c2d4a3fbd572fef2d59cb856521ee1746789", "76da7eab258081c257ebd87f7a559d44e31d8315", "23f5854b38a15c2ae201e751311665f7995b5e10", "bc6a927b079fc1c4189f00108ff792a6d3d190e7", "75a927501749c2cbc0e19a58f798f04de59df64a", "97de32d8ca162944e5f7e83071c596d13d168109", "55905e637d4c06cd16e331ac9417d93d896472fd", "d5a823053663a77869d4b38a8026c51ebc2e975e", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "c509de93b3d34ecd178f598814bd5177a0a29726", "ef79912287a8d40dc56e9f72150dced664c8e9df", "137bbe604334584fd4a1d6eb9218a588ae3dda3e", "36eff562f65125511b5dfab68ce7f7a943c27478", "fea7e2bcbe852bcb33ce70a4d57664e954f0e82a", "89a16eb847e5039fe5d9c6372ab45145400c9aa1", "7e5a550cdcb59464a73bfdc54f541fc5e5636ea3", "c41eb895616e453dcba1a70c9b942c5063cc656c", "3d846cb01f6a975554035d2210b578ca61344b22", "36f652172792f8aab1cf3c4441a72a1bf79d17c8", "492f57ee9ceb61fb5a47ad7aebfec1121887a175", "0d635696ef2c768095d9f6378df93241a0e78d16", "0834e74304b547c9354b6d7da6fa78ef47a48fa8", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "5e925a9f1e20df61d1e860a7aa71894b35a1c186", "8090ca9679d5c2ab40a090385e545944713031b0", "50d85cb114f7c5e779a6772f2931e77dddd54a5e", "db16e908246f32b60a6e0a8e27093aa145fbb1ed", "fec691d09b564986ad27162ce15344604c840ff9", "88816ae492956f3004daa41357166f1181c0c1bf", "b1a5961609c623fc816aaa77565ba38b25531a8e", "9922d9aa363ceccf40dcee230a48ee3c9e8611ff"], "url": "https://www.semanticscholar.org/paper/b9915d00a7f46e51e587aff4365e1c48ab8ef496"}, "c6d2d950b64973f1bec3b0dd0ed46fbf0319351d": {"id": "c6d2d950b64973f1bec3b0dd0ed46fbf0319351d", "title": "Towards Zero-Shot Sign Language Recognition", "authors": [{"authorId": "39032755", "name": "Yunus Can Bilge", "paperCount": 7, "citationCount": 43, "hIndex": 4}, {"authorId": "1939006", "name": "R. G. Cinbis", "paperCount": 52, "citationCount": 2007, "hIndex": 19}, {"authorId": "1398643531", "name": "Nazli Ikizler-Cinbis", "paperCount": 54, "citationCount": 1540, "hIndex": 16}], "abstract": "This paper tackles the problem of zero-shot sign language recognition (ZSSLR), where the goal is to leverage models learned over the seen sign classes to recognize the instances of unseen sign classes. In this context, readily available textual sign descriptions and attributes collected from sign language dictionaries are utilized as semantic class representations for knowledge transfer. For this novel problem setup, we introduce three benchmark datasets with their accompanying textual and attribute descriptions to analyze the problem in detail. Our proposed approach builds spatiotemporal models of body and hand regions. By leveraging the descriptive text and attribute embeddings along with these visual representations within a zero-shot learning framework, we show that textual and attribute based class definitions can provide effective knowledge for the recognition of previously unseen sign classes. We additionally introduce techniques to analyze the influence of binary attributes in correct and incorrect zero-shot predictions. We anticipate that the introduced approaches and the accompanying datasets will provide a basis for further exploration of zero-shot learning in sign language recognition.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2022, "reference_count": 104, "citation_count": 5, "influential_paper_citations": 1, "is_open_access": true, "citations": ["1fd869e3b69b19b7613723bf5a21aa00a8dc63fa", "f9c2127e745a73e34f93e2090bb385adbc74c775", "4a5fbeccf07737f52822b6edec7765e35ba7ee1a", "f1eadac5a772f123222e54e2ad9c5e6f937353c0", "1a49480f904adfd570aa9778da14528d4f7c237b"], "references": ["30e85b1651b1d2b726e71e2747d09b187bbd045a", "631bbcce16387e76e4780d7c84b07b2a37d6bfc4", "964502ea316fe9049c464a0925e3dc4246023a0d", "0b2538f9c22273db62b205402864062bf222b68c", "5a65e6184d18cd93ccbaa4d940a27749358ef824", "05dcdfece56d1869895f53ed581d8ad64118c05f", "d9e98cbe813ead99c5f4f57be4ff6949fb51442a", "40eea50b67d04e821117d82153f3f3f36e33d22a", "0030923414ff02a4180bc29809003d503be213e5", "874063b6c71aef8382eb66a66d8a1c1188e78a9a", "04323b8f60e261af54aec4b5cd345cbdd475267d", "797243368d6ccde3b70bab9e1265f4e1d4e1cc43", "310a8e2c9f2650fa2e44fdf0d82d11c0cb3e387e", "4bbfd46721c145852e443ae4aad35148b814bf91", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "212d7af298a4bd9090b834293067d2f091a95cfc", "95a2e44cd649beddc0a0d7105ae06d2e91237551", "9831061dd9f3e53057ce98aa1208731119cb7f12", "644602c65a5d8f30e62be027eb7b47f7c335191a", "81a1660d57738347a04b22920571bc394dd97a9e", "5893385c2eaa691e2286d8b7720e5344f6bbd9b8", "cb3f5defe2120076ebbcc89b9256bbfcb8b4d8a1", "a10cd29fec9dca66250dbde19db5e831f7ce6406", "e060e32f8ad98f10277b582393df50ac17f2836c", "acd87843a451d18b4dc6474ddce1ae946429eaf1", "28b85543e8f12c3d2d2227dcc9f5e87c685535ea", "3a6f6c4f477a50c3a513deef4c85392107d26b3b", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "b61a3f8b80bbd44f24544dc915f52fd30bbdf485", "442e10a3c6640ded9408622005e3c2a8906ce4c2", "2b1fbf14de0381a48c736e625fbb1329d71c798c", "89e88ab5b7ba7557573ad773a0a077484bee3759", "de7bdc64e75f008efd9b25fdc5250f528757a698", "bae3db226cce8fff187c00fe6b430251e05623de", "8998bf389329b9eae9e13fa395b1f53943ed2e91", "31b4f2a9fcd8e45a5b4f4cf95e3caa8b460c062f", "95f8cfd71e7c1aa5ae720e1bd68be2fd7ef1047e", "79f9a15b4e838d6db91249a85d72fadb07aee927", "5091316bb1c6db6c6a813f4391911a5c311fdfe0", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "a30203598d758af7112afa0c5e6f85ef726daec1", "7d76a09aa363685bc0f04a502ed853dc09a574e2", "f3c7313e929998845885a5512dc095929a847ba1", "aaa0ac831a2546303611419cca74af367effb92a", "23eed412efdfdd5d798892a8d6ec5f9273a3952d", "4b18303edf701e41a288da36f8f1ba129da67eb7", "c4048de57777afb4873fdd01b18f0976b903bf87", "6540cb7971d1a9d72562d465172e010fbb729bc3", "3ca2e45db5c88d830c3e31134feacd793c09ee2f", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "018e730f8947173e1140210d4d1760d05c9d3854", "a10eec3fd2a1974e071ca6ae0043e2b1e0533785", "de8b0a219bd8dbacc9f83b2b46d95fe0f24b95a2", "ccbc09d498cad330c37f94e15b77bf220b10ccb4", "d6714ee0a3c3c5ead3d681d4bec8e60f042928ef", "0b544dfe355a5070b60986319a3f51fb45d1348e", "109156b008f569fd279bddc48ae0d462b82ca635", "9bc0295460089592d04e754a5fd427060b7bfa8c", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "5cb6700d94c6118ee13f4f4fecac99f111189812", "f038e8c3656f5c7a4846a7eca731eb567255adcb", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "caa632d101a41a7860562e4399a5eaa9a4088b55", "140b6f1132cfd34e409a286e921fabacff3783f3", "a11c1e8c6d38996e3e1e06d2f928e6cecdde55c7", "8bd62710fdaac5e5558bafffecbe4d3107a0c31c", "8e080b98efbe65c02a116439205ca2344b9f7cd4", "23e568fcf0192e4ff5e6bed7507ee5b9e6c43598", "675afff03b9a6169fae4bc79dae93119461acb9d", "c30b9fb837e912ccf3919fdb64e9543fca57799e", "7483fd4a7716f144c624b1bf1241280759727648", "23627371978343e21efcba7d00d909b03757d6fb", "aea0f946e8dcddb65cc2e907456c42453f246a50", "d6b47a0ee5149be524aa585aef6ca35e6a903fe6", "c6a8aef1bf134294482d8088f982d5643347d2ff", "0566bf06a0368b518b8b474166f7b1dfef3f9283", "ad9ee25977b6faa3e81c5d42246bc8d1c2f54a10", "5a6722fe7577f78b4cd82ab76265067d7d7a8baf", "461d2c494d0353834c54f13e74cc80cd56dbe365", "f5ce4b8bab38f89ccffe5d9ddc4f8bb7856e62c4", "fd357c773c5531b35d9050fcb066fdd35211611f", "5cddbfcb50efc3ce1c8034cef61acb8486f77434", "6957ecc299b76faaef1dc2c833a0f76ab76eb35e", "6cb1f6e1de7aa9c183b156d52e58770514381b0e", "843ab3e628aadd9415d96e16d6aef638c474bac1", "da5aa6975984089719f7415cc968edea64184bd1", "2f83f6e1afadf0963153974968af6b8342775d82", "13a813cc37a2f814068387973a36e253ebc99fd5", "225843dbd17c19fd4c499d092218d29b99e9b05a", "fc0763a2da79777ad9174b9eac3767f16a86cbc9", "59fe0f477f81a8671956b8d1363bdc06ae8b08b3", "7b76de8ad61dea2ce053891122163c0630c01689", "eb6095949b5cead71686dd3e048c7150109ad022", "f7953a15c53265d5f351b9abce5cbc3b7bfb7ee2", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "ddbb230dd1743ffa0f2af23b44de52097298fee4", "1ed3e20bbcf12c60e766189987074ac2935d7140", "bc1a59c675036cd4b1b82014024ee494f93a2928", "946d2cb93578c7bbda8e1e1e29c3ec61605f8d9a", "bb0128a4b4f1c64b44dfb526ba88113a0c6863e9", "69494481ace5549e8dfe2041cdc006090997c7a7"], "url": "https://www.semanticscholar.org/paper/c6d2d950b64973f1bec3b0dd0ed46fbf0319351d"}, "4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c": {"id": "4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c", "title": "Classification of Long Sequential Data using Circular Dilated Convolutional Neural Networks", "authors": [{"authorId": "2145342032", "name": "Lei Cheng", "paperCount": 11, "citationCount": 32, "hIndex": 3}, {"authorId": "21639949", "name": "R. Khalitov", "paperCount": 5, "citationCount": 10, "hIndex": 1}, {"authorId": "2117902249", "name": "Tong Yu", "paperCount": 16, "citationCount": 373, "hIndex": 9}, {"authorId": "2109424253", "name": "Zhirong Yang", "paperCount": 64, "citationCount": 1087, "hIndex": 16}], "abstract": null, "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2022, "reference_count": 73, "citation_count": 1, "influential_paper_citations": 1, "is_open_access": false, "citations": ["b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc"], "references": ["7e5709d81558d3ef4265de29ea75931afeb1f2dd", "1a883522f3c0051d70be1f8cbdb8989a77395006", "9ed25f101f19ea735ca300848948ed64064b97ca", "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "7e9ff94476f41041c75e253e84f487db00e9c861", "f24f62fabec6bca02658c320ff9b43c84947c5de", "466f7cfe7442738ee974b12939b4c2e32ee098bf", "5070c4871691eb53267786c512e84f3c27b354c5", "7c5c149699a0ba54b52cd5b9e291077f4a1f9d13", "657329c633709dd1ac34a30d57341b186b1a47c2", "6c59c85cb393f69b4c2b70846359b2fc1611fc08", "3ab39796f61eb75d59fdf7d0462da6069c66d3ee", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "6f68e1bb253925d8431588555d3010419f322e04", "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "5b8b3566ee19bb81a61c43dcaf13b17fbcd2430f", "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "5deacb99c47b8407aa478f84424f18a0b8940606", "34a4e6818d680875ff0bef9a76de0376118446d1", "d8da125d2511d037df036c0f0da7c57135e24409", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "2b4d86479a5ab855a2d78e92b3da9757d200ae7f", "396023afbea6f1d99f6ab1bf33b8acb12c29f9eb", "36e30516683032634975c53e60f3737b6e35ff80", "21da617a0f79aabf94272107184606cefe90ab75", "4543360d6b6133d143121eda079cd4b7667d2277", "794ca7a3a856683221797c6e03cdc6ef798d1f5e", "0a728daad8594c38df2047f63b119f1286fe63a9", "e09c58ef25f4f8af6dafb36d645ef3aea1f00ab6", "0d3c46a3cbfe06cec259fec954b6ff6df6c1a566", "565ab57eede8bf6ef9c42df51216b9f85287c234", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "921196c32213a229245a9705ee4768bc941e7a26", "84c1717345dd451e7a61fe89807b4c017754fc4e", "cab372bc3824780cce20d9dd1c22d4df39ed081a", "0701eb3607b86e198e4394ecd48442e5709e98ca", "ee4a012a4b12d11d7ab8c0e79c61e807927a163c", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "4a73a1840945e87583d89ca0216a2c449d50a4a3", "13d9323a8716131911bfda048a40e2cde1a76a46", "210f258524deabc3d08cbbea4e4ca5c2a98f4846", "f958d4921951e394057a1c4ec33bad9a34e5dad1", "98445f4172659ec5e891e031d8202c102135c644", "df0402517a7338ae28bc54acaac400de6b456a46", "6903ea1adc08200dfe2df5a54896c4c76a0088d1", "77f0a39b8e02686fd85b01971f8feb7f60971f80", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "7f5fc84819c0cf94b771fe15141f65b123f7b8ec", "2505fe509225f7d26acc3f4106ecee083bdcfb3d", "d46b81707786d18499f911b4ab72bb10c65406ba", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "8a756d4d25511d92a45d0f4545fa819de993851d", "84069287da0a6b488b8c933f3cb5be759cb6237e", "e01eae8dea6fbaa1ae7fc83535053932268df430", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "3bf2e6941dbb87ac0d2c771c159e1e27366a26e3", "24459451bc8a4a378b02463db4e11490191219ed", "5a9e85af0bc45472e9c14e956819f1324085aaa1", "9c40543348fef37369c5b19ea26994ca1db8d9e8", "5d90f06bb70a0a3dced62413346235c02b1aa086", "43b75305c42d282ee11adedb0b75d8d17d3f324c", "5c8c63ee0da654f7ae3cc296cc5d8fc3200a433b", "162d958ff885f1462aeda91cd72582323fd6a1f4", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "d0be39ee052d246ae99c082a565aba25b811be2d", "111fd833a4ae576cfdbb27d87d2f8fc0640af355", "f74ded11f72099d16591a1191d72262ae6b5f14a"], "url": "https://www.semanticscholar.org/paper/4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c"}, "fb6ecf67c275fe775a12ad4ddf2c8dc7cfad1348": {"id": "fb6ecf67c275fe775a12ad4ddf2c8dc7cfad1348", "title": "Unifying Grokking and Double Descent", "authors": [{"authorId": "2019153", "name": "P. Battaglia", "paperCount": 113, "citationCount": 10987, "hIndex": 42}, {"authorId": "2158860", "name": "Jessica B. Hamrick", "paperCount": 57, "citationCount": 5516, "hIndex": 21}, {"authorId": "2603033", "name": "V. Bapst", "paperCount": 33, "citationCount": 4170, "hIndex": 18}, {"authorId": "1398105826", "name": "Alvaro Sanchez-Gonzalez", "paperCount": 54, "citationCount": 4089, "hIndex": 19}, {"authorId": "145478807", "name": "Mateusz Malinowski", "paperCount": 46, "citationCount": 5335, "hIndex": 17}, {"authorId": "2844530", "name": "A. Tacchetti", "paperCount": 44, "citationCount": 2700, "hIndex": 14}, {"authorId": "143724694", "name": "David Raposo", "paperCount": 27, "citationCount": 4796, "hIndex": 14}, {"authorId": "48627702", "name": "Ryan Faulkner", "paperCount": 13, "citationCount": 2473, "hIndex": 8}, {"authorId": "2193477577", "name": "Caglar", "paperCount": 1, "citationCount": 0, "hIndex": 0}, {"authorId": "2193477440", "name": "Gulcehre", "paperCount": 1, "citationCount": 0, "hIndex": 0}, {"authorId": "2193477566", "name": "Francis Song", "paperCount": 1, "citationCount": 0, "hIndex": 0}, {"authorId": "3577056", "name": "Andy Ballard", "paperCount": 20, "citationCount": 6858, "hIndex": 7}, {"authorId": "2058362", "name": "J. Gilmer", "paperCount": 56, "citationCount": 11940, "hIndex": 21}, {"authorId": "35188630", "name": "George E. Dahl", "paperCount": 61, "citationCount": 31990, "hIndex": 32}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2193474939", "name": "Kelsey", "paperCount": 1, "citationCount": 0, "hIndex": 0}, {"authorId": "2193478345", "name": "Allen", "paperCount": 1, "citationCount": 0, "hIndex": 0}, {"authorId": "36942233", "name": "C. Nash", "paperCount": 19, "citationCount": 2424, "hIndex": 11}, {"authorId": "2066201331", "name": "Victoria Langston", "paperCount": 5, "citationCount": 2170, "hIndex": 4}, {"authorId": "1745899", "name": "Chris Dyer", "paperCount": 253, "citationCount": 33766, "hIndex": 74}, {"authorId": "1688276", "name": "Daan Wierstra", "paperCount": 63, "citationCount": 59256, "hIndex": 44}], "abstract": "A principled understanding of generalization in deep learning requires unifying disparate observations under a single conceptual framework. Previous work has studied grokking , a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar double descent . These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2022, "reference_count": 19, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["1c07e314985161ec42ba895eb4869ffc5d360736", "20de79ec4fe682b68930eb4dcd91b1801b8d4731", "6915f983d0d207ec9ce2bb12eefed86f3a14584c", "52b2156c830b9dfbe92ced52f8ab3b1956927808", "77d956cdab4508d569ae5741549b78e715fd0749", "f00c87cac325a6dda1e6982bb042c7583ed5590d", "2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c", "002614a13b02f331be5e739a5a4a31e19ca28a60", "447f852688a8f4e038c9639270212d4e8ed4b02f", "ea415809bf87ef4b99966c6c50de6cb996a02a97", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "e0fe9b2f77288bc5e6f778611a49e62e98231f8c", "a6582abc47397d96888108ea308c0168d94a230d"], "url": "https://www.semanticscholar.org/paper/fb6ecf67c275fe775a12ad4ddf2c8dc7cfad1348"}, "fcf48e3be4357eaa6b1cde19b257b828243db038": {"id": "fcf48e3be4357eaa6b1cde19b257b828243db038", "title": "Leave Graphs Alone: Addressing Over-Squashing without Rewiring", "authors": [{"authorId": "145478807", "name": "Mateusz Malinowski", "paperCount": 46, "citationCount": 5335, "hIndex": 17}, {"authorId": "2844530", "name": "A. Tacchetti", "paperCount": 44, "citationCount": 2700, "hIndex": 14}, {"authorId": "143724694", "name": "David Raposo", "paperCount": 27, "citationCount": 4796, "hIndex": 14}, {"authorId": "35030998", "name": "Adam Santoro", "paperCount": 52, "citationCount": 7734, "hIndex": 29}, {"authorId": "3577056", "name": "Andy Ballard", "paperCount": 20, "citationCount": 6858, "hIndex": 7}, {"authorId": "2058362", "name": "J. Gilmer", "paperCount": 56, "citationCount": 11940, "hIndex": 21}, {"authorId": "35188630", "name": "George E. Dahl", "paperCount": 61, "citationCount": 31990, "hIndex": 32}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2801204", "name": "N. Heess", "paperCount": 164, "citationCount": 26898, "hIndex": 52}, {"authorId": "1688276", "name": "Daan Wierstra", "paperCount": 63, "citationCount": 59256, "hIndex": 44}, {"authorId": "2066364817", "name": "R. Garnett", "paperCount": 8, "citationCount": 48, "hIndex": 3}, {"authorId": "2479152", "name": "Danai Koutra", "paperCount": 147, "citationCount": 5222, "hIndex": 33}], "abstract": "Recent works have investigated the role of graph bottlenecks in preventing long-range information propagation in message-passing graph neural networks, causing the so-called \u2018over-squashing\u2019 phenomenon. As a remedy, graph rewiring mechanisms have been proposed as preprocessing steps. Graph Echo State Networks (GESNs) are a reservoir computing model for graphs, where node embeddings are recursively computed by an untrained message-passing function. In this paper, we show that GESNs can achieve a signi\ufb01cantly better accuracy on six heterophilic node classi\ufb01cation tasks without altering the graph connectivity, thus suggesting a different route for addressing the over-squashing problem", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2022, "reference_count": 25, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["711100e891dc69321b0dbb3c86820a9f511e0242", "9aa607ca2227483c17219d8dade06b35c54d3851", "04ac11f8db83406b03d92aa4571fc3e6c176c1e3", "3bfa808ce20b2736708c3fc0b9443635e3f133a7", "363aef59e61292fb85cc678cba64e3639463ad9a", "21e33bd0ad95ee1f79d8b778e693fd316cbb72d4", "2e3a86c4b8f6883b371f718eb0a35857a6bf9b95", "cc56d1210f4ee7a5a351cf64eb3bbed18b48b22f", "94194703e83b5447f519fd8bcbb903916e05aaf9", "446cd7da78f629cfdcef88b91ffbd24022a0b67b", "2a6d160b529272964ce1a6707adf52f3d6ba4861", "81a4fd3004df0eb05d6c1cef96ad33d5407820df", "36652428740cd30d245d55889f01a7fb04a91c93", "36eff562f65125511b5dfab68ce7f7a943c27478", "18b47b83a373f33d6b902a3615f42c10f7600d72", "5d1bfeed240709725c78bc72ea40e55410b373dc", "3d93a9224f4789f7a2eb8e3dc31e93cdfe6f64a8", "3a758adba701dc81020e4d2858b1298044db71b5", "69e5339c0c3928a354e848b9ccf5349f6397e60b", "ec2b2569b3a0d70a5b45d48b041dec9060d85eb7", "3efd851140aa28e95221b55fcc5659eea97b172d", "507a568d812248cc8edc52d1282ac5f829effab2", "0d073966e48ffb6dccde1e4eb3f0380c10c6a766", "7677a6fb8027392481dd78d60001832f86b1e4ea", "c783ad767f09dfb63baf0584175604ab1df7f3ca"], "url": "https://www.semanticscholar.org/paper/fcf48e3be4357eaa6b1cde19b257b828243db038"}, "657329c633709dd1ac34a30d57341b186b1a47c2": {"id": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers", "authors": [{"authorId": "39788470", "name": "Aurko Roy", "paperCount": 24, "citationCount": 2522, "hIndex": 13}, {"authorId": "2814161", "name": "M. Saffar", "paperCount": 19, "citationCount": 332, "hIndex": 5}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2529182", "name": "David Grangier", "paperCount": 79, "citationCount": 12665, "hIndex": 34}], "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 59, "citation_count": 259, "influential_paper_citations": 29, "is_open_access": true, "citations": ["71b6394ad5654f5cd0fba763768ba4e523f7bbca", "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "7e9ff94476f41041c75e253e84f487db00e9c861", "de18baa4964804cf471d85a5a090498242d2e79f", "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "800cfb3d23115cdcd4d114234b65bbdf2080f798", "e2db22251792e9dd809d5ffb0feaab50a687cdb0"], "references": ["3df83a60f55c64b40e6dbcd99cf9f67894a0736e", "70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "f51497f463566581874c941353dd9d80069c5b77", "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "f6390beca54411b06f3bde424fb983a451789733", "bf442ab269074665a68e4dbbe19e4efc97862541", "7d67237398986a6088c696df0bf57646c714508f", "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "f4238bd2385a52413ccbacfd9e409a650235bd13", "21da617a0f79aabf94272107184606cefe90ab75", "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "7d3ab2a839b077a318022f7842225db55033b2c3", "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "c9552f9e2a7a7656c4c9ef9569a824dffa1fd181", "fb507ada871d1e8c29e376dbf7b7879689aa89f9", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "21b786b3f870fc7fa247c143aa41de88b1fc6141", "520ddb38b59b8fae2209ddc7c6640462cf153eec", "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "680aafd3d51e666b297e27b93d9554cc2caf1c4d", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "1db9bd18681b96473f3c82b21edc9240b44dc329", "7570afa31c68e24fce1342b7d67c591787219bc1", "d1c424c261c577958917055f72fb9e2ad0348865", "f0afdccf2903039d202085a771953a171dfd57b1", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "252571243aa4c0b533aa7fc63f88d07fd844e7bb", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "510e26733aaff585d65701b9f1be7ca9d5afc586", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "efbd381493bb9636f489b965a2034d529cd56bcd", "be8c6c69f3e357bfad2987e45b62cff7e7474378", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "0936352b78a52bc5d2b5e3f04233efc56664af51", "7fe83e1a713ccb5ba19bce9ca933f958843916bb", "a418524a3576afff4dc2178ed169e692915bd46b", "93499a7c7f699b6630a86fad964536f9423bb6d0", "a2e2970dd99b86fd6198c0b756c6cd4d52e34c3b", "b624504240fa52ab76167acfe3156150ca01cf3b", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "c3823aacea60bc1f2cabb9283144690a3d015db5", "8829e3873846c6bbad5aca111e64f9d2c1b24299", "d862bda57dd0a51a1bff42b0e5fbeaaaa0c0d14f", "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "44ddac48353ead135eef4096859956eaa31be2a5", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "4c0d2d4269895eb367d7b0d38d9de3e99bf3f3ae", "4fc1bf60275c419eeb85e28793305a789dca4717", "1b443db0552343e4d02ad24112dc0667a12335a7", "6fb07b90b7fd2785ffec0da1069e75c53f7313c2", "2352d9105de31032538900dfb2ce7c95f6402963"], "url": "https://www.semanticscholar.org/paper/657329c633709dd1ac34a30d57341b186b1a47c2"}, "e3de7c1e8031915e50464dfbc133eea1c009ece5": {"id": "e3de7c1e8031915e50464dfbc133eea1c009ece5", "title": "Credit Score Prediction Using Machine Learning", "authors": [{"authorId": "38163334", "name": "Debabrata Swain", "paperCount": 38, "citationCount": 98, "hIndex": 5}, {"authorId": "150271637", "name": "Raunak Agrawal", "paperCount": 4, "citationCount": 0, "hIndex": 0}, {"authorId": "2162550456", "name": "Ayush Chandak", "paperCount": 3, "citationCount": 3, "hIndex": 1}, {"authorId": "2090152642", "name": "Vedant Lapshetwar", "paperCount": 1, "citationCount": 0, "hIndex": 0}, {"authorId": "2090417271", "name": "Naman Chandak", "paperCount": 1, "citationCount": 0, "hIndex": 0}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": null, "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 12, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["089ca7b74d9337849b763083994cc7e67797b3a5", "426b88b0757d9f8e3ea1eeaaa99985dd2eec80e9", "8a8c8469d3a330b52b9314a234a0a8d07c2330dc", "2619e2a8a0dfb8e4dbb160caf14a0a10e3aaf763", "e2b4090903fc408b0967874ddd20acaacf5fa4b2", "70d417e49d7c4f7b0dd1d7fe7f14da0e4100eb8b", "460d6f066e2af6295b945cd191a4a0ef9c8f47a9", "68eb90d2d8f5507f723d774e3d8f5fa6125afab8", "d51c099b2ef1b8fa131825b582ef23c6c50acc17", "64e0f41fc9a476b2a7c8dcb3b412e9a8e0948cee", "0bd1c88eecdb44ac2a9d9641b1984b1c62bc969a", "d73137dceb91cf261529410d30b0d5c67bb49651"], "url": "https://www.semanticscholar.org/paper/e3de7c1e8031915e50464dfbc133eea1c009ece5"}, "27ac832ee83d8b5386917998a171a0257e2151e2": {"id": "27ac832ee83d8b5386917998a171a0257e2151e2", "title": "Attention Augmented Convolutional Networks", "authors": [{"authorId": "4689792", "name": "Irwan Bello", "paperCount": 14, "citationCount": 2706, "hIndex": 9}, {"authorId": "2368067", "name": "Barret Zoph", "paperCount": 46, "citationCount": 25445, "hIndex": 31}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "1789737", "name": "Jonathon Shlens", "paperCount": 108, "citationCount": 67183, "hIndex": 50}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}], "abstract": "Convolutional networks have enjoyed much success in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighbourhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we propose to augment convolutional networks with self-attention by concatenating convolutional feature maps with a set of feature maps produced via a novel relative self-attention mechanism. In particular, we extend previous work on relative self-attention over sequences to images and discuss a memory efficient implementation. Unlike Squeeze-and-Excitation, which performs attention over the channels and ignores spatial information, our self-attention mechanism attends jointly to both features and spatial locations while preserving translation equivariance. We find that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3% top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 AP in COCO Object Detection on top of a RetinaNet baseline.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 62, "citation_count": 592, "influential_paper_citations": 45, "is_open_access": true, "citations": ["268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "ad7ddcc14984caae308c397f1a589aae75d4ab71", "3e398bad2d8636491a1034cc938a5e024c7aa881", "0170fc76e934ee643f869df18fb617d5357e8b4e", "177e957f5cd93229c9794ea652c646d2557b4a69", "8cb34cbdcf65c23ef98430441b14a648c4e8d992", "c143ea9e30b1f2d93a9c060253845423f9e60e1f"], "references": ["df67d46e78aae0d2fccfb6212d101a342259c01b", "72564a69bf339ff1d16a639c86a764db2321caab", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "b5375995ab8d679a581ffcc2f2e8d3777d60324b", "bdc046e65bc80cf13929ca0c3934d6faee830723", "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "9ea92ebeb7462f2db346cfa3281ad7497b1063d6", "693c97ecedb0a84539b7162c95e89fa3cd84ca73", "8a8cfa45b4c0d071fbffa091c02670b19c94b693", "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "b842cd963a5e61c48b467c92aee035485256ee12", "611a2dc033e5c02460b3ebc26c5297b4a1d0eb52", "e4b64a75d321311447e11c363b45cc07bb74acc2", "b7339c1deeb617c894cc08c92ed8c2d4ab14b4b5", "cd8ddaaf56e38dddafdeac3f9643b9b5e9d35d54", "de9cda8be4974787dca126cb663dc934c8105e0a", "8ba5f5106cd039a22f0d7fdf97d700e427dde282", "794ca7a3a856683221797c6e03cdc6ef798d1f5e", "ab63be50ee3bb93950f2e176da97d9c63c85e683", "de95601d9e3b20ec51aa33e1f27b1880d2c44ef2", "10bb4ef7a6719ea132e00f0ab5680919a4131d99", "51cdeeef710d1c84e10beadc8480c137ffe8d328", "f723eb3e7159f07b97464c8d947d15e78612abe4", "36ab504f16b6ac49194da43d03171f5d32b80a9f", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "8c1b00128e74f1cd92aede3959690615695d5101", "1db9bd18681b96473f3c82b21edc9240b44dc329", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "8899094797e82c5c185a0893896320ef77f60e64", "d0611891b9e8a7c5731146097b6f201578f47b2f", "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "168b7d0ab57a331a228ce21ffd1becbb93066f79", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "3fea412361b2d14cb3c6723968b421c1c8cb38e8", "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36", "a312a573ef81793d56401e932ef6c9498791a3d1", "d7878c2044fb699e0ce0cad83e411824b1499dc8", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "67d968c7450878190e45ac7886746de867bf673d", "b022f2a277a4bf5f42382e86e4380b96340b9e86", "b5c26ab8767d046cb6e32d959fdf726aee89bb62", "1c4e9156ca07705531e45960b7a919dc473abb51", "77f0a39b8e02686fd85b01971f8feb7f60971f80", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "9653d5c2c7844347343d073bbedd96e05d52f69b", "4d376d6978dad0374edfa6709c9556b42d3594d3", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "71b7178df5d2b112d07e45038cb5637208659ff7", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "bc6dff14a130c57a91d5a21339c23471faf1d46f", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "5d90f06bb70a0a3dced62413346235c02b1aa086", "c4d13788112f0fec457d31e1f7de9a53bbcec8e6", "5a554c8d22d47ac499aeb7fb0532ca9be65e5a2e", "f42b865e20e61a954239f421b42007236e671f19", "162d958ff885f1462aeda91cd72582323fd6a1f4", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/27ac832ee83d8b5386917998a171a0257e2151e2"}, "2e82c6cd222d78be880009d23b46e0f8c749667c": {"id": "2e82c6cd222d78be880009d23b46e0f8c749667c", "title": "Studying Stand-Alone Self-Attention in Vision Models", "authors": [{"authorId": "3377142", "name": "Prajit Ramachandran", "paperCount": 15, "citationCount": 3878, "hIndex": 12}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "4689792", "name": "Irwan Bello", "paperCount": 14, "citationCount": 2706, "hIndex": 9}, {"authorId": "6639036", "name": "Anselm Levskaya", "paperCount": 14, "citationCount": 4368, "hIndex": 11}, {"authorId": "1789737", "name": "Jonathon Shlens", "paperCount": 108, "citationCount": 67183, "hIndex": 50}], "abstract": null, "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 0, "citation_count": 1, "influential_paper_citations": 0, "is_open_access": false, "citations": ["2bbffec6395d3a8f81f5fe9137f7e078bd0f0336"], "references": [], "url": "https://www.semanticscholar.org/paper/2e82c6cd222d78be880009d23b46e0f8c749667c"}, "68ccecb380ecfc0a4b294b84e3d0b6ff6884c4df": {"id": "68ccecb380ecfc0a4b294b84e3d0b6ff6884c4df", "title": "Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation", "authors": [{"authorId": "20048351", "name": "Vihan Jain", "paperCount": 22, "citationCount": 3229, "hIndex": 15}, {"authorId": "2123694087", "name": "Gabriel Ilharco", "paperCount": 17, "citationCount": 1068, "hIndex": 14}, {"authorId": "31702389", "name": "Alexander Ku", "paperCount": 18, "citationCount": 1905, "hIndex": 11}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2042413", "name": "Eugene Ie", "paperCount": 39, "citationCount": 1485, "hIndex": 19}, {"authorId": "1387994164", "name": "Jason Baldridge", "paperCount": 129, "citationCount": 5622, "hIndex": 41}], "abstract": "Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation(VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language under-standing plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset (Anderson et al.,2018b) and propose a new metric, Coverage weighted by Length Score (CLS). We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new data set, Room-for-Room (R4R). Using R4R and CLS, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 41, "citation_count": 89, "influential_paper_citations": 31, "is_open_access": true, "citations": ["1b9ce27801c077433245ad0f9e43e3c38441cecd", "5a94aaa3ad624608e46a75de49452a03de568a07", "19c8c5a32cb0d77cd22ee1201e31306f0cd6100a", "8f167ec1149921fac63b1ea855443de109bb013a", "a68517ba51802fa8d4fde32e4f32f6b31ca28dd2", "bcd411becb0f438720f84fd7d7a16ef6b94c8271", "6536f36648d39f0f9f6105562f76704fcc0b19e8", "f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d", "f0152a8fd87cb60ad30c296823829802c13a9986", "077dbc662622cb3419415320d06de0602c50a8d0"], "references": ["e23f1585ef7940c2022ecf81767f01a0b6180ee6", "29e13746fa5aed13e51558a521a39aaeaa99c1b1", "b5cc6634724b2238c88bcc324ec01a2c91c1b909", "c66b8e508718f4b7f14829e5c2cde0add31d2693", "b033400e9a80915a928f4603582e5e8bf7656a85", "4d9064465d0fa3a2b3e9873b5c8e6b829b47664f", "ae09e1df06d7fdf7bece8c454085ddbf5fedec2a", "6654ba1d3e61cdf5f4decc7464436046cf602ed1", "56a0ead811a1bf15e42be8a9a007b0299636f213", "ede8ba65c4db10d357d9c3bf8e75b092f536fc84", "6609eed6e39ccb6ee8761bb895218b1b384b23ae", "8e22df7e93be72e3ab8dbabdaf7859e68a25e96f", "7c747f7fe9e1e811eee4430febb7787384bf35e8", "b58e80ad8c6e6844c41535080ccbdef06bce3b6e", "302ae0d991d62dee82b63530b487a50469810af4", "74b284a66e75b65f5970d05bac000fe91243ee49", "6bd9642470ff8c2089427f7a6392cd17d213a334", "8ca25b95fa13ece1d131c5a0df427e7c1c584da4", "ab743387bb3dad8ef541d43698372d2e98b05c7c", "2231f44be9a8472a46d8e8a628b4e52b9a8f44e0", "f84463e9aff9a72939e9a52b1d0dbc43ae3831d3", "8e0540ea3c3cc8cd009b2006d96c4b3ac2a84e52", "6a3df0b51f6d2bdea20d59fee94e1eff4010608f", "69e76e16740ed69f4dc55361a3d319ac2f1293dd", "2c1890864c1c2b750f48316dc8b650ba4772adc5", "f678a0041f2c6f931168010e7418c500c3f14cdb", "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "f01fc808592ea7c473a69a6e7484040a435f36d9", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "b883728f0d2791e29d350f16f24116c01f67a157", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "0459cea88c904883157a6d163fd2a126e9d1b367", "851f27df3a1ec8daa693f6642194562e3fe9769a", "61bdf0c093b01101eb2d78703ed4bdecb403c875", "4c915c1eecb217c123a36dc6d3ce52d12c742614", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "1f4731d5133cb96ab30e08bf39dffa874aebf487", "ea65866e3d1fd3e3e630005147e373ff7a886009", "45786063578e814444b8247028970758bbbd0488"], "url": "https://www.semanticscholar.org/paper/68ccecb380ecfc0a4b294b84e3d0b6ff6884c4df"}, "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d": {"id": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models", "authors": [{"authorId": "3377142", "name": "Prajit Ramachandran", "paperCount": 15, "citationCount": 3878, "hIndex": 12}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "4689792", "name": "Irwan Bello", "paperCount": 14, "citationCount": 2706, "hIndex": 9}, {"authorId": "6639036", "name": "Anselm Levskaya", "paperCount": 14, "citationCount": 4368, "hIndex": 11}, {"authorId": "1789737", "name": "Jonathon Shlens", "paperCount": 108, "citationCount": 67183, "hIndex": 50}], "abstract": "Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 71, "citation_count": 632, "influential_paper_citations": 53, "is_open_access": false, "citations": ["268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "ad7ddcc14984caae308c397f1a589aae75d4ab71", "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "3e398bad2d8636491a1034cc938a5e024c7aa881", "2def61f556f9a5576ace08911496b7c7e4f970a4", "177e957f5cd93229c9794ea652c646d2557b4a69", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "c143ea9e30b1f2d93a9c060253845423f9e60e1f", "dbe077f8521ecbe0a1477d6148c726d4f053d9c9"], "references": ["df67d46e78aae0d2fccfb6212d101a342259c01b", "72564a69bf339ff1d16a639c86a764db2321caab", "022dd244f2e25525eb37e9dda51abb9cd8ca8c30", "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "27ac832ee83d8b5386917998a171a0257e2151e2", "b5375995ab8d679a581ffcc2f2e8d3777d60324b", "18d41e3bd94cf38736e37580912c3b4ba56f08d5", "758876b5d85fa318f66968cf3e4e30ec4e0e5cbd", "4b344351fe43544317efc9adaebe6791c4242814", "693c97ecedb0a84539b7162c95e89fa3cd84ca73", "9405cc0d6169988371b2755e573cc28650d14dfe", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "b7339c1deeb617c894cc08c92ed8c2d4ab14b4b5", "cd8ddaaf56e38dddafdeac3f9643b9b5e9d35d54", "8ba5f5106cd039a22f0d7fdf97d700e427dde282", "794ca7a3a856683221797c6e03cdc6ef798d1f5e", "c898161e3691625bcef780b78100f70e8bb94154", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "1db9bd18681b96473f3c82b21edc9240b44dc329", "ce2845cadc5233ff0a647aa22ae3bbe646258890", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "a62bdda9ae6f86fc06d7edf5d3b429eda3a6640e", "6b89c45ed9e12d3392e28730f80234aa82e41cbc", "8899094797e82c5c185a0893896320ef77f60e64", "d0611891b9e8a7c5731146097b6f201578f47b2f", "cab372bc3824780cce20d9dd1c22d4df39ed081a", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "43428880d75b3a14257c3ee9bda054e61eb869c0", "3647d6d0f151dc05626449ee09cc7bce55be497e", "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "66386a946a04534275bd466862364d139790f41f", "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "67d968c7450878190e45ac7886746de867bf673d", "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "b022f2a277a4bf5f42382e86e4380b96340b9e86", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "df0402517a7338ae28bc54acaac400de6b456a46", "848938e6199bad08f1db6f3239b260cfa901e95f", "3056add22b20e3361c38c0472d294a79d4031cb4", "77f0a39b8e02686fd85b01971f8feb7f60971f80", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "b624504240fa52ab76167acfe3156150ca01cf3b", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "4d376d6978dad0374edfa6709c9556b42d3594d3", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "71b7178df5d2b112d07e45038cb5637208659ff7", "2a002ce457f7ab3088fbd2691734f1ce79f750c4", "aa7bfd2304201afbb19971ebde87b17e40242e91", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "039ad1ad259a9bd98e24b0738ba048282188d184", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "5d90f06bb70a0a3dced62413346235c02b1aa086", "9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d", "69e68bfaadf2dccff800158749f5a50fe82d173b", "d5ad1fdd277219257c38df86770c9fd68f4c74f0", "162d958ff885f1462aeda91cd72582323fd6a1f4", "4ea293ac6d42d09ccb9ffab7bd72dcf6102c3eab", "6dc61f37ecc552413606d8c89ffbc46ec98ed887", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "33c3e56439b11e2d77d99da667ae86afbf6e1ec3", "8d3a318b62d2e970122da35b2a2e70a5d12cc16f"], "url": "https://www.semanticscholar.org/paper/d6dccb5d71fbb6f5765f89633ba3a8e6809a720d"}, "fb507ada871d1e8c29e376dbf7b7879689aa89f9": {"id": "fb507ada871d1e8c29e376dbf7b7879689aa89f9", "title": "Music Transformer: Generating Music with Long-Term Structure", "authors": [{"authorId": "7900665", "name": "Cheng-Zhi Anna Huang", "paperCount": 26, "citationCount": 1043, "hIndex": 11}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}, {"authorId": "35577716", "name": "Ian Simon", "paperCount": 33, "citationCount": 4030, "hIndex": 19}, {"authorId": "41231781", "name": "Curtis Hawthorne", "paperCount": 26, "citationCount": 1520, "hIndex": 16}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "2555924", "name": "Andrew M. Dai", "paperCount": 71, "citationCount": 10296, "hIndex": 32}, {"authorId": "28552618", "name": "M. Hoffman", "paperCount": 102, "citationCount": 16748, "hIndex": 37}, {"authorId": "47153176", "name": "Monica Dinculescu", "paperCount": 21, "citationCount": 631, "hIndex": 9}, {"authorId": "2396681", "name": "D. Eck", "paperCount": 107, "citationCount": 6782, "hIndex": 40}], "abstract": "Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minutelong compositions (thousands of steps, four times the length modeled in Oore et al. (2018)) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 26, "citation_count": 253, "influential_paper_citations": 39, "is_open_access": false, "citations": ["3cfb319689f06bf04c2e28399361f414ca32c4b3", "530a059cb48477ad1e3d4f8f4b153274c8997332", "05f5f8b2065a520846d89771ebaea2bb1534e9c6", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "67dea28495cab71703993d0d52ca4733b9a66077", "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "657329c633709dd1ac34a30d57341b186b1a47c2", "7f631586a368f1762866b01ff9f43c265851d52e", "b2a839e3ee68e81b863b73ee08c6626c94477fef"], "references": ["5ad465e098052d0e645fe6f92d94ffa7282d5496", "0ec95fac7c5a923862ef4d443bbda2bd1e37f8a1", "a6e4beb28b345fce7470da122b4e45e2cd0dcd12", "642c1b4a9da95ea4239708afc5929a5007a1870d", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "1db9bd18681b96473f3c82b21edc9240b44dc329", "7570afa31c68e24fce1342b7d67c591787219bc1", "f83ef3250ba1166d7c1c7585da7dd78e0641fae7", "889c3b4394826639d483c039467cd9a05e68e73c", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "a066233a0b52a0e07197540ff2c73f42daf198f4", "54ca2690b17d4381b562d3cbc95a753d6fb3318f", "a0295975f9e9c662b99cb007c6d14a226334ad57", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "3cccc93064dae265eeb7bc76d02cdc67c942f0a5", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "705fd4febe2fff810d2f72f48dcda20826eca77a", "18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8", "32f078a7478d1ec2169599500a4507aceaccdda7", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "0f3bd4e6b16817e332b9be8357e3eddb8a461990", "a4ffd2f5dd98ee744c013060c5bc06503336d931", "4f7476037408ac3d993f5088544aab427bc319c1"], "url": "https://www.semanticscholar.org/paper/fb507ada871d1e8c29e376dbf7b7879689aa89f9"}, "03455d37be62139f11257054807aedeaac954687": {"id": "03455d37be62139f11257054807aedeaac954687", "title": "Visualizing Music Transformer", "authors": [{"authorId": "7900665", "name": "Cheng-Zhi Anna Huang", "paperCount": 26, "citationCount": 1043, "hIndex": 11}, {"authorId": "47153176", "name": "Monica Dinculescu", "paperCount": 21, "citationCount": 631, "hIndex": 9}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2396681", "name": "D. Eck", "paperCount": 107, "citationCount": 6782, "hIndex": 40}], "abstract": "Like language, music can be represented as a sequence of discrete symbols that form 1 a hierarchical syntax, with notes being roughly like characters and motifs of notes 2 like words. Unlike text, music relies heavily on repetition on multiple timescales 3 to build structure and meaning. Music Transformer has shown compelling results 4 in generating music with structure. How does the model capture motifs and build 5 phrasing? In this paper, we introduce a tool for visualizing self-attention on 6 polyphonic music with an interactive pianoroll. We use music transformer as both 7 a descriptive tool and a generative model. For the former, we use it to analyze 8 existing music to see if the resulting self-attention structure corroborates with the 9 musical structure known from music theory. For the latter, we inspect the model\u2019s 10 self-attention during generation, in order to understand how past notes affect future 11 ones. As relative self-attention has been shown to be particularly useful for music, 12 we compare and contrast the attention structure of regular attention to that of 13 relative attention, and examine its impact on the resulting generated music. For 14 example, for the JSB Chorales dataset, a model trained with relative attention is 15 more consistent in attending to all the voices in the preceding timestep and the 16 chords before, and at cadences to the beginning of a phrase, allowing it to create an 17 arc. In contrast, for regular attention we often see the attention \u201cshrink\u201d to focusing 18 mainly on the preceding two or three events, resulting in a certain voice repeating 19 the same note for a long duration, perhaps due to overconfidence. 20 Submitted to 32nd Conference on Neural Information Processing Systems (NIPS 2018). Do not distribute.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 0, "citation_count": 1, "influential_paper_citations": 0, "is_open_access": false, "citations": ["ec578b5bba19c604116794c374a3f5df9a7aa261"], "references": [], "url": "https://www.semanticscholar.org/paper/03455d37be62139f11257054807aedeaac954687"}, "19b7769dab4e6092aa4b7eeb8aa078a7b725c9b4": {"id": "19b7769dab4e6092aa4b7eeb8aa078a7b725c9b4", "title": "Relational inductive biases, deep learning, and graph networks", "authors": [{"authorId": "2019153", "name": "P. Battaglia", "paperCount": 113, "citationCount": 10987, "hIndex": 42}, {"authorId": "2158860", "name": "Jessica B. Hamrick", "paperCount": 57, "citationCount": 5516, "hIndex": 21}, {"authorId": "2603033", "name": "V. Bapst", "paperCount": 33, "citationCount": 4170, "hIndex": 18}, {"authorId": "1398105826", "name": "Alvaro Sanchez-Gonzalez", "paperCount": 54, "citationCount": 4089, "hIndex": 19}, {"authorId": "3133079", "name": "V. Zambaldi", "paperCount": 19, "citationCount": 4355, "hIndex": 14}, {"authorId": "145478807", "name": "Mateusz Malinowski", "paperCount": 46, "citationCount": 5335, "hIndex": 17}, {"authorId": "2844530", "name": "A. Tacchetti", "paperCount": 44, "citationCount": 2700, "hIndex": 14}, {"authorId": "143724694", "name": "David Raposo", "paperCount": 27, "citationCount": 4796, "hIndex": 14}, {"authorId": "35030998", "name": "Adam Santoro", "paperCount": 52, "citationCount": 7734, "hIndex": 29}, {"authorId": "48627702", "name": "Ryan Faulkner", "paperCount": 13, "citationCount": 2473, "hIndex": 8}, {"authorId": "1854385", "name": "\u00c7aglar G\u00fcl\u00e7ehre", "paperCount": 38, "citationCount": 38103, "hIndex": 30}, {"authorId": "2107148568", "name": "H. F. Song", "paperCount": 18, "citationCount": 3505, "hIndex": 16}, {"authorId": "5055381", "name": "A. J. Ballard", "paperCount": 35, "citationCount": 3118, "hIndex": 13}, {"authorId": "2058362", "name": "J. Gilmer", "paperCount": 56, "citationCount": 11940, "hIndex": 21}, {"authorId": "35188630", "name": "George E. Dahl", "paperCount": 61, "citationCount": 31990, "hIndex": 32}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "145254624", "name": "Kelsey R. Allen", "paperCount": 32, "citationCount": 3100, "hIndex": 11}, {"authorId": "36942233", "name": "C. Nash", "paperCount": 19, "citationCount": 2424, "hIndex": 11}, {"authorId": "2066201331", "name": "Victoria Langston", "paperCount": 5, "citationCount": 2170, "hIndex": 4}, {"authorId": "1745899", "name": "Chris Dyer", "paperCount": 253, "citationCount": 33766, "hIndex": 74}, {"authorId": "2801204", "name": "N. Heess", "paperCount": 164, "citationCount": 26898, "hIndex": 52}, {"authorId": "1688276", "name": "Daan Wierstra", "paperCount": 63, "citationCount": 59256, "hIndex": 44}, {"authorId": "143967473", "name": "Pushmeet Kohli", "paperCount": 342, "citationCount": 47692, "hIndex": 97}, {"authorId": "46378362", "name": "M. Botvinick", "paperCount": 239, "citationCount": 53255, "hIndex": 83}, {"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "47002813", "name": "Yujia Li", "paperCount": 47, "citationCount": 9052, "hIndex": 26}, {"authorId": "1996134", "name": "Razvan Pascanu", "paperCount": 131, "citationCount": 32921, "hIndex": 60}], "abstract": "Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. \nThe following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between \"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 193, "citation_count": 1853, "influential_paper_citations": 203, "is_open_access": false, "citations": ["0ee0801ba010a441403f9ed666ef9bf006b3aa07", "8f566001453bc6be0a935bf69ffd90d9db3af32b", "845b4941d8c016aa5f8967da2f86d38ef6c18fa3", "f55781f7ce6fd31e946f0efe76d5bf89858391d1", "30b38ca8151bbd5a5ff45bce94297d1248ff58b5", "f1e5e65941617604923225cc4bf464e370fcae67", "9e20f6874feaaf7c9994f9875b1d9cab17a2fd59", "aa1cda29362b9d670d602c7fc6964499d2a364bd", "a211bc92374dbf5b8935ac42569bc6b147855cbd", "5e7047851d05b2ecef5de451dda5404acda726de"], "references": ["ebff4eb2f94dcf38171a5ca6a24ee95bc8e88c10", "a3eb4996ca07058de31c07a0462aac958c3cb3e7", "fe257027193ea4a74fdab99d7509ce4002ad7de6", "e1799aaf23c12af6932dc0ef3dfb1638f01413d1", "12d64afc8a19b1234a766aba5684036ce7937d0d", "5e49c80f8b12a100c5f4518897c4cbf72710c252", "43879cf527f4918955fd55128baa6745174d8555", "4a18affba68096f53a8a884e4a9ebd34e65d305f", "def1049b5aae96c8e1eab0ca58d77ac9c2f0e3e9", "6c44f8e62d824bcda4f291c679a5518bbd4225f6", "6fe338f8741c41e18f7efbe6767ca5b2b099ce6c", "567a182d7a37c1944fc353abb04c07440009992b", "57c36f13b188816051a27478e2f56bb284f4fb13", "0a69a34408555a6fcdb0491e9bf82af8675876ae", "a1172d0de6164bb7eaadbcdbc10e7b03b773b6ad", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "dcd5b17b26c028b574cfcadd0d4e47b8d169ce5c", "e0155830d8982da4631cb71546fca782b2e00c20", "2afa9966c37b7747d954a4dcd61e986247783683", "249408527106d7595d45dd761dd53c83e5a02613", "f32f16ca3c27ff945198c6551a5d35fae3b1a660", "289fb3709475f5c87df8d97f129af54029d27fee", "bdea8b6ceabaeb86bd23c2d2585da1ff3858d968", "5c1a251a6609ddfb6d6d7425169c8170d95db638", "f3a9749c602968b19c21a83aab11f35ce2fb86d6", "86de01329bd423fee0996d0bd7eb52f097d96926", "6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2", "84032c19bad3493957d1319babd19bde2821fee3", "e156edf6d11dc19580c2e70940645fb107893817", "adfc4a14e319bd4a49b88452c74b03d7ae4400eb", "67b25141426bd0956439bce9aa3f2624d2bd3594", "f615bd164110160e160c98f59d7bfcc931a3cdc1", "4e926648026c73bb5a7af2833d7f5edef83827c5", "5e2bb96c47ccaa16a4e7192e8fadb3b3e1c3acdc", "6a0aaefce8a27a8727d896fa444ba27558b2d381", "8899094797e82c5c185a0893896320ef77f60e64", "4df7bbe3ca7806f39a490c99f17867a0ac299bc3", "572a1f77306e160c3893299c18f3ed862fb5f6d9", "856451974cce2d353d5d8a5a72104984a252375c", "d0f2d7236e43f129744e88130fb71f8f872d2b31", "33998aff64ce51df8dee45989cdca4b6b1329ec4", "d798756dd70dea6064e0cb2a6a9fd159aff84828", "9ba0186ed40656329c421f55ada7313293e13f17", "da1d97dba0a34b1cc9171eb5b0e24d331eceb15c", "eecab00a7c6d58792ec5e620ab1fc37043545a14", "cfa75e0a93337adb1ee4fbc93775b84ca724db2c", "c4c06578f4870e4b126e6837907929f3c900b99f", "b00cea45c85e68002d34b608aecb4fddd18210aa", "e4ee8e2fcbdf2b65ca39c28f36c81d4c56b82d0d", "0fe30b40d443dfd808a64bc5a8d62a9875adc872", "bd1d59433e3b7ae7207c24b4cd1838acea91425c", "0c31441de0e50b3a76a2e7908835d42a815a7e7f", "3e2742cbc683e2422e504f248ccf63f1a7983c69", "5808f5285bc60a73bc240621ad0fce606867ebc1", "a625dee1fb23d1c842a961bc354c0aef3a20c132", "921cfea6806403486d3bf42c41f4d601fc4eea04", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "6b7d6e6416343b2a122f8416e69059ce919026ef", "007112213ece771be72cbecfd59f048209facabd", "099cdb087f240352a02286bf9a3e7810c7ebb02b", "f306b1a973d9fa8c693036ca75fa8e30ad709635", "e24cdf73b3e7e590c2fe5ecac9ae8aa983801367", "98e8b2f6a8583e83ce0159dd29cf5d848adcbd24", "a456265138c088a894301c0433dae938705a9bec", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "204a4a70428f3938d2c538a4d74c7ae0416306d8", "15f91ae7590b88c4a533daeefa4b6fa7c0c277e5", "a2155552ca5afb784a3c1d67a5bcbd4e688b6e05", "d997beefc0922d97202789d2ac307c55c2c52fba", "d7878c2044fb699e0ce0cad83e411824b1499dc8", "f09f7888aa5aeaf88a2a44aea768d9a8747e97d2", "0e779fd59353a7f1f5b559b9d65fa4bfe367890c", "282a380fb5ac26d99667224cef8c630f6882704f", "75bcac37b154eec4623c1423d7b330fc2055a67e", "3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7", "a1786540a4e15f0757e1b84a02f98ed436a969e0", "53e6f10e70393bdbafa98238272977760196fddf", "644ca74f80463415613847ab01cff067fb58f0ad", "edbc873a248768a626ef2bc57b3d1eff30de0e11", "fdadf9f863a3d09e42f7fcdbbd8d3e088679ebbd", "36eff562f65125511b5dfab68ce7f7a943c27478", "405c31c85a324942811f3c9dc53ce3528f9284df", "b5c26ab8767d046cb6e32d959fdf726aee89bb62", "213bdfd1ff527f4ce1c298f6f116a4b4240d7425", "eb597cceec6e0889d1423ae688e8854bfa6f822d", "ae42c0cff384495683192b06bd985cdd7a54632a", "784ee73d5363c711118f784428d1ab89f019daa5", "a438fbb5124d446b4ef6a78c5c1a161e26d967c2", "376f23cce537235122fdce5524d084e3a869c403", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "36ee2c8bd605afd48035d15fdc6b8c8842363376", "c41eb895616e453dcba1a70c9b942c5063cc656c", "e02f59cf876cb40233573ff78a1609f969d301cc", "50295c19e177480ba3599300de1ab837cc62b08c", "7c6de5a9e02a779e24504619050c6118f4eac181", "0772905d40b9afa3dc087a88184f09f3b3e1464f", "7260c0692f8d265e11c4e9c4c8ef4c185bd587ad", "2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1", "322cf9bcde458a45eaeca989a1eec92f7c6db984", "561c3fa53d36405186da9cab02bd68635c3738aa", "5c077b3ad4de4f2ea99561908aa9be1520f18a14", "846aedd869a00c09b40f1f1f35673cb22bc87490", "a2499dd426c46c645ee805d7594b6687547c72d4", "b59d91e0699d4e1896a15bae13fd180bdaf77ea5", "492f57ee9ceb61fb5a47ad7aebfec1121887a175", "21c99706bb26e9012bfb4d8d48009a3d45af59b2", "4e88a9e28c4a4a4804700809df13ab822ac6a6ad", "815c84ab906e43f3e6322f2ca3fd5e1360c64285", "5d1bfeed240709725c78bc72ea40e55410b373dc", "93499a7c7f699b6630a86fad964536f9423bb6d0", "e49ff72d420c8d72e62a9353e3abc053445e59bd", "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "b36b7f7c68923d14ba2859b5d28a1124616a8c89", "60e801e3dfc9812e294ed9de6d579e0293d61643", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "0834e74304b547c9354b6d7da6fa78ef47a48fa8", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d", "4d376d6978dad0374edfa6709c9556b42d3594d3", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "193edd20cae92c6759c18ce93eeea96afd9528eb", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "8a756d4d25511d92a45d0f4545fa819de993851d", "0289597a130c7cb4fc543cbe0d8c779b829fce96", "0b544dfe355a5070b60986319a3f51fb45d1348e", "fff114cbba4f3ba900f33da574283e3de7f26c83", "5e925a9f1e20df61d1e860a7aa71894b35a1c186", "34f25a8704614163c4095b3ee2fc969b60de4698", "2582ab7c70c9e7fcb84545944eba8f3a7f253248", "29ade9f04f11dd8d434f051563f03928ed62c21b", "687bac2d3320083eb4530bf18bb8f8f721477600", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "9664ab17170e0442f35e27d8b3fac7398aa12d08", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "27e38351e48fe4b7da2775bf94341738bc4da07e", "15cad06c606ce6c4f88e186d4e3bbb78aca648e3", "cfa2646776405d50533055ceb1b7f050e9014dcb", "9c0ddf74f87d154db88d79c640578c1610451eec", "317794c81f54371dda5950a5ee7a41ed10298ab2", "7e1874986cf6433fabf96fff93ef42b60bdc49f8", "6e1a3ec1552109b66fa05b080c8c9e4120d38cd9", "d0a9b181fc252108de45720d4645ac245e1ba463", "3efd851140aa28e95221b55fcc5659eea97b172d", "104c4017c200f434dc7ecfbef143b5f135497abc", "6c799aedea37d239a925bf368e0b7753407a14af", "b8f57509a228f1c84bf67094ec1fa8a99407368b", "737e2061ec4e343e84191d558e63ac09235dfbe1", "6cf35ec34efa592f83e3a1b748aea14957fc784a", "7e9690058b4f04875a2c7781efd1247a43bf6747", "16d3b1859b935d5ec36116f69b1bfce9df6bf8c4", "54fa671463d476a2c41ff3e4b8129b07b77c099e", "14f16a6d737a82a8b026ba6e378f84eb1e5d377c", "ee557d8918504b3098de11e696b9b5c484702ae1", "9ca9f28676ad788d04ba24a51141a9a0a0df4d67", "769bfd4a4b45979cf83bb56c054ebcaaaf8b35d7", "cf7d7684600d3ebe916ca093eda123a9dad41459", "134152cb1d5803351756c65d8321c7dbc47a954d", "69e68bfaadf2dccff800158749f5a50fe82d173b", "605d738a39df3c5e596613ab0ca6925f0eecdf35", "b8d9ed504f70e38f9674df05bf3aadf729b5d379", "bae3eda9605700b14237f4d04652ab6759c68eef", "62064218665ad89f0cb2a44f5b19f7703d9c7e71", "e168d9ed5147cbd7cce56a84b79f75daa938617a", "053a25c6b6b63f95fb4e1577f0d4cf26eacae0a1", "05581f42968029766c5877329fe2ca9f60461a6d", "564427596799f7967c91934966cd3c6bd31cb06d", "cd7cb71039105686bc041de166ad7449176b2ac5", "a34e35dbbc6911fa7b94894dffdc0076a261b6f0", "9438172bfbb74a6a4ea4242b180d4335bb1f18b7", "6a835df43fdc2f79126319f6fa033bb42147c6f6", "71dd4d477ca17b4db3b270d25225822ff3a41fac", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93", "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7", "052008dc9dca0f5d7ad0f9a856fee3e8ee9103e9", "bb0419bccc2244ed33c9c42341f342511262daa3", "eb324f42d42dc29d9f89e044a76516227e4e2c66", "e6af68db1e6a291cbf6afb265f5fc2e82423b71b", "9f7d9abb2277e924a291033d5c3d1195989e80ff", "c547e1f79e6039d05c5ae433a36612d7f8e4d3f5", "9b486c647916df9f8be0f8d4fc5c94c493bfaa80", "cccc0a4817fd5f6d8758c66b4065a23897d49f1d", "07941d79ab0f885349121b2e5a369afb3fc57eaf", "9d90cb376b472d079b50282f7d253ac8b33e8149", "ee66f87c06337fb430a90897112de06fb61f6a9f", "5703c2643f7e8012b9bfb2ed53f34114acfc10f8"], "url": "https://www.semanticscholar.org/paper/19b7769dab4e6092aa4b7eeb8aa078a7b725c9b4"}, "1db9bd18681b96473f3c82b21edc9240b44dc329": {"id": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer", "authors": [{"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}, {"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "31702389", "name": "Alexander Ku", "paperCount": 18, "citationCount": 1905, "hIndex": 11}, {"authorId": "47497262", "name": "Dustin Tran", "paperCount": 66, "citationCount": 5150, "hIndex": 30}], "abstract": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. We propose another extension of self-attention allowing it to efficiently take advantage of the two-dimensional nature of images. While conceptually simple, our generative models trained on two image data sets are competitive with or significantly outperform the current state of the art in autoregressive image generation on two different data sets, CIFAR-10 and ImageNet. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve significantly over previously published autoregressive super-resolution models. Images they generate fool human observers three times more often than the previous state of the art.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 24, "citation_count": 826, "influential_paper_citations": 42, "is_open_access": false, "citations": ["268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "21b786b3f870fc7fa247c143aa41de88b1fc6141", "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "2def61f556f9a5576ace08911496b7c7e4f970a4", "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "c143ea9e30b1f2d93a9c060253845423f9e60e1f", "220d47d9b2872f0f0c33084fb4425424d91fa0ac", "dbe077f8521ecbe0a1477d6148c726d4f053d9c9"], "references": ["642c1b4a9da95ea4239708afc5929a5007a1870d", "d1c424c261c577958917055f72fb9e2ad0348865", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "aa6f7ad0a06b52a8be89dbd8d056561418276ff2", "8578744cea4cf7b65ee8a51c64f766b4c31a2e7e", "ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921", "488bb25e0b1777847f04c943e6dbc4f84415b712", "b01871c114b122340209562972ff515b86b16ccf", "df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3", "0936352b78a52bc5d2b5e3f04233efc56664af51", "6e90fd78e8a3b98af3954aae5209703aa966603e", "13fe71da009484f240c46f14d9330e932f8de210", "8388f1be26329fa45e5807e968a641ce170ea078", "0875fc92cce33df5cf7df169590dbf0ca00d2652", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "f4278dac2abafb66a0a824f1d59e72b67e7540a1", "34f25a8704614163c4095b3ee2fc969b60de4698", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "32f078a7478d1ec2169599500a4507aceaccdda7"], "url": "https://www.semanticscholar.org/paper/1db9bd18681b96473f3c82b21edc9240b44dc329"}, "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f": {"id": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers", "authors": [{"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "73416451", "name": "Youlong Cheng", "paperCount": 11, "citationCount": 553, "hIndex": 5}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "47497262", "name": "Dustin Tran", "paperCount": 66, "citationCount": 5150, "hIndex": 30}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "3344182", "name": "Penporn Koanantakool", "paperCount": 12, "citationCount": 396, "hIndex": 7}, {"authorId": "2052793706", "name": "Peter Hawkins", "paperCount": 5, "citationCount": 936, "hIndex": 4}, {"authorId": "34946720", "name": "HyoukJoong Lee", "paperCount": 31, "citationCount": 3046, "hIndex": 21}, {"authorId": "2069751443", "name": "Mingsheng Hong", "paperCount": 2, "citationCount": 304, "hIndex": 2}, {"authorId": "39660914", "name": "C. Young", "paperCount": 77, "citationCount": 12886, "hIndex": 29}, {"authorId": "35474601", "name": "Ryan Sepassi", "paperCount": 8, "citationCount": 2097, "hIndex": 7}, {"authorId": "3135881", "name": "Blake A. Hechtman", "paperCount": 20, "citationCount": 1002, "hIndex": 13}], "abstract": "Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the \"batch\" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 52, "citation_count": 249, "influential_paper_citations": 28, "is_open_access": false, "citations": ["3cfb319689f06bf04c2e28399361f414ca32c4b3", "7a064df1aeada7e69e5173f7d4c8606f4470365b", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "fdacf2a732f55befdc410ea927091cad3b791f13", "c18663fea10c8a303d045fd2c1f33cacf9b73ca3", "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "1882f194cb43828852cc052887671e55a80f945a", "094ff971d6a8b8ff870946c9b3ce5aa173617bfb"], "references": ["f971658ab845d7573c4bbb760d5e7e5332025254", "abd8cf4e6a1f3a4c61ac46a58bfa86c904d8d546", "3ea088eae8637530d1108065acab244f3b6c280d", "c39c1e0eec76c3bad44d088aad0d9cfb8e95f2e2", "9654d0807b807ec7c0a78f717b0dc3d4dcb1ad7c", "16b0e58f16e3336311fdb4a40ffb4755edca31ca", "2f541c71646b330cb52f9b60957c950dcb75fcaf", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "510e26733aaff585d65701b9f1be7ca9d5afc586", "a845b1493914f7ea7fcb45cd140d38515a95b556", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "132540b32d857d1841b028184036ff24c496a31b", "f20340ebc8f466fc10b46015ccfc6dfaa61ea40f", "ce1109de9fd72ad10da896211ca50a0ff0f292ed", "b8106b64eebf3b7dab5ddbc2226fc3c5f000176f", "fdf57b4bca2ccf01ae5a00a8b3e761270bdf32ea", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "3127190433230b3dc1abd0680bb58dced4bcd90e", "3ffae44b736b6dbd6d715977bb6381297dd94304", "f2905b93d056f3c71a4ac92b737a4c77680ce28e", "3492a16bccda6e64cd9eee62aabcab304c16ce7e", "6f4e48c2a5de9337d147ebbb7d0ff0e555adceca", "fd6d2cf4ec3d765ecd084a747ab4c1d71d8609e0", "b582d4a005c3288858eb3910e9233edb35323f49", "34d51059c4b97a888970354be43603fa4fa86c84", "831cc6d9b7a333b38d34d923b52aed438e90ee1e"], "url": "https://www.semanticscholar.org/paper/2270b8628fd8ca67ae39d277f45bc3c38ac63d5f"}, "2d08ed53491053d84b6de89aedbf2178b9c8cf84": {"id": "2d08ed53491053d84b6de89aedbf2178b9c8cf84", "title": "Fast Decoding in Sequence Models using Discrete Latent Variables", "authors": [{"authorId": "69894932", "name": "\u0141ukasz Kaiser", "paperCount": 6, "citationCount": 310, "hIndex": 2}, {"authorId": "39788470", "name": "Aurko Roy", "paperCount": 24, "citationCount": 2522, "hIndex": 13}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "1751569", "name": "Samy Bengio", "paperCount": 379, "citationCount": 51213, "hIndex": 85}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}], "abstract": "Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difficult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallelizable during training, yet still operate sequentially during decoding. \nInspired by [arXiv:1711.00937], we present a method to extend sequence models using discrete latent variables that makes decoding much more parallelizable. We first auto-encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel. To this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with previously introduced methods. Finally, we evaluate our model end-to-end on the task of neural machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. While lower in BLEU than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 49, "citation_count": 175, "influential_paper_citations": 22, "is_open_access": false, "citations": ["75acc731bdd2b626edc74672a30da3bc51010ae8", "15e81c8d1c21f9e928c72721ac46d458f3341454", "f87de21b46683b5743c4d82af3c9cb8bbcd26f21", "5e3a59695261f03aa3f09a8a5ac6166fb63e0a2e", "3360768fcb265a8b7c1ee5ba7cfe03de0e2fad62", "d9291740b644fc5feb4999c76ec2f50457ef3a77", "1e5b826ddf0754f6e93234ba1260bd939c255e7f", "51429728ca33b034270289345dc8343f5fc9a36b", "87639a90e0ab573236efeb79cf24efafc2463dcf", "9da95e99afd4ea899bd1fb40dd350e0be0a12a84"], "references": ["a74eacdd133918f82b953fc8dcd615ddca7efdfa", "6503c617db449861b96043b2bc892a1b4cb40a95", "7570afa31c68e24fce1342b7d67c591787219bc1", "1723f1bb6fa033d638d0127e056470a9431246c9", "15e81c8d1c21f9e928c72721ac46d458f3341454", "4bf6ce4a9366cdba069a45651606538f2febd8e6", "682d194235ba3b573889836ba118502e8b525728", "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83", "f466157848d1a7772fb6d02cdac9a7a5e7ef982e", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "43428880d75b3a14257c3ee9bda054e61eb869c0", "7d77a29f2e1dc796d202d6cf01f299da7c197c22", "a642bbbaf8822565f9b812ea279c596cc54ce4c3", "29e944711a354c396fad71936f536e83025b6ce0", "515a21e90117941150923e559729c59f5fdade1c", "98445f4172659ec5e891e031d8202c102135c644", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "df0402517a7338ae28bc54acaac400de6b456a46", "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "266e8622d57457ad76224649c6b00adf23c0b76d", "5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "d82b55c35c8673774a708353838918346f6c006f", "3e47c4c2dd98c49b7771c7228812d5fd9eee56a3", "eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "018300f5f0e679cee5241d9c69c8d88e00e8bf31", "484ad17c926292fbe0d5211540832a8c8a8e958b", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "0e2261c75bf0ac263ff4d2eaf90f996d82622633", "4748d22348e72e6e06c2476486afddbc76e5eca7", "b71ac1e9fb49420d13e084ac67254a0bbd40f83f", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "c50dca78e97e335d362d6b991ae0e1448914e9a3", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "4c915c1eecb217c123a36dc6d3ce52d12c742614", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "62e8aa57505cf982c8f2f546b02d79309e713a04", "793df5d30b0a041aa0fa1777c8461d214325cf5c", "ee90bfa64cea003aa1b0669e2a960f846f931b86", "b32ee423671b727523a0dfdfea039e7a7fca36b8", "5f69ba8c1824d79808f001737ac17b274b908c5a"], "url": "https://www.semanticscholar.org/paper/2d08ed53491053d84b6de89aedbf2178b9c8cf84"}, "2da824e19bd49a2e37739421fa003818c413946f": {"id": "2da824e19bd49a2e37739421fa003818c413946f", "title": "An Improved Relative Self-Attention Mechanism for Transformer with Application to Music Generation", "authors": [{"authorId": "7900665", "name": "Cheng-Zhi Anna Huang", "paperCount": 26, "citationCount": 1043, "hIndex": 11}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "41231781", "name": "Curtis Hawthorne", "paperCount": 26, "citationCount": 1520, "hIndex": 16}, {"authorId": "2555924", "name": "Andrew M. Dai", "paperCount": 71, "citationCount": 10296, "hIndex": 32}, {"authorId": "28552618", "name": "M. Hoffman", "paperCount": 102, "citationCount": 16748, "hIndex": 37}, {"authorId": "2396681", "name": "D. Eck", "paperCount": 107, "citationCount": 6782, "hIndex": 40}], "abstract": "Music relies heavily on self-reference to build structure and meaning. We explore the Transformer architecture [27] as a generative model for music, as self-attention has shown compelling results on tasks that require long-term structure such as Wikipedia summary generation [18]. However, timing information is critical for polyphonic music, and Transformer does not explicitly model absolute or relative timing in its structure. To address this challenge, Shaw et al. [22] introduced relative position representations to self-attention to improve machine translation. However, the formulation was not scalable to longer sequences. We propose an improved formulation which reduces the memory requirements of the relative position computation from O(ld) to O(ld), where l is the length of sequences and d is the hidden size, making it possible to train much longer sequences and achieve faster convergence. In experiments on symbolic music we find that relative selfattention substantially improves sample quality for unconditioned generation and is able to generate sequences of lengths longer than those from the training set. When primed with an initial sequence, the model generates continuations that develop the prime coherently and exhibit long-term structure. Relative self-attention can be instrumental in capturing richer relationships within a musical piece 2 3.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 30, "citation_count": 61, "influential_paper_citations": 4, "is_open_access": false, "citations": ["c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "21da617a0f79aabf94272107184606cefe90ab75", "36e30516683032634975c53e60f3737b6e35ff80", "5ad465e098052d0e645fe6f92d94ffa7282d5496", "a9ec03dbe702f6909acd1f1f14a3395d0141043b", "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "84476fdf6ead3553f4493dff8e02308439d6222b", "781d11168bd189d4fa3a9a2d6644215504e189d0", "75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1", "94576783bc73bf55a0091203a3d45a0a4665a1ae"], "references": ["0ec95fac7c5a923862ef4d443bbda2bd1e37f8a1", "a6e4beb28b345fce7470da122b4e45e2cd0dcd12", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "7570afa31c68e24fce1342b7d67c591787219bc1", "889c3b4394826639d483c039467cd9a05e68e73c", "4275d4c4bd10742b321467f175f16198ed7d17d7", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "a066233a0b52a0e07197540ff2c73f42daf198f4", "54ca2690b17d4381b562d3cbc95a753d6fb3318f", "a0295975f9e9c662b99cb007c6d14a226334ad57", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "3cccc93064dae265eeb7bc76d02cdc67c942f0a5", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "9a2acc56f9d262767472d69f81df6bddefabf01f", "705fd4febe2fff810d2f72f48dcda20826eca77a", "18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8", "32f078a7478d1ec2169599500a4507aceaccdda7", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "2f83f6e1afadf0963153974968af6b8342775d82", "0f3bd4e6b16817e332b9be8357e3eddb8a461990", "a4ffd2f5dd98ee744c013060c5bc06503336d931", "16517d233873a40d7fd5b79250946082ed6bfea1", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "4f7476037408ac3d993f5088544aab427bc319c1", "603bdbb17ba1f909280405a076455ac4f878fbf3"], "url": "https://www.semanticscholar.org/paper/2da824e19bd49a2e37739421fa003818c413946f"}, "4ced2086c4a87ee381b256d2e717ea00073d420a": {"id": "4ced2086c4a87ee381b256d2e717ea00073d420a", "title": "Towards a better understanding of Vector Quantized Autoencoders", "authors": [{"authorId": "39788470", "name": "Aurko Roy", "paperCount": 24, "citationCount": 2522, "hIndex": 13}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "2072676", "name": "Arvind Neelakantan", "paperCount": 30, "citationCount": 9338, "hIndex": 18}], "abstract": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image reconstruction results on SVHN together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 46, "citation_count": 9, "influential_paper_citations": 1, "is_open_access": false, "citations": ["34e75b1eb986ceab79e5c463cf1a82bcf4e87944", "9fb60d02e9b7a94a6adc1575d583d5a64c10da00", "b74937a4408fc1cd2d01cdeef633373923bfe991", "b83763308e481e029e6903a2bbca7a19070a44ae", "31554ea02f510879ddff96136d30a1124e82099c", "8e23606793d4ce455302b110f50033c6e241d9aa", "d5b992bce205a6bbed4a62f1bd6c8983371f6259", "6576ca8f779d745c9b75a138763cc0ef9a405a54", "e92a26fc08e149102860b7af92b686cedf9defce"], "references": ["48925fef94500cf19ee220ed74217816f1ab5e60", "2d08ed53491053d84b6de89aedbf2178b9c8cf84", "9c5c89199114858eafbe50b46d77d38ffd03b28a", "1723f1bb6fa033d638d0127e056470a9431246c9", "15e81c8d1c21f9e928c72721ac46d458f3341454", "682d194235ba3b573889836ba118502e8b525728", "45dfef0cc1ed96558c1c650432ce39d6a1050b6a", "f466157848d1a7772fb6d02cdac9a7a5e7ef982e", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "2d1b8f60f2724efd6c9344870fb60e8525157d70", "977560251c2bd4c28a6c7c707c29f4091c5e6247", "a642bbbaf8822565f9b812ea279c596cc54ce4c3", "6ce1922802169f757bbafc6e087cc274a867c763", "66386a946a04534275bd466862364d139790f41f", "29e944711a354c396fad71936f536e83025b6ce0", "515a21e90117941150923e559729c59f5fdade1c", "b01871c114b122340209562972ff515b86b16ccf", "6a97d2668187965743d1b825b306defccbabbb4c", "df0402517a7338ae28bc54acaac400de6b456a46", "57a10537978600fd33dcdd48922c791609a4851a", "0936352b78a52bc5d2b5e3f04233efc56664af51", "0c908739fbff75f03469d13d4a1a07de3414ee19", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "484ad17c926292fbe0d5211540832a8c8a8e958b", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "0e2261c75bf0ac263ff4d2eaf90f996d82622633", "02227c94dd41fe0b439e050d377b0beb5d427cda", "b452a856a3e3d4d37b1de837996aa6813bedfdcf", "226966243877f186d346be01047cf71cee1b5ec4", "4c915c1eecb217c123a36dc6d3ce52d12c742614", "606f7ac92d5170fab86e012c93fb95202d6a9823", "2352d9105de31032538900dfb2ce7c95f6402963", "266a32615cdc2028eb743dc19d28242c8e67e357", "d36efb9ad91e00faa334b549ce989bfae7e2907a", "ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed"], "url": "https://www.semanticscholar.org/paper/4ced2086c4a87ee381b256d2e717ea00073d420a"}, "642c1b4a9da95ea4239708afc5929a5007a1870d": {"id": "642c1b4a9da95ea4239708afc5929a5007a1870d", "title": "Tensor2Tensor for Neural Machine Translation", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "1751569", "name": "Samy Bengio", "paperCount": 379, "citationCount": 51213, "hIndex": 85}, {"authorId": "2445241", "name": "E. Brevdo", "paperCount": 27, "citationCount": 11311, "hIndex": 13}, {"authorId": "1565641737", "name": "Fran\u00e7ois Chollet", "paperCount": 22, "citationCount": 10847, "hIndex": 12}, {"authorId": "19177000", "name": "Aidan N. Gomez", "paperCount": 32, "citationCount": 48744, "hIndex": 13}, {"authorId": "2776283", "name": "Stephan Gouws", "paperCount": 22, "citationCount": 7162, "hIndex": 12}, {"authorId": "145024664", "name": "Llion Jones", "paperCount": 21, "citationCount": 50175, "hIndex": 15}, {"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "2583391", "name": "Nal Kalchbrenner", "paperCount": 35, "citationCount": 30929, "hIndex": 25}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "35474601", "name": "Ryan Sepassi", "paperCount": 8, "citationCount": 2097, "hIndex": 7}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}], "abstract": "Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 20, "citation_count": 394, "influential_paper_citations": 43, "is_open_access": false, "citations": ["295065d942abca0711300b2b4c39829551060578", "1fa9ed2bea208511ae698a967875e943049f16b6", "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "726320cdbd04804ffa8f3a78c095bd1b55a2a695", "d08463bd665589d04619f04dbde84183ffcf2e63", "b59233aab8364186603967bc12d88af48cc0992d", "90afc52bbacf37e6d3ca16ead35499f661222d7d", "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "1db9bd18681b96473f3c82b21edc9240b44dc329"], "references": ["1db9bd18681b96473f3c82b21edc9240b44dc329", "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83", "3a6d4cd0768ae8768e733280d362bdb4d25924e7", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "43428880d75b3a14257c3ee9bda054e61eb869c0", "510e26733aaff585d65701b9f1be7ca9d5afc586", "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "98445f4172659ec5e891e031d8202c102135c644", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "df0402517a7338ae28bc54acaac400de6b456a46", "b60abe57bc195616063be10638c6437358c81d1e", "46200b99c40e8586c8a0f588488ab6414119fb28", "1af68821518f03568f913ab03fc02080247a27ff", "eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/642c1b4a9da95ea4239708afc5929a5007a1870d"}, "6f6127faf516fb4dc47a24aaab9d5e96c4fcc751": {"id": "6f6127faf516fb4dc47a24aaab9d5e96c4fcc751", "title": "Theory and Experiments on Vector Quantized Autoencoders", "authors": [{"authorId": "39788470", "name": "Aurko Roy", "paperCount": 24, "citationCount": 2522, "hIndex": 13}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2072676", "name": "Arvind Neelakantan", "paperCount": 30, "citationCount": 9338, "hIndex": 18}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}], "abstract": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 40, "citation_count": 55, "influential_paper_citations": 7, "is_open_access": false, "citations": ["c17985a669522e7e85ae3d34754c7df49c7187d1", "5e3a59695261f03aa3f09a8a5ac6166fb63e0a2e", "3517b9824def42f3c723c6c63eda7ade12d25538", "51429728ca33b034270289345dc8343f5fc9a36b", "12e524ff107ede4e653ad06391be9585ad8d5fa0", "d6cfb4e345b1031040ccd3683730854c560a2b0d", "fb50f6f4f81f361bf1c6dbc93cc8fab5aee12fdf", "310f52474578c3375ec98b383654e3d0bc0ff54a", "775f6da764a134e7bd0e361c27f7d7411afef4d3", "4a70df03329fe23dc454273e87a95fa39396dd88"], "references": ["2d08ed53491053d84b6de89aedbf2178b9c8cf84", "9c5c89199114858eafbe50b46d77d38ffd03b28a", "1723f1bb6fa033d638d0127e056470a9431246c9", "682d194235ba3b573889836ba118502e8b525728", "f466157848d1a7772fb6d02cdac9a7a5e7ef982e", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "2d1b8f60f2724efd6c9344870fb60e8525157d70", "977560251c2bd4c28a6c7c707c29f4091c5e6247", "a642bbbaf8822565f9b812ea279c596cc54ce4c3", "6ce1922802169f757bbafc6e087cc274a867c763", "66386a946a04534275bd466862364d139790f41f", "29e944711a354c396fad71936f536e83025b6ce0", "515a21e90117941150923e559729c59f5fdade1c", "b01871c114b122340209562972ff515b86b16ccf", "6a97d2668187965743d1b825b306defccbabbb4c", "df0402517a7338ae28bc54acaac400de6b456a46", "57a10537978600fd33dcdd48922c791609a4851a", "0936352b78a52bc5d2b5e3f04233efc56664af51", "266e8622d57457ad76224649c6b00adf23c0b76d", "0c908739fbff75f03469d13d4a1a07de3414ee19", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "018300f5f0e679cee5241d9c69c8d88e00e8bf31", "484ad17c926292fbe0d5211540832a8c8a8e958b", "0e2261c75bf0ac263ff4d2eaf90f996d82622633", "b452a856a3e3d4d37b1de837996aa6813bedfdcf", "226966243877f186d346be01047cf71cee1b5ec4", "4c915c1eecb217c123a36dc6d3ce52d12c742614", "606f7ac92d5170fab86e012c93fb95202d6a9823", "2352d9105de31032538900dfb2ce7c95f6402963", "266a32615cdc2028eb743dc19d28242c8e67e357", "d36efb9ad91e00faa334b549ce989bfae7e2907a", "ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed"], "url": "https://www.semanticscholar.org/paper/6f6127faf516fb4dc47a24aaab9d5e96c4fcc751"}, "7dd811ba1bf177f905288afae31566945b77caa5": {"id": "7dd811ba1bf177f905288afae31566945b77caa5", "title": "Large Scale Multi-Domain Multi-Task Learning with MultiModel", "authors": [{"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "19177000", "name": "Aidan N. Gomez", "paperCount": 32, "citationCount": 48744, "hIndex": 13}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "145024664", "name": "Llion Jones", "paperCount": 21, "citationCount": 50175, "hIndex": 15}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}], "abstract": null, "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 0, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": [], "url": "https://www.semanticscholar.org/paper/7dd811ba1bf177f905288afae31566945b77caa5"}, "8ba5f5106cd039a22f0d7fdf97d700e427dde282": {"id": "8ba5f5106cd039a22f0d7fdf97d700e427dde282", "title": "Music Transformer", "authors": [{"authorId": "7900665", "name": "Cheng-Zhi Anna Huang", "paperCount": 26, "citationCount": 1043, "hIndex": 11}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "35577716", "name": "Ian Simon", "paperCount": 33, "citationCount": 4030, "hIndex": 19}, {"authorId": "41231781", "name": "Curtis Hawthorne", "paperCount": 26, "citationCount": 1520, "hIndex": 16}, {"authorId": "2555924", "name": "Andrew M. Dai", "paperCount": 71, "citationCount": 10296, "hIndex": 32}, {"authorId": "28552618", "name": "M. Hoffman", "paperCount": 102, "citationCount": 16748, "hIndex": 37}, {"authorId": "47153176", "name": "Monica Dinculescu", "paperCount": 21, "citationCount": 631, "hIndex": 9}, {"authorId": "2396681", "name": "D. Eck", "paperCount": 107, "citationCount": 6782, "hIndex": 40}], "abstract": "Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 23, "citation_count": 139, "influential_paper_citations": 13, "is_open_access": false, "citations": ["21da617a0f79aabf94272107184606cefe90ab75", "0c5bc409e62e65f86838968a2a7cdae5fa0b288b", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "27ac832ee83d8b5386917998a171a0257e2151e2", "7c5c149699a0ba54b52cd5b9e291077f4a1f9d13", "fc46ccb83dc121c33de7ab6bdedab7d970780b2f", "b2a839e3ee68e81b863b73ee08c6626c94477fef", "054ab6f0e392c3580a364814144babf16bd2d2dd", "6a70f88b77a88db4102af25aadd66952944ac5d7", "2b39bcfca5f7dac199632e384692ef669ef2157f"], "references": ["0ec95fac7c5a923862ef4d443bbda2bd1e37f8a1", "a6e4beb28b345fce7470da122b4e45e2cd0dcd12", "642c1b4a9da95ea4239708afc5929a5007a1870d", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "1db9bd18681b96473f3c82b21edc9240b44dc329", "7570afa31c68e24fce1342b7d67c591787219bc1", "f83ef3250ba1166d7c1c7585da7dd78e0641fae7", "889c3b4394826639d483c039467cd9a05e68e73c", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "a066233a0b52a0e07197540ff2c73f42daf198f4", "54ca2690b17d4381b562d3cbc95a753d6fb3318f", "a0295975f9e9c662b99cb007c6d14a226334ad57", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "3cccc93064dae265eeb7bc76d02cdc67c942f0a5", "705fd4febe2fff810d2f72f48dcda20826eca77a", "18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8", "32f078a7478d1ec2169599500a4507aceaccdda7", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "0f3bd4e6b16817e332b9be8357e3eddb8a461990", "a4ffd2f5dd98ee744c013060c5bc06503336d931", "4f7476037408ac3d993f5088544aab427bc319c1"], "url": "https://www.semanticscholar.org/paper/8ba5f5106cd039a22f0d7fdf97d700e427dde282"}, "8ce5e6787943be32a695c9a590a66de1021943f6": {"id": "8ce5e6787943be32a695c9a590a66de1021943f6", "title": "Visualizing Music Self-Attention", "authors": [{"authorId": "2054861179", "name": "A. Huang", "paperCount": 6, "citationCount": 19, "hIndex": 3}, {"authorId": "47153176", "name": "Monica Dinculescu", "paperCount": 21, "citationCount": 631, "hIndex": 9}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2396681", "name": "D. Eck", "paperCount": 107, "citationCount": 6782, "hIndex": 40}], "abstract": "Like language, music can be represented as a sequence of discrete symbols that 1 form a hierarchical syntax, with notes being roughly like characters and motifs 2 of notes like words. Unlike text however, music relies heavily on repetition on 3 multiple timescales to build structure and meaning. The Music Transformer has 4 shown compelling results in generating music with structure [3]. In this paper, 5 we introduce a tool for visualizing self-attention on polyphonic music with an 6 interactive pianoroll. We use music transformer as both a descriptive tool and a 7 generative model. For the former, we use it to analyze existing music to see if the 8 resulting self-attention structure corroborates with the musical structure known 9 from music theory. For the latter, we inspect the model\u2019s self-attention during 10 generation, in order to understand how past notes affect future ones. We also 11 compare and contrast the attention structure of regular attention to that of relative 12 attention [6, 3], and examine its impact on the resulting generated music. For ex13 ample, for the JSB Chorales dataset, a model trained with relative attention is more 14 consistent in attending to all the voices in the preceding timestep and the chords 15 before, and at cadences to the beginning of a phrase, allowing it to create an arc. 16 We hope that our analyses will offer more evidence for relative self-attention as a 17 powerful inductive bias for modeling music. We invite the reader to checkout video 18 animations of music attention and interact with the visualizations at https:// 19 storage.googleapis.com/nips-workshop-visualization/index.html. 20", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 6, "citation_count": 3, "influential_paper_citations": 0, "is_open_access": false, "citations": ["dd618e8ece2d2a248487522c987df685258c047b", "fa6bdc6a2ec326a69fb6ef40ebab34beb86ca114", "aabba0d03cebb59d3c27326df6e3be4a43434d26"], "references": ["0ec95fac7c5a923862ef4d443bbda2bd1e37f8a1", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "584787afd1d01034117047f6c03ff79735ec3c14", "b624504240fa52ab76167acfe3156150ca01cf3b", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"], "url": "https://www.semanticscholar.org/paper/8ce5e6787943be32a695c9a590a66de1021943f6"}, "c8efcc854d97dfc2a42b83316a2109f9d166e43f": {"id": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations", "authors": [{"authorId": "38759328", "name": "Peter Shaw", "paperCount": 17, "citationCount": 1422, "hIndex": 8}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 17, "citation_count": 1174, "influential_paper_citations": 168, "is_open_access": true, "citations": ["3cfb319689f06bf04c2e28399361f414ca32c4b3", "05f5f8b2065a520846d89771ebaea2bb1534e9c6", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "0ca7d8c3250d43d14fdde46bf6fc299654d861ef", "5156381d63bb3e873533b08f203cb56c2d79b6c9", "0c5bc409e62e65f86838968a2a7cdae5fa0b288b", "47ae807cd511b35e78a2cd4e198283dea6dafd41", "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "805a6d1df9f460abfcea3d51d181cf1e80680be4"], "references": ["33998aff64ce51df8dee45989cdca4b6b1329ec4", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "43428880d75b3a14257c3ee9bda054e61eb869c0", "98445f4172659ec5e891e031d8202c102135c644", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "93499a7c7f699b6630a86fad964536f9423bb6d0", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e"], "url": "https://www.semanticscholar.org/paper/c8efcc854d97dfc2a42b83316a2109f9d166e43f"}, "490c5796f6994715e477b84332ff8dfe4bcbda42": {"id": "490c5796f6994715e477b84332ff8dfe4bcbda42", "title": "Decoding the neural representation of story meanings across languages", "authors": [{"authorId": "145707560", "name": "Morteza Dehghani", "paperCount": 91, "citationCount": 1640, "hIndex": 24}, {"authorId": "3135812", "name": "Reihane Boghrati", "paperCount": 33, "citationCount": 732, "hIndex": 12}, {"authorId": "48266060", "name": "K. Man", "paperCount": 14, "citationCount": 320, "hIndex": 6}, {"authorId": "4232341", "name": "J. Hoover", "paperCount": 73, "citationCount": 871, "hIndex": 14}, {"authorId": "40026778", "name": "Sarah I. Gimbel", "paperCount": 24, "citationCount": 512, "hIndex": 12}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "1833924", "name": "J. Zevin", "paperCount": 74, "citationCount": 2819, "hIndex": 26}, {"authorId": "1403609363", "name": "Mary Helen Immordino\u2010Yang", "paperCount": 73, "citationCount": 3060, "hIndex": 27}, {"authorId": "35168946", "name": "A. Gordon", "paperCount": 149, "citationCount": 2050, "hIndex": 25}, {"authorId": "2656777", "name": "A. Damasio", "paperCount": 371, "citationCount": 95460, "hIndex": 125}, {"authorId": "2253951", "name": "J. Kaplan", "paperCount": 74, "citationCount": 4047, "hIndex": 29}], "abstract": "Drawing from a common lexicon of semantic units, humans fashion narratives whose meaning transcends that of their individual utterances. However, while brain regions that represent lower\u2010level semantic units, such as words and sentences, have been identified, questions remain about the neural representation of narrative comprehension, which involves inferring cumulative meaning. To address these questions, we exposed English, Mandarin, and Farsi native speakers to native language translations of the same stories during fMRI scanning. Using a new technique in natural language processing, we calculated the distributed representations of these stories (capturing the meaning of the stories in high\u2010dimensional semantic space), and demonstrate that using these representations we can identify the specific story a participant was reading from the neural data. Notably, this was possible even when the distributed representations were calculated using stories in a different language than the participant was reading. Our results reveal that identification relied on a collection of brain regions most prominently located in the default mode network. These results demonstrate that neuro\u2010semantic encoding of narratives happens at levels higher than individual semantic units and that this encoding is systematic across both individuals and languages. Hum Brain Mapp 38:6096\u20136106, 2017. \u00a9 2017 Wiley Periodicals, Inc.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 74, "citation_count": 47, "influential_paper_citations": 2, "is_open_access": true, "citations": ["505d655df513d990ab63ad59d7166497e7a03037", "67ec09d6a7526c40583881a009f300e14cb6b49c", "8e557932685d74b99d485eb4e7caccac4dc1394a", "f93efbc718c850f3b1d95c287de6c1a51eb9c1a7", "a7aedd956e50cb9f4e8a98a97cd0f1c7ad475538", "e727dbabdc22729211870c3cf83461e3ce037aaa", "6dce96532da0131ce9c85a69adba666f2c0086fc", "a2a147ce79f471a45d4b0e891c0bbb33d83d2674", "de5a663ba6f27b3710d69d9bf0d08071dce131c8", "2b31936832bdadb4b751c5294f73888571c37db0"], "references": ["ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "caacccd18a45333cd475b08a57e70ebff809e0c8", "c87f3c8898788dd583c4d2a59a974a44b9e11c4c", "7cf2a692291ac4ff596c00846d06fd4cb5505a29", "dd37afb502cd1a7cac1364d2c378b8d0515a5ccc", "4995be055d63be04ed979d05cec97888b308e488", "66d0c1a5933caa3e5f1e21d7cb676ca2bfab32ab", "67bbff920b98e0edf357ba053856eb19c93bb213", "e7d2105857ca1a5ae2e87d9f2e30588dc4275671", "b3f0c6d948162a867501c2392dcd48160017153f", "0e5768381dcd523108ef892186e83e10a8b7d6d1", "22ae02d81c21cb90b0de071550cfb99e6a623e62", "04ebd82c48a580476fc5acad61b8ee036f92f1f5", "b45dc384c42b9dcde475b6c4e0a8f81a025f1d61", "57e562b46338f176e3b20c2dd0b66f17dfbef9e8", "1935827b73f2bcad1134fb9464d90a63e55ffdf7", "5de85fab4cc64a15bc9bc29b9f763b8e6860008c", "f9da44d5fd72878b9c37207324eba70ef310fde3", "78d11c85912e2860e490f2751401873425db273e", "5a56b5320644f58c4eca020b8429270ff595a4ee", "b099568f69e08bb020ba7760c68dd8b94f0a5ae5", "768bba6e8493ed40b100f4a1b380ab7b73abf09d", "7ba1e481ee084f12fc9b7f09e601bdc6c70f699d", "22cebf22de747bd68beb8820406c7c52979ee9ed", "55b848c6433e93025f4da0629f0bf3cac7555bf3", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "c83cb156e8812906bf79337032e076f734eb5d80", "a4cea1dd0ec85f7e2e97d5df20ac1e71c19c7ad1", "0afd6f7dd95b9ff4f9315e6bc42e020474ab2a85", "0e54d4927707da5041deca788c477673cbbd031a", "b7fe355bdec6d2fb281ea67491fa304a6b526c7d", "238c5eadaa866f73371fa3756ceb9ba29809bdae", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "dddd2c3d3c5c1385249fb5b9b49d914ced1fe50c", "f1e5952e2dd3562556693598e04a039712b747d0", "68e7d429fd3acb0a7e63ef41764c970f2c9d8bd3", "51a56e42b1357a6ce56ff888f4b5f57152bb3922", "b7de5050ec474022da256a3a46886e7f0900a5b0", "2e38a6740057fffcf9671c6e953d7983893fb546", "8596ca7e3f182dfac67cde3909530b7d0eba6283", "7ea7138e6cc2bb5d243e1deb1c08d09af7519f8a", "52c59c634e4800fb1ec64c141c51630737cea61b", "7503a862a170859927006ef358d46637378e10d1", "168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "83a6cacc126d85c45605797406262677c256a6af", "40f9af863dd53262ed3ad32f6eeb5bb6d15f47df", "f6865ed39f0fc55b43c5d6b93e8ca597a21a9ccb", "85c32ebdd6ebfb01960916196e89a6e897be0745", "b5ff5731b77346ce1c55e84deb04bcfc36aff178", "1f7bff9b2e65c3d618be721084963d26f1a161cf", "a2404efe53398d7900830f1c25bdecf42bf1002e", "f0ab48fa9e934e5c06c3faead27bc7dffeb7be0f", "c6e767e2b6976d682ab82e698654c49e18321562", "5e044a11d595268c2979f5b2afd89c4c62605e7c", "be4b47bcd9a5f14c136733843eb31c19d4ca2523", "18febd50be4e2a7ad81887a441340fcc3585cc27", "165fd770b8893f8511852d44f4d4ac7241eebeeb", "bb8477e072893dda0aac76e9f09c36dff58edf21", "347d8e55ef632c972f27aac096563d2774faa246", "28d1aa4ccd5ba29ca55e71ccebcb26ab908c382a", "8022c53a102d417f4fe2e7cd6cd330cca3e40352", "bfed5e67dc558c4d591714e0032f2dc10b1ac5ba", "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "291c9d66cf886ed27d89dc03d42a4612b66ab007", "b839f64b7ab0d473631adbf04e6ea2313e4f1741", "761a61b576b9e006859e043767abc0385fb29057", "f871a29ba78dc99358434f4bfab825e1f532380c", "70a74f9d606b658cb5bdbcbfc0d145b7407fbba2", "20a80a7356859daa4170fb4da6b87b84adbb547f", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "1e8817b3efd3f5ca8b16d0cc2930b4d0821a8c44", "b9877b5f4752c730a98a937eae76c3551ed1d248", "3d69509a402be3b2e809b385594b118eba695c92", "7c8aaf72d19e7b1447d1933f7a26fef36ceecc52"], "url": "https://www.semanticscholar.org/paper/490c5796f6994715e477b84332ff8dfe4bcbda42"}, "9ae0a24f0928cab1554a6ac880f6b350f85be698": {"id": "9ae0a24f0928cab1554a6ac880f6b350f85be698", "title": "One Model To Learn Them All", "authors": [{"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "19177000", "name": "Aidan N. Gomez", "paperCount": 32, "citationCount": 48744, "hIndex": 13}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "145024664", "name": "Llion Jones", "paperCount": 21, "citationCount": 50175, "hIndex": 15}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}], "abstract": "Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 33, "citation_count": 270, "influential_paper_citations": 12, "is_open_access": false, "citations": ["75acc731bdd2b626edc74672a30da3bc51010ae8", "2b0d7e51efd004fe3847f54863540c79312f3546", "9f1e9e56d80146766bc2316efbc54d8b770a23df", "b57e6468740d9320f3f14c6079168b8e21366416", "9784fbf77295860b2e412137b86356d70b25e3c0", "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "c17985a669522e7e85ae3d34754c7df49c7187d1", "891a1921de33cd402c7afd5fc76f01f968835c19", "3e95925d2bca43223453010ff8516a492287ce19", "d55d1d035e91220335edff0fe8f5d249d8c4a00b"], "references": ["07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "510e26733aaff585d65701b9f1be7ca9d5afc586", "a486e2839291111bb44fa1f07731ada123539f75", "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "b5c26ab8767d046cb6e32d959fdf726aee89bb62", "98445f4172659ec5e891e031d8202c102135c644", "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "df0402517a7338ae28bc54acaac400de6b456a46", "7f5fc84819c0cf94b771fe15141f65b123f7b8ec", "1af68821518f03568f913ab03fc02080247a27ff", "eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f500b1a7df00f67c417673e0538d86abb8a333fa", "0b544dfe355a5070b60986319a3f51fb45d1348e", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "528beaa995bf1bd1ae451b18218674af9ecd2b50", "72387d6050efa32a1fe7ba9b07f2f35af8d9d046", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "6658bbf68995731b2083195054ff45b4eca38b3a", "fdf533eeb1306ba418b09210387833bdf27bb756", "80e9e3fc3670482c1fee16b2542061b779f47c4f", "57458bc1cffe5caa45a885af986d70f723f406b4", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/9ae0a24f0928cab1554a6ac880f6b350f85be698"}, "010df54445ab5f47582eb668dc3488a3e46b55d3": {"id": "010df54445ab5f47582eb668dc3488a3e46b55d3", "title": "Unsupervised Neural Hidden Markov Models", "authors": [{"authorId": "2748455", "name": "Ke M. Tran", "paperCount": 18, "citationCount": 418, "hIndex": 9}, {"authorId": "3312309", "name": "Yonatan Bisk", "paperCount": 76, "citationCount": 4343, "hIndex": 26}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "1695463", "name": "D. Marcu", "paperCount": 170, "citationCount": 18674, "hIndex": 57}, {"authorId": "152971314", "name": "Kevin Knight", "paperCount": 258, "citationCount": 19644, "hIndex": 70}], "abstract": "In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 39, "citation_count": 54, "influential_paper_citations": 10, "is_open_access": true, "citations": ["7ea59779ffb392f099d5304680126b4299f43750", "51429728ca33b034270289345dc8343f5fc9a36b", "912a9c5a32d50fafd5ddb34b5616d22f87c7d637", "a03675379685d88c727bc985a323cc71d06f2514", "0ce366976258d221d28971ecf343c9d535537fc3", "7cc6795ce4f673f083cfbb7a09a8bea9a12cb623", "b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347", "b05386241ac7cce9abc154f580d0a6376a77ecc8", "1f103b3e71c52ec8ec3ac20f788219cebf9a20e0", "38a9371d05b07aa9c63993e80fd544caeca5f612"], "references": ["053b3605632c69d0c74c8a054840cfad89268ce3", "9ca3af4440eb4aa4fd0a65dfa559685b2c39cd42", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "889e57259a1d6017701fb2c2ceece82f9f4eff4c", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "93499a7c7f699b6630a86fad964536f9423bb6d0", "5b8364c21155d3d2cd38ea4c8b8580beba9a3250", "51d735419392dbe961c60bff7eee95388b8d6d3d", "0d449230ce76c11fb86cb6d1effb7e80703f44bb", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "f0ddb2bc6e5464d992ddbcdfdc7e894150fc81f2", "fb7c1a527d712938ae4c541d84c16162518847d7", "c0b624c46b51920dfec5aa02cc86323c0beb0df5", "34f25a8704614163c4095b3ee2fc969b60de4698", "71480da09af638260801af1db8eff6acb4e1122f", "02ef54074c7765825af96e94f8d41a21df7f7d35", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "5acaefc604fc0af0d77d5f01e32810b2da74416c", "e509e885ea98ad0762a78fad1369201ae240c301", "0f531f8db68e981bbf1f4e543d15b519ba3325b0", "ce72e9e72b8a2bc36cec0bb84424d2e57d5e12cc", "343733a063e491d234a36d3e1090a739318b3566", "48d704f56d074a72f23d4dea85c8202337d20a13", "92997b1b2096835c5aab4a2ef0d2585f939fd941", "9e022fa8effeaaff77d86be0a3d1bf50d899b5b8", "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025", "89622de8b2d3f066e740aa0e559be2edb259a9f7", "a6d815f8b3fd31fbf6db122a0e367d3e2a58ee9b", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "59ce9cdbde13affc05a6c1f48a51ee7b0fcb154b", "3de5d40b60742e3dfa86b19e7f660962298492af", "d36efb9ad91e00faa334b549ce989bfae7e2907a"], "url": "https://www.semanticscholar.org/paper/010df54445ab5f47582eb668dc3488a3e46b55d3"}, "0f5bb9ae0c060b349597c0b2582bf271a5a2156a": {"id": "0f5bb9ae0c060b349597c0b2582bf271a5a2156a", "title": "Supertagging With LSTMs", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "3312309", "name": "Yonatan Bisk", "paperCount": 76, "citationCount": 4343, "hIndex": 26}, {"authorId": "1757166", "name": "Kenji Sagae", "paperCount": 110, "citationCount": 3957, "hIndex": 33}, {"authorId": "37503904", "name": "Ryan Musa", "paperCount": 12, "citationCount": 210, "hIndex": 5}], "abstract": "In this paper we present new state-of-the-art performance on CCG supertagging and parsing. Our model outperforms existing approaches by an absolute gain of 1.5%. We analyze the performance of several neural models and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 22, "citation_count": 67, "influential_paper_citations": 19, "is_open_access": true, "citations": ["f5d6779073901989f14de984e2236891a801c2bd", "eec3a236ecd185712ce65fb336141f8656eea13d", "16c0ef924da1f6b510c9c783ac764156f5a3d631", "e75b3c12da067552fda910a5bbed8b4d0e82dbcb", "440dc6e5a06f35890d04b36768fe7b6e6d320684", "29ebd778dcb1447ba68200cfed8a98b895b2221c", "9e496d85869a9ab023d01a742f20bc32bcb446b6", "64026039ca47452d10ee70df751e51f8a666357a", "fc07649b873a37cb074591ebb9cb0bde9d82bd49", "fee62123e1d2ac56065675983475b079e1e9106f"], "references": ["440dc6e5a06f35890d04b36768fe7b6e6d320684", "35c1668dc64d24a28c6041978e5fcca754eb2f4b", "318fb72e53eb43667a78f32ec22f7e3135036e1b", "6dab1c6491929d396e9e5463bc2e87af88602aa2", "f5d93f0b6d77e2681072e0bd79735a3b0c457c1b", "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99", "7f16003c767e4502b8ec33713d7d3f35536c6324", "9336e77d12aa05019f3ef696b622f6bf6b61aef3", "865270ae385e53c825658098b0584c88ca68e73c", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "2a6626dadb6b624011c39270cc7678ac98e162a3", "a538b05ebb01a40323997629e171c91aa28b8e2f", "82332211c1d477990a9bab981ffa763ef7e21b6e", "2d45f21c9deb17987a6be71b3c9a2758791540a2", "4c4dcf6655204130f330002a9fb45c4fd436d5ea", "aef00c31da11a76e86eae825a6c83dc72d6e19ed", "23e95aa03b5ddf4380af437581372e4d8414c121", "0b44fcbeea9415d400c5f5789d6b892b6f98daff"], "url": "https://www.semanticscholar.org/paper/0f5bb9ae0c060b349597c0b2582bf271a5a2156a"}, "2eeb74085c00449ea08fc0f68fa151d181e893e0": {"id": "2eeb74085c00449ea08fc0f68fa151d181e893e0", "title": "Efficient Structured Inference for Transition-Based Parsing with Neural Networks and Error States", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "1757166", "name": "Kenji Sagae", "paperCount": 110, "citationCount": 3957, "hIndex": 33}], "abstract": "Transition-based approaches based on local classification are attractive for dependency parsing due to their simplicity and speed, despite producing results slightly below the state-of-the-art. In this paper, we propose a new approach for approximate structured inference for transition-based parsing that produces scores suitable for global scoring using local models. This is accomplished with the introduction of error states in local training, which add information about incorrect derivation paths typically left out completely in locally-trained models. Using neural networks for our local classifiers, our approach achieves 93.61% accuracy for transition-based dependency parsing in English.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 38, "citation_count": 15, "influential_paper_citations": 1, "is_open_access": true, "citations": ["4be0dd53aa1c751219fa6f19fed8a6324f6d2766", "eb63a32fa4b7759ae6bcc547d5f0d3f5d2bbec1b", "6789e0dbd294cccb3b7dd4e001c9e8ba4813f334", "0e08a9fa9d85646f52665be81e82b6755fba9c77", "8602398403281dae0694b4e0488eb501d6db49ef", "df80d1d53e8c8e17eb65174addeec668b8a59b71", "af7a5099588b009b0955e15c79113025224f6078", "9424a154e1bab2f809b95c448aa6c7a2b3f53ffb", "323be28da915fef62ef24c6efb891d5103504bad", "3884ca0b719e00aafe2887f5325af0bc54c43069"], "references": ["5b0c87c3368f270137f5eec7fe1a52c6d98123ef", "0fac3a87cf6f214a10f4e39d73e119ea81b52e46", "d7d85acbc469dcb0997607475b3c547ee2e238cf", "a14045a751f5d8ed387c8630a86a3a2861b90643", "f90fc459c305f13b3fab0abf42b8dd1801c59511", "245e077b014e00ca6601f54a9e5856dd12619fab", "71480da09af638260801af1db8eff6acb4e1122f", "d32c62d9709a10fc75096246971edc36a5855b53", "2466d675f9a8b07cc8295bf075a4bab7e8b7c5b2", "d4bd0035fe14832626279e6c3c72b73c21c7f5d8", "69531a4477351f2a6b8286d27f1ee4c340c6cb63", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "91c3130e89d2fd37caface9047389c71de14e2a1", "c1d96c0f421f8b519cafdbb4e499faa1c797ed9b", "11aa6801c417dd97552737c587fd8d7f480d10ab", "bc1022b031dc6c7019696492e8116598097a8c12", "413c1142de9d91804d6d11c67ff3fed59c9fc279", "79ab3c49903ec8cb339437ccf5cf998607fc313e", "311a30c808ee16e13e8d1a9ba5c12213de5ddf60", "a538b05ebb01a40323997629e171c91aa28b8e2f", "3c9d9f3c6f7508f4e29730924529dc993c27cddc", "053f1cf10ced2321c1853f307075f0a6a83b6840", "ad1181d188f730b7917a95ae452efb48f830c90a", "f66821598f4db7a6a2f54a6a4ae43e391649f4c1", "790ecefeaf2b471b439743a772ccce026131bef5", "10a9abb4c78f0be5cc85847f248d3e8277b3c810", "798957b4bbe99fcf9283027d30e19eb03ce6b4d5", "30cfbd61be61124a8bfb1d4f6b43a5fcb03e4319", "8e19e281dff468d586d02f0e7ac80e4cfd87aab9", "41828fc3dab24784f95e6976e8aaa73f68e1840e", "f0e1883cf9d1b3c911125f46359f908557fc5827", "5a7958b418bceb48a315384568091ab1898b1640", "f4ba954b0412773d047dc41231c733de0c1f4926", "566eb7be43b8a2b2daff82b03711098a84859b2a", "bece46ed303f8eaef2affae2cba4e0aef51fe636", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "3de5d40b60742e3dfa86b19e7f660962298492af"], "url": "https://www.semanticscholar.org/paper/2eeb74085c00449ea08fc0f68fa151d181e893e0"}, "6afb411e0944740801f95f101a90a4e4532921dc": {"id": "6afb411e0944740801f95f101a90a4e4532921dc", "title": "Name Tagging for Low-resource Incident Languages based on Expectation-driven Learning", "authors": [{"authorId": "38629264", "name": "Boliang Zhang", "paperCount": 54, "citationCount": 1580, "hIndex": 19}, {"authorId": "34741133", "name": "Xiaoman Pan", "paperCount": 42, "citationCount": 926, "hIndex": 15}, {"authorId": "1785372925", "name": "Tianlu Wang", "paperCount": 26, "citationCount": 1984, "hIndex": 12}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "144016781", "name": "Heng Ji", "paperCount": 290, "citationCount": 9850, "hIndex": 50}, {"authorId": "152971314", "name": "Kevin Knight", "paperCount": 258, "citationCount": 19644, "hIndex": 70}, {"authorId": "1695463", "name": "D. Marcu", "paperCount": 170, "citationCount": 18674, "hIndex": 57}], "abstract": "In this paper we tackle a challenging name tagging problem in an emergent setting the tagger needs to be complete within a few hours for a new incident language (IL) using very few resources. Inspired by observing how human annotators attack this challenge, we propose a new expectation-driven learning framework. In this framework we rapidly acquire, categorize, structure and zoom in on ILspecific expectations (rules, features, patterns, gazetteers, etc.) from various non-traditional sources: consulting and encoding linguistic knowledge from native speakers, mining and projecting patterns from both mono-lingual and cross-lingual corpora, and typing based on cross-lingual entity linking. We also propose a cost-aware combination approach to compose expectations. Experiments on seven low-resource languages demonstrate the effectiveness and generality of this framework: we are able to setup a name tagger for a new IL within two hours, and achieve 33.8%-65.1% F-score 1.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 57, "citation_count": 26, "influential_paper_citations": 1, "is_open_access": true, "citations": ["616253f6b1e83ede361457de2f51b0bf70555b13", "d0edf834c1d86aac1835e23c11b23b8723b65136", "c8a3a1391845667702d4d5df10be0c5761202cbe", "0b7a189d937c546ed67b729b41c2b74c0587db62", "bfd384dfb550fc5d9b4ce506c28c8626cbcebc76", "977406e0582a33a645e9e6f28af106cd1d7713cc", "2593f607a156e811ee7237fd52febbb78595ff6f", "e3ca15ccbec5ee2309031d8d90cec4d87ae11327", "5816cb81f615042db1da3ab5924baf7e022f8b29", "3f79e7b2859603031ac3ef88acbaf8b332a28890"], "references": ["46966feec5fb7eed499b1cd6a31b95ff04d93113", "ea2cf981682def8d3ab65a61f648da035093ce82", "e72e5ee5de14fd463ab58ce830474157258e3578", "b79b12fb29ce3f47f9544551aa4cd1f49776ef03", "3dd63c9b5ea5df6ec020a34bf8ea7c1bf15997c7", "696b505083d34c6f995aef88d0352d70d7f7e8c8", "37dc166f8162bc2515026ebaeda522f7e0d7ad42", "d5fd570cd2511c53b07efb478d4fa206ec1376c6", "2f23ca5f8e86007810140595c647f5c10e3afe19", "1cf53c62906a48b9d607f94468bf777855199a28", "1d890fd611e8683a47f5d98d9bf16f15048ea219", "a5dd230970c90f311f10b9c59283eb4131f4fc79", "818826f356444f3daa3447755bf63f171f39ec47", "487ed99e00bf6803a53a6059ceccd1510a63e72d", "4a554da55fd9ff76c99e25d2ce937b225dc1100c", "d6eb31985086ab66bd91b2997f28d2e95f1b77cd", "467ba4a4d08f353dac593d7d8a238bd9e16766c7", "3cdf3459d3f4fa134ff2924fee9570674bf45bdd", "4f410ab5c8b12b34b38421241366ee456bbebab9", "421151fa75e40dd86414215abf29d9f2c052a2e1", "747b07e626b02f07c76ef92fe3ca8e5028daa23a", "caa9f04521b556f2987f2276231136982483755a", "646dcd2e6d903b7ebc24f631cf36c7b9e62352f2", "7f7476da21cea86bc5714df4660f78dfe3f8edc1", "28c80bce6fe1b21dc52ad165c3a329eeddab8c3d", "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "b078a4454d2fef90e76248117d6085bd9cae882f", "a04b0d9d310f24dea83dba7cfb45fa9997810449", "d74bb665ecbee0218b9a969db17b4c9a40152aa0", "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "96752dc3a8c3edff8ef69b9c0fdeff9bde2b48f1", "0089e4ef21552c97f0331f64677ffb63a18efb63", "51bdd0b839fe8ab1daafa05d1ed03edff601f4c7", "9bf3920ae3153a8c49a7db7c94246707ee08a30e", "471b0adb7995a8be34ba6ac5374a75e6e19bf2a9", "f4ba954b0412773d047dc41231c733de0c1f4926", "2ace5141ef4de095049952c2f7c78bb386d432bb", "6b1499fe8f469571dc5a84741a071dd0c2770378", "1235d0014dcf09877a7d90508579abff9e552227", "b0cda5ef90bbbb03ba418830823759025e438c7a", "cee045e890270abae65455667b292db355d53728", "779126e98eacf61314abf08b2c7dad997a79cfa5", "465cadc35e5695e131b25d7cd925a7ee7c533b35", "aa8c4d7b07e1b03dfe214b247e62c27de33b63b6", "1c0ece611643cfb8f3a23e4802c754ea583ebe37", "23cd1072618ab651e77114fe55642e46d93824d5", "a3836dd4cac89aa94ee2af6d52bf9bcccc7d9f77", "d3a4d07522560b8390c83c6f611d492950750c91", "a935c0d674457aad94210f9bdeeb136c5ea54ecd", "591a11539b1eb7714563969f837d30c454afd148", "bfa035c0e723f8f540500db038ca6e26d599029d"], "url": "https://www.semanticscholar.org/paper/6afb411e0944740801f95f101a90a4e4532921dc"}, "8d6a67a5f4192280a35ccaaf4e660c1772c54a63": {"id": "8d6a67a5f4192280a35ccaaf4e660c1772c54a63", "title": "Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies", "authors": [{"authorId": "2368067", "name": "Barret Zoph", "paperCount": 46, "citationCount": 25445, "hIndex": 31}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "143823227", "name": "Jonathan May", "paperCount": 120, "citationCount": 3348, "hIndex": 26}, {"authorId": "152971314", "name": "Kevin Knight", "paperCount": 258, "citationCount": 19644, "hIndex": 70}], "abstract": "We present a simple algorithm to efficiently train language models with noise-contrastive estimation (NCE) on graphics processing units (GPUs). Our NCE-trained language models achieve significantly lower perplexity on the One Billion Word Benchmark language modeling challenge, and contain one sixth of the parameters in the best single model in Chelba et al. (2013). When incorporated into a strong Arabic-English machine translation system they give a strong boost in translation quality. We release a toolkit so that others may also train large-scale, large vocabulary LSTM language models with NCE, parallelizing computation across multiple GPUs.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 19, "citation_count": 41, "influential_paper_citations": 5, "is_open_access": true, "citations": ["5f2cbe15668ed1365d3b3f34b13691673a75bfea", "58001259d2f6442b07cc0d716ff99899abbb2bc7", "7ae976870e17ed36dc41bfd7fcc13e6860573340", "eec15bc21ddc668e75115972e6716dff0f1944df", "bd3f21856f64dbfeb1be4723d75122e534f8b5e8", "f0085dc1fe376ef240f233002b8ce57c2cfe0106", "d40691bb0b5675748b67a69db6a71fd3ab290eb4", "bdf5e611c842ca6cd93bb342afb82bed85fb8aa6", "63d9c3a2abebc39e73b8935e5d63f80f432ff7df", "9557aa7ce18555d21f238a90057c9295a5b796f6"], "references": ["2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "93499a7c7f699b6630a86fad964536f9423bb6d0", "ac973bbfd62a902d073a85ca621fd297e8660a82", "dcfade77ecd26c1b21c68021ff482dba6fef8063", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "0894b06cff1cd0903574acaa7fcf071b144ae775", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "71480da09af638260801af1db8eff6acb4e1122f", "53ac234b5c0950bc5240a48fa7077e9f728f9547", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025", "57b3f8df7020b67df71a96974adef8d5282ed396", "d01737b617acc555153f4660417908bf3971b1a5", "a7e925a65860e90b2b4eb427a8bc497f76b2fe6e", "1f12451245667a85d0ee225a80880fc93c71cc8b"], "url": "https://www.semanticscholar.org/paper/8d6a67a5f4192280a35ccaaf4e660c1772c54a63"}, "00d9f17b6a9a24af68c04e87fb62e4d1f1e3ecce": {"id": "00d9f17b6a9a24af68c04e87fb62e4d1f1e3ecce", "title": "Model Invertibility Regularization: Sequence Alignment With or Without Parallel Data", "authors": [{"authorId": "2900341", "name": "Tomer Levinboim", "paperCount": 21, "citationCount": 173, "hIndex": 8}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}], "abstract": "We present Model Invertibility Regularization (MIR), a method that jointly trains two directional sequence alignment models, one in each direction, and takes into account the invertibility of the alignment task. By coupling the two models through their parameters (as opposed to through their inferences, as in Liang et al.\u2019s Alignment by Agreement (ABA), and Ganchev et al.\u2019s Posterior Regularization (PostCAT)), our method seamlessly extends to all IBMstyle word alignment models as well as to alignment without parallel data. Our proposed algorithm is mathematically sound and inherits convergence guarantees from EM. We evaluate MIR on two tasks: (1) On word alignment, applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT. (2) On Japanese-to-English backtransliteration without parallel data, applied to the decipherment model of Ravi and Knight, MIR learns sparser models that close the gap in whole-name error rate by 33% relative to a model trained on parallel data, and further, beats a previous approach by Mylonakis et al.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 19, "citation_count": 10, "influential_paper_citations": 1, "is_open_access": true, "citations": ["ada937c9f51316c6ac87f9d1d4509383d23e0c21", "c27d7e93a41001184a44cd63c132c93dc2c50820", "fdbdd4e0461d23905104460a02a176907d945f44", "de45a6f4473a321d3dd70b0f5e327a0783b57326", "4f71fd26be06f479099b17af95d408c7ab272830", "6773573fe5e28b6d439c1cdf88d33166c00dc702", "c4eba54af99405d0b8f20faaba3fcc3c0efb9ec5", "4ca5e052c4d74c5a12d2ff95cc4620d994869743", "9972b44d53d289641e4e47dfa9fa8a3a20f064f4", "57e32d2bec0cc2e775a28bf8737287d841878e9d"], "references": ["9d64d96cf633c3c6fbc86c9c5d74349799e305e3", "aa1b28f1010d4b563e2abf0c3ee1d2d888c941b9", "537018f8ee3502faea7fcd00f511f6cacf89ea68", "e4f5c9d0ab8ea3a91b0f9ffa698fa79c43463115", "3dc2bab8e75d866db77be7725272f1f803565747", "7fdbb9f2a0caaa0813d26756a2d071959b3dd5a5", "60cce28d1f56786930e86e5798d55e4a7948b0da", "aad4f99b06c25a9911820f5430b57b68d145a278", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "f4f6bfacb4cd508df62540f5aa9ba30cd83dd127", "cb826a3899752b796f14df1c50378c64954a6b0a", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "683305450fcb46f6832108308fc436df1b9eb80e", "ab7b5917515c460b90451e67852171a531671ab8", "d36efb9ad91e00faa334b549ce989bfae7e2907a", "d4143c46910f249bedbdc37caf88e4c292124c08"], "url": "https://www.semanticscholar.org/paper/00d9f17b6a9a24af68c04e87fb62e4d1f1e3ecce"}, "1ddb0c06d6497698f2189517e6a97d8a5e3fa1ac": {"id": "1ddb0c06d6497698f2189517e6a97d8a5e3fa1ac", "title": "Unifying Bayesian Inference and Vector Space Models for Improved Decipherment", "authors": [{"authorId": "143705905", "name": "Qing Dou", "paperCount": 15, "citationCount": 263, "hIndex": 7}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "152971314", "name": "Kevin Knight", "paperCount": 258, "citationCount": 19644, "hIndex": 70}, {"authorId": "1745899", "name": "Chris Dyer", "paperCount": 253, "citationCount": 33766, "hIndex": 74}], "abstract": "We introduce into Bayesian decipherment a base distribution derived from similarities of word embeddings. We use Dirichlet multinomial regression (Mimno and McCallum, 2012) to learn a mapping between ciphertext and plaintext word embeddings from non-parallel data. Experimental results show that the base distribution is highly beneficial to decipherment, improving state-of-the-art decipherment accuracy from 45.8% to 67.4% for Spanish/English, and from 5.1% to 11.2% for Malagasy/English.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 25, "citation_count": 17, "influential_paper_citations": 2, "is_open_access": false, "citations": ["562c09c112df56c5696c010d90a815d6018a86c8", "c2a7afbb5609a723f8eea91bfde4b02579b048d6", "11fdffe08eb1b08bfa8e3fdabe1accfd0e336be3", "776b4f1d6ed07f1623831eae2849562cf4381394", "92c22d3743bcc080783b3deb1ed4889f967df286", "8d3feb186556ead77590979b42c42374549a1166", "aecddd82840323e5bd43f9c73a32fed88ee93c8c", "4c365cf1f3227dba366704ef2cb0e16df4a34579", "5cb452c06449c0ffb639862ea1ded5de046317db", "153b858e58cdab15dc62a6426a1bc7aa391bea9b"], "references": ["2a396ba78f6d743e7382e9966a5d8a925b3150d1", "55c5c21f715def30a04229def84e5f6271059063", "9d64d96cf633c3c6fbc86c9c5d74349799e305e3", "0157dcd6122c20b5afc359a799b2043453471f7f", "0f16ab376632ee83f2a3af21e96ebb925a8ac8b8", "d3975b7f0ff53b983224b7f1480580ca8ef22ba0", "2466d675f9a8b07cc8295bf075a4bab7e8b7c5b2", "0f68ea4c0958539fa2b8badf61b8bc1d0e7d7085", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "d1f37d9cab68eb8cda669cc949394732f33264b4", "9334e1139b4058b8d9a830df72fa1677f7cb9dee", "0140748c277f5599b11b80f77d1d721da92a1f41", "aed835ee229be00d3e1326c3488a8ec7cfcdeeb0", "537018f8ee3502faea7fcd00f511f6cacf89ea68", "f0400d682db84f11d184e0b1bda6d24fa46fcc0f", "51cfde9bbc25a4cff1d5666815674c83886d933e", "dade7863c1f2a69bf15496438ecd379963b4b543", "697743250a6ab36b35262f22fb231bc0d0636006", "3709b6cb2ed14c04b60e38d5f75e89c41317e93d", "af053bbb2a9ed53d73f5ab22a804720b0887c927", "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "399da68d3b97218b6c80262df7963baa89dcc71b", "6818f2b2fefee4ff9bc21ba67cb36e69a8a71b55", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab"], "url": "https://www.semanticscholar.org/paper/1ddb0c06d6497698f2189517e6a97d8a5e3fa1ac"}, "62ce2e3b75a6842a439218d56b7dd91d2fb7f959": {"id": "62ce2e3b75a6842a439218d56b7dd91d2fb7f959", "title": "Documentary Linguistics and Computational Linguistics: A response to Brooks", "authors": [{"authorId": "21308992", "name": "Steven Bird", "paperCount": 152, "citationCount": 12420, "hIndex": 39}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}, {"authorId": "69533036", "name": "F. Frowein", "paperCount": 7, "citationCount": 16, "hIndex": 2}, {"authorId": "1991080", "name": "Florian R. Hanke", "paperCount": 7, "citationCount": 104, "hIndex": 4}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": "In mid-2012 we organized a two-week workshop in Papua New Guinea (PNG) to provide training in basic techniques and technologies for language documentation, and to gain understanding of how these technologies might be improved in the future. It was a diverse program, combining the expertise of scholars from ten institutions. It was also a diverse audience, including academics, teachers, students, archivists, translators, pastors, and farmers from across the country. Approximately twenty local languages were represented. The central idea of Brooks\u2019 assessment of the workshop is that its computational goal was incompatible with its documentary goal. However, we would say that there was a single goal, namely, to document languages of PNG. We would particularly guard against the possible misperception that data is collected from PNG languages to fuel machine translation in general. While that would admittedly be interesting, machine translation, in this context, is not an end in itself but a means to an end, which is documentation. Much of Brooks\u2019 commentary addresses our reliance on textual sources. We agree with his reasons, and most were already raised in our article. We had planned to include spoken language recordings among the workshop activities, using 34 voice recorders donated by Olympus, but the recorders turned out to be tied up in student projects. This was one of several logistical challenges of organizing a workshop in PNG, challenges which made the execution of the workshop turn out differently from its conception. As always, there are things we would do differently the second time, including a stronger emphasis on oral language recording. In fact, it was for this purpose that the Aikuma mobile phone app was developed (Hanke & Bird 2013, Bird et al 2014a, b). Nevertheless, texts are a form of", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 6, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["ecfd6eafd5cd657c844b500037992887176c278f", "e94d7ede450e0d86c759449c472e672242a1c023", "15515ebac84d924e2940db01b5ec940716477e7f", "3b5c3ee1233eb67f8f252fffe87bbba3603b3d4e", "114a67d5e1ddc048c5ca80d709995ebd35ca2548", "6e198ce93046c12ca000fc76c405b2242dca1fd0"], "url": "https://www.semanticscholar.org/paper/62ce2e3b75a6842a439218d56b7dd91d2fb7f959"}, "901da402912b47406e99fc4bdfc9b48122343c73": {"id": "901da402912b47406e99fc4bdfc9b48122343c73", "title": "Sequence Alignment With or Without Parallel Data", "authors": [{"authorId": "2900341", "name": "Tomer Levinboim", "paperCount": 21, "citationCount": 173, "hIndex": 8}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}], "abstract": "We present Model Invertibility Regularization (MIR), a method that jointly trains two directional sequence alignment models, one in each direction, and takes into account the invertibility of the alignment task. By coupling the two models through their parameters (as opposed to through their inferences, as in Liang et al.\u2019s Alignment by Agreement (ABA), and Ganchev et al.\u2019s Posterior Regularization (PostCAT)), our method seamlessly extends to all IBMstyle word alignment models as well as to alignment without parallel data. Our proposed algorithm is mathematically sound and inherits convergence guarantees from EM. We evaluate MIR on two tasks: (1) On word alignment, applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT. (2) On Japanese-to-English backtransliteration without parallel data, applied to the decipherment model of Ravi and Knight, MIR learns sparser models that close the gap in whole-name error rate by 33% relative to a model trained on parallel data, and further, beats a previous approach by Mylonakis et al.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 11, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["9d64d96cf633c3c6fbc86c9c5d74349799e305e3", "537018f8ee3502faea7fcd00f511f6cacf89ea68", "e4f5c9d0ab8ea3a91b0f9ffa698fa79c43463115", "3dc2bab8e75d866db77be7725272f1f803565747", "7fdbb9f2a0caaa0813d26756a2d071959b3dd5a5", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "f4f6bfacb4cd508df62540f5aa9ba30cd83dd127", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "683305450fcb46f6832108308fc436df1b9eb80e", "ab7b5917515c460b90451e67852171a531671ab8", "d36efb9ad91e00faa334b549ce989bfae7e2907a"], "url": "https://www.semanticscholar.org/paper/901da402912b47406e99fc4bdfc9b48122343c73"}, "2a396ba78f6d743e7382e9966a5d8a925b3150d1": {"id": "2a396ba78f6d743e7382e9966a5d8a925b3150d1", "title": "Beyond Parallel Data: Joint Word Alignment and Decipherment Improves Machine Translation", "authors": [{"authorId": "143705905", "name": "Qing Dou", "paperCount": 15, "citationCount": 263, "hIndex": 7}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "152971314", "name": "Kevin Knight", "paperCount": 258, "citationCount": 19644, "hIndex": 70}], "abstract": "Inspired by previous work, where decipherment is used to improve machine translation, we propose a new idea to combine word alignment and decipherment into a single learning process. We use EM to estimate the model parameters, not only to maximize the probability of parallel corpus, but also the monolingual corpus. We apply our approach to improve Malagasy-English machine translation, where only a small amount of parallel data is available. In our experiments, we observe gains of 0.9 to 2.1 Bleu over a strong baseline.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 39, "citation_count": 20, "influential_paper_citations": 1, "is_open_access": true, "citations": ["a5c3fa3aecf8c34db9167a131221628cbcd1b239", "593338fbbb79065891beaf985a95860bd39f52bf", "1ddb0c06d6497698f2189517e6a97d8a5e3fa1ac", "ae0cee081d474fcc095ca150504f6e7dec7561c3", "31aee42153784fb7b6cba0210f46ce96d854ec98", "93a1f125b6574b76224edf3ad83beefa3f98fac1", "f28f21116c5302da5288e012edf5d5b6cd6c08f8", "dc90cdf9ce12dae869148a73847d209f577ce7dc", "89a548ff05bf456f1495c0f9caee24bf9ad6b385", "8dc3e88ed6bfcae912146db37cfecf0e1fe82fc6"], "references": ["9d64d96cf633c3c6fbc86c9c5d74349799e305e3", "55c5c21f715def30a04229def84e5f6271059063", "690b6a1e710e9b3569d536f428b2d0532d9bea08", "2466d675f9a8b07cc8295bf075a4bab7e8b7c5b2", "a2d87894b31c8621ec3c3d72328bdfb142ea4b5a", "0f16ab376632ee83f2a3af21e96ebb925a8ac8b8", "d3975b7f0ff53b983224b7f1480580ca8ef22ba0", "c0ac73f8cb1630cddc9c8f953ed4d30a6cb6a5b4", "0f68ea4c0958539fa2b8badf61b8bc1d0e7d7085", "9334e1139b4058b8d9a830df72fa1677f7cb9dee", "0140748c277f5599b11b80f77d1d721da92a1f41", "c3ed5bbc1fbcce6b95c596db367668d4f7ff2714", "8eb2ba39451789f4e0d9e8e5ab97202f5a7b2c80", "db4f1718cb56fea2ae99c7df01237c198697bfa5", "aed835ee229be00d3e1326c3488a8ec7cfcdeeb0", "537018f8ee3502faea7fcd00f511f6cacf89ea68", "f0400d682db84f11d184e0b1bda6d24fa46fcc0f", "c70550f81e3d582da97f82777ac502cf4652d6e1", "dade7863c1f2a69bf15496438ecd379963b4b543", "83e3bba16f45e2a792e6cc69514529c9c08fe0e7", "3594af2ebf510609651bf282dfea65c8e837b1a7", "3709b6cb2ed14c04b60e38d5f75e89c41317e93d", "dc5b083275ee111dc5e276bd5a9178de8c781c3e", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "af053bbb2a9ed53d73f5ab22a804720b0887c927", "6addb75d8aa2910706aecdfe4b91586efefbf1e1", "1f12451245667a85d0ee225a80880fc93c71cc8b", "120ac09b2734ba9d785f6f3def85fe1936aa4322", "327c88dd06722a967be9c6b1176fbd79554967e7", "168268c0142667335fe4ab40ef6673641c9e1c25", "dd7edbb79b02e333145f645f897e55b9d387c29c", "59c442932e9fcfcac6df5566c2bcd1ec331548c9", "6818f2b2fefee4ff9bc21ba67cb36e69a8a71b55", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "ab7b5917515c460b90451e67852171a531671ab8", "a1f9fcf2ccc313a5018e536e76e75d1f7992937b", "459b30a9a960080f3b313e41886b1aa0e51e882c", "d36efb9ad91e00faa334b549ce989bfae7e2907a"], "url": "https://www.semanticscholar.org/paper/2a396ba78f6d743e7382e9966a5d8a925b3150d1"}, "939071cff1ec0fa75a9a31a3a5c6c3c27d892e21": {"id": "939071cff1ec0fa75a9a31a3a5c6c3c27d892e21", "title": "Aligning context-based statistical models of language with brain activity during reading", "authors": [{"authorId": "2001748", "name": "Leila Wehbe", "paperCount": 49, "citationCount": 843, "hIndex": 12}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "152971314", "name": "Kevin Knight", "paperCount": 258, "citationCount": 19644, "hIndex": 70}, {"authorId": "40975594", "name": "Tom Michael Mitchell", "paperCount": 349, "citationCount": 33200, "hIndex": 79}], "abstract": "Many statistical models for natural language processing exist, including context-based neural networks that (1) model the previously seen context as a latent feature vector, (2) integrate successive words into the context using some learned representation (embedding), and (3) compute output probabilities for incoming words given the context. On the other hand, brain imaging studies have suggested that during reading, the brain (a) continuously builds a context from the successive words and every time it encounters a word it (b) fetches its properties from memory and (c) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is. This hints to a parallelism between the neural networks and the brain in modeling context (1 and a), representing the incoming words (2 and b) and integrating it (3 and c). We explore this parallelism to better understand the brain processes and the neural networks representations. We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography (MEG) when subjects read a story. For that purpose we apply the neural network to the same text the subjects are reading, and explore the ability of these three vector representations to predict the observed word-by-word brain activity. Our novel results show that: before a new word i is read, brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context. Secondly, the neural network embedding of word i can predict the MEG activity when word i is presented to the subject, revealing that it is correlated with the brain\u2019s own representation of word i. Moreover, we obtain that the activity is predicted in different regions of the brain with varying delay. The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions. Finally, we show that the output probability computed by the neural networks agrees with the brain\u2019s own assessment of the probability of word i, as it can be used to predict the brain activity after the word i\u2019s properties have been fetched from memory and the brain is in the process of integrating it into the context.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 21, "citation_count": 85, "influential_paper_citations": 12, "is_open_access": true, "citations": ["644a33399711b31f8a5a1b464f6ffd7c2264fedc", "e3567830f32444917af2d06c213435b7f1a92cd2", "8981a71bb7dc6d0e9a25e24fc742e9cd5a511135", "048a4bad7fbd0cc5068422d0300a83e8f03b078d", "6add8d6f1ba7e78924dad8ba61df9808ed4d9ca6", "ed8f2927523a4892058808c2fa5fe8daba266b10", "5c71e858e28e08c397b3da23e16ca0181df9402d", "95ae26c5f196a1e08f3bf01245beaa1b0ba2b8ad", "2fcee98165b4e3e059cda62b05e5a56b843a4b9f", "944ace602889575c85b2d7a4cf9fb30c5de53768"], "references": ["22cebf22de747bd68beb8820406c7c52979ee9ed", "1460f028f74ca61e5c9dc50b6338e07bcfd8db3d", "b67832cfddf1d599979723ff1232e9d37afde792", "71480da09af638260801af1db8eff6acb4e1122f", "1f9c33dec705caed741491726caab57d379459c4", "b7de5050ec474022da256a3a46886e7f0900a5b0", "2415cf4542cca1030a84131aabe7177efa2ef329", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "86d62362d50fd3d26f0c049fc72d4cf40bd218b6", "a538b05ebb01a40323997629e171c91aa28b8e2f", "57835864620ace72b03d7d184e14d100f9fe077c", "70d0e14b7c56dd2b447643c84496c7aab3a47589", "44ad920f1d046adb042ea6a33d4b216f2d76ba81", "c407ecf09027afbd98a1ce335b48f81cca523edb", "985cceaca8606443f9129616a26bbbbf952f2d7f", "64c5eeda863afe8c784f363cb9ea7be9a9e4a7c2", "3506c78fe50be53c0afc6fe929117e82b932fcad", "51080fd6f50f1d2a009397fa3b9d966da01d4c6b", "156e7730b8ba8a08ec97eb6c2eaaf2124ed0ce6e", "8f3941c68d5f886340dc961a6ffce1894d8aa6ca", "28a215515d10b0ccadef846c61894e2e8a4ff65a"], "url": "https://www.semanticscholar.org/paper/939071cff1ec0fa75a9a31a3a5c6c3c27d892e21"}, "b45717480ff87740bb727782ea34e61f79875ebd": {"id": "b45717480ff87740bb727782ea34e61f79875ebd", "title": "Smaller, Faster And Accurate Models For Statistical Machine Translation", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": null, "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 0, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": [], "url": "https://www.semanticscholar.org/paper/b45717480ff87740bb727782ea34e61f79875ebd"}, "624a5c97be5d3ec63d48c34db25726008e5d92a4": {"id": "624a5c97be5d3ec63d48c34db25726008e5d92a4", "title": "Learning Whom to Trust with MACE", "authors": [{"authorId": "2022288", "name": "Dirk Hovy", "paperCount": 122, "citationCount": 4293, "hIndex": 35}, {"authorId": "1400419309", "name": "Taylor Berg-Kirkpatrick", "paperCount": 95, "citationCount": 4043, "hIndex": 29}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "144547315", "name": "E. Hovy", "paperCount": 647, "citationCount": 40036, "hIndex": 92}], "abstract": "Non-expert annotation services like Amazon\u2019s Mechanical Turk (AMT) are cheap and fast ways to evaluate systems and provide categorical annotations for training data. Unfortunately, some annotators choose bad labels in order to maximize their pay. Manual identification is tedious, so we experiment with an item-response model. It learns in an unsupervised fashion to a) identify which annotators are trustworthy and b) predict the correct underlying labels. We match performance of more complex state-of-the-art systems and perform well even under adversarial conditions. We show considerable improvements over standard baselines, both for predicted label accuracy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download 1 .", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2013, "reference_count": 27, "citation_count": 266, "influential_paper_citations": 63, "is_open_access": false, "citations": ["0d441ab58a1027cb64084ad065cfea5e15b8e74c", "470c7c26f16bc52ea68a159c84f07b78d58fd02a", "ee5fff85d3ec62698eddba162f054b7e73670b2a", "ece56ab633f11d1592a3d4f9386412d3f48fcf95", "3ce9c44c6505e133bb5656cad566d326e43a76ff", "a1e91322798c7ba97ab115c58817adb06005c0c1", "599c95e1069b8184e37d233cfb4bd0b4e0f77daf", "25ae911c13da7ef9def56ee30170920ebd48a668", "8c5ced1d6677eecf173ba895da11d736eb3c7a90", "5cc7d945b90e9fdb0388c5e49e95ff464a1affe5"], "references": ["91ecef06c5338ba321428a953a038f7a163e56de", "cb3796eee6752fc0824cfcb633f4bb03e8161554", "c5f4e80a36ee108a45b2e3b88318ea21d4889d3c", "3e49987a3f40b22f075fd0bb8d4427a4c8cd1169", "c2e250b4b49a9aa04b68dfd40dc69b022b1f8b3d", "da6918ed87095d1313bd20606a934f899d4084b0", "35afa910207445caa3a509a23d41117e7595f262", "e7cf8403c7cde43b0b67c40fb171fd5e6cdeaf4b", "4eeccdc5d43b31e1e2b1f5b835ad2453445f8b2b", "cce9bd61d78ecd3cebf430151d09614a1392b99f", "6112a178ef8005119d59e7cc7cc73bbe7c852ec4", "6953420c593842697dd09bc2cf7ffbbaf67a6e8e", "87e205145e4bf080d26bba90f2add571fb749128", "0165568bcc1a819c18564567f2ec15d859be2519", "d513d8b6470c7cbbeca8563505de8711325a3179", "087be7b3d737ee817b64246e7c9a4fb6cd57dc24", "c6e8d5e9ca4e58c413857b7fbd3a11054f2262cb", "89622de8b2d3f066e740aa0e559be2edb259a9f7", "dbafcda95fa3e7e9b5627472d6d4fb46454828ce", "33d65cb3e2e852a5618e9432c2d067f47ab09832", "ad8ba0c6e3ff4fceef3a70189d53ee3a2e1e7a72", "c80c7ab615b2fad5148a7848dbdd26a2dc50dd3d", "d36efb9ad91e00faa334b549ce989bfae7e2907a", "c04f7f16ba5d5750e7be4b7a73f53b46eda2d54f"], "url": "https://www.semanticscholar.org/paper/624a5c97be5d3ec63d48c34db25726008e5d92a4"}, "71480da09af638260801af1db8eff6acb4e1122f": {"id": "71480da09af638260801af1db8eff6acb4e1122f", "title": "Decoding with Large-Scale Neural Language Models Improves Translation", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2651937", "name": "Yinggong Zhao", "paperCount": 22, "citationCount": 404, "hIndex": 7}, {"authorId": "2372307", "name": "Victoria Fossum", "paperCount": 11, "citationCount": 523, "hIndex": 8}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}], "abstract": "We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2013, "reference_count": 24, "citation_count": 271, "influential_paper_citations": 37, "is_open_access": false, "citations": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "1956c239b3552e030db1b78951f64781101125ed", "1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6", "0894b06cff1cd0903574acaa7fcf071b144ae775", "56edaa1368ff4dfa45388e4be24fdfbded7d88a7", "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "247328a082d86199ed5a98e1d726aa205c1da9df", "e75b3c12da067552fda910a5bbed8b4d0e82dbcb", "9ec499af9b85f30bdbdd6cdfbb07d484808c526a"], "references": ["64da1980714cfc130632c5b92b9d98c2f6763de6", "d36b19b4c5977dd2a2796a5ad3508a3d8a087809", "37bd286d18965c943ca1937b7a13d4f8ad34d9b4", "5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "522e90b9fccfd3c1c0603359eb04757d770c1ab5", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "953dbe4541b82b6ac54dce85ab83734a02b6d30b", "96364af2d208ea75ca3aeb71892d2f7ce7326b55", "77dfe038a9bdab27c4505444931eaa976e9ec667", "3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "a538b05ebb01a40323997629e171c91aa28b8e2f", "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "0db6eb46ca9941660acc775e3ca39bf4434c18be", "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "8b395470a57c48d174c4216ea21a7a58bc046917", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "d6fb7546a29320eadad868af66835059db93d99f", "1f12451245667a85d0ee225a80880fc93c71cc8b", "de2df29b0a0312de7270c3f5a0af6af5645cf91a", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "611e948f2cf0b6e519ac04ce689d48a43a78f9ce", "44e525e52a680205b842de21c586efe85c7d7ef5"], "url": "https://www.semanticscholar.org/paper/71480da09af638260801af1db8eff6acb4e1122f"}, "a2a76bc4f30f7551b72145797d9604db1d0adf41": {"id": "a2a76bc4f30f7551b72145797d9604db1d0adf41", "title": "The International Workshop on Language Preservation: An Experiment in Text Collection and Language Technology", "authors": [{"authorId": "21308992", "name": "Steven Bird", "paperCount": 152, "citationCount": 12420, "hIndex": 39}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}, {"authorId": "69533036", "name": "F. Frowein", "paperCount": 7, "citationCount": 16, "hIndex": 2}, {"authorId": "5656905", "name": "Andrea L. Berez", "paperCount": 46, "citationCount": 197, "hIndex": 7}, {"authorId": "70613399", "name": "M. Eby", "paperCount": 8, "citationCount": 17, "hIndex": 3}, {"authorId": "1991080", "name": "Florian R. Hanke", "paperCount": 7, "citationCount": 104, "hIndex": 4}, {"authorId": "2056488540", "name": "Ryan Shelby", "paperCount": 3, "citationCount": 14, "hIndex": 2}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "41133277", "name": "Ada Wan", "paperCount": 6, "citationCount": 12, "hIndex": 2}], "abstract": "With hundreds of endangered and under-documented languages, Papua New Guinea presents an enormous challenge to the documentary linguistics community. This article reports on a workshop held at the University of Goroka in May and June of 2012. The workshop aimed to collect written texts and their translations for several languages, while building local capacity through hands-on training, and improving our understanding of the appropriate use of technology. The majority of participants were mother tongue speakers who seek to preserve their languages through the preparation of written language resources.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2013, "reference_count": 17, "citation_count": 7, "influential_paper_citations": 1, "is_open_access": false, "citations": ["763a2c057f529bfd3bf417fa60d93b3d5b219068", "0cc32a96f44bfa362056bb87741d01e3a91d2ee7", "bc800601c24eb64dbee2bc58bdf7e453931584ca", "48a8ad43452245083db56e9fcfdb6aa67adac1f8", "025d43da93733e9a95592aff81cae8d458acf13b", "7b5bb4deadea5b46ca3eeca0d67cdc1865a8c116", "6adabb3b3346525e742043d78d8fdf820154f781"], "references": ["114a67d5e1ddc048c5ca80d709995ebd35ca2548", "1b993b6e66c79a16ffa8b59b820fe63072a45b92", "6e198ce93046c12ca000fc76c405b2242dca1fd0", "f5c67fffef7ebe756159d525f088405741343223", "ee4def9e45cff20997dd64f5f06abaaf00a837c6", "36ffcc1cc218ca36de384a107fb48e5abe2e6359", "667103ea0f6c9df4d49a46ec13ef312289f5c9ae", "dee069a0945c471a7cb07acf94fa70ba5c805bff", "e2b335a605281908dbe5216876c16bc4a82ab157", "3744ed63d1c15cc8033fe6c8ee9331abc23f7d10", "c648c647bf3c8589ed1814eb1495d3a7dcff1a72", "ab7b5917515c460b90451e67852171a531671ab8", "a678f709553174a1139c54b538dc409f40298bcb"], "url": "https://www.semanticscholar.org/paper/a2a76bc4f30f7551b72145797d9604db1d0adf41"}, "8eb2ba39451789f4e0d9e8e5ab97202f5a7b2c80": {"id": "8eb2ba39451789f4e0d9e8e5ab97202f5a7b2c80", "title": "Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "144768480", "name": "Liang Huang", "paperCount": 75, "citationCount": 4268, "hIndex": 31}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}], "abstract": "Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an l0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2012, "reference_count": 34, "citation_count": 35, "influential_paper_citations": 0, "is_open_access": false, "citations": ["1881b09a3f9770bf8b7b2ffc473b35e2379b1ff3", "a8a29510dfb634ce5d2f97137dab52fa4ec5e36d", "672159105ca8ac2e543ba4e9549ffe333b8566d9", "96645793d9961042f735d114d17095722f42e357", "acdd7809fe86f300d659792b27733601646fc860", "a7fb2cf3d5a0b30458e74d316a1ce3d6ab6f40b1", "26cf02e5e8b0a6d5feea952efddcbb9040bf9939", "c0371ecfc0d1af5005af1fb2c2b0b95d822fd870", "7f0232b9ffbc637cdddaed2520098cddc066e5d6", "36a9f6c1f6c0353635b2e0c530f20dab816a384d"], "references": ["6fb74fc8ec03a3f23b01e898186748e6c4ac0085", "aa1b28f1010d4b563e2abf0c3ee1d2d888c941b9", "a51e7df1baaae9a2757b14a050d7b045df20ef70", "095c7aeb61dbb9d4e1182b0ba14f2944c2fa2040", "270f0ef0f96ccb6a8c628f923372727b8d8a135b", "75050fa8347fc0989a643f3debf343f1c9cab9d1", "61bee45a5a723b9f340a203c405088c835331212", "9b12788644f85878230974be9def6472cfeec59a", "7fdbb9f2a0caaa0813d26756a2d071959b3dd5a5", "ed7c7c079c8c54d3b82e016cc52a7a2c3a61f237", "52805ca2a7f5f6e73dc90ff20f1ca2f198dd031b", "3bb5a439a0d610a7eac68f73068cdd278b8c9775", "0db6eb46ca9941660acc775e3ca39bf4434c18be", "932a106c21a1db1e1876459c1521d27fd152caac", "f4f6bfacb4cd508df62540f5aa9ba30cd83dd127", "a4d8678e42a834b539218eba14c51453691f1910", "64a007a07cbeab1b6949f196e58fdbe93ef1a297", "cee30e5fe700b98bc408bc40ea9ec396520b473a", "c6a83c4fcc99ba6753109301949c5b7cfa978079", "aac7d167fb9e515583dd437d2a855275cda4ddff", "cb826a3899752b796f14df1c50378c64954a6b0a", "b78d00a50745c1833e513b8c188d372a35a5a184", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "65d34977d9055f42e51dc1e7d9b4ca2f36c17537", "59c442932e9fcfcac6df5566c2bcd1ec331548c9", "ab7b5917515c460b90451e67852171a531671ab8", "d36efb9ad91e00faa334b549ce989bfae7e2907a", "d4143c46910f249bedbdc37caf88e4c292124c08"], "url": "https://www.semanticscholar.org/paper/8eb2ba39451789f4e0d9e8e5ab97202f5a7b2c80"}, "4742c64d9a57556de375269c5f257717d6f3c0d7": {"id": "4742c64d9a57556de375269c5f257717d6f3c0d7", "title": "Rule Markov Models for Fast Tree-to-String Translation", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2013337", "name": "Haitao Mi", "paperCount": 52, "citationCount": 1563, "hIndex": 19}, {"authorId": "144768480", "name": "Liang Huang", "paperCount": 75, "citationCount": 4268, "hIndex": 31}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}], "abstract": "Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B) as composed rules.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2011, "reference_count": 30, "citation_count": 19, "influential_paper_citations": 2, "is_open_access": false, "citations": ["a80496d80932592749bd7f4d2c9ea4267237765e", "a8a29510dfb634ce5d2f97137dab52fa4ec5e36d", "9b96a0c9dff57484cc2b5b3a0a394ea20c9d669b", "76232f8fa3377c7382220a196470d11cb30fb45c", "47383663320442a3004551c3235416661bfcc153", "6c4b2071bb58161c5140fb6585bd1b47cedc04c1", "2e06e975942facea3e5572ad829636a53aa4aafb", "b9657cc862298e102413220743a8046ea5ec8c63", "7fcca057819b8eca61061a6af2172ff50d8f152c", "958ffa31d19a3ebe0b4f162a31c41d105e957ff9"], "references": ["399429f1a423c709bb16e9c5d223a9fafaa4f6b5", "e202e4482a24fbc0cbcd55347935121b82c7e81c", "9e427c9ec538ad2e985257383814c5c38b50c91d", "b0ab6db333e835b2d216213c5036361b4fd590f4", "c0a785bc32a724e70840a71e1f60ac5901775ae2", "6e1b12dd7ca3443ba6e0fb38dfa3587a4eb5d539", "d01737b617acc555153f4660417908bf3971b1a5", "7e982f360b44094552264010781a476d85ac78a7", "489b5ea0325df00639465d0f92e2bcf34d16db04", "5d29fe90bb44f80b35d77786f7b561fe29fc3639", "53b4aaf51c6d1c164b19e8f9df5cfde560eeb6a7", "33c8447e7b7ac1338ae151bba154bd2a78f8981e", "cb826a3899752b796f14df1c50378c64954a6b0a", "a7e925a65860e90b2b4eb427a8bc497f76b2fe6e", "1f12451245667a85d0ee225a80880fc93c71cc8b", "a9d59174cc50b119ee4be19b3e65177431e37003", "399da68d3b97218b6c80262df7963baa89dcc71b", "6d7f8d91b805d6962ed16fc2bacb40bc5e11b825", "6c9f553e723a40a6713453b734b552c1928bf52b", "06e0f2d078af6144bdc14d3bdb06df4f7306d587", "b3e53b9c0e3a7a60e7a5295e9b08af74d6fb3dbf", "467671c61c57792d2b5fc6cf272243edfd90cff0"], "url": "https://www.semanticscholar.org/paper/4742c64d9a57556de375269c5f257717d6f3c0d7"}, "ff35bed91a92df31ad281c0f59fe07e09c3f4767": {"id": "ff35bed91a92df31ad281c0f59fe07e09c3f4767", "title": "Models and Training for Unsupervised Preposition Sense Disambiguation", "authors": [{"authorId": "2022288", "name": "Dirk Hovy", "paperCount": 122, "citationCount": 4293, "hIndex": 35}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "3139108", "name": "S. Tratz", "paperCount": 44, "citationCount": 849, "hIndex": 16}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}, {"authorId": "144547315", "name": "E. Hovy", "paperCount": 647, "citationCount": 40036, "hIndex": 92}], "abstract": "We present a preliminary study on unsu-pervised preposition sense disambiguation (PSD), comparing different models and training techniques (EM, MAP-EM with L0 norm, Bayesian inference using Gibbs sampling). To our knowledge, this is the first attempt at un-supervised preposition sense disambiguation. Our best accuracy reaches 56%, a significant improvement (at p <.001) of 16% over the most-frequent-sense baseline.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2011, "reference_count": 21, "citation_count": 16, "influential_paper_citations": 4, "is_open_access": false, "citations": ["07bb6fc501eef8a6c298ebcf60b7dea46b6a23c0", "27cef09fff06703f66e6c85e98d7221046fde5aa", "39aaf8fbdc97b45aaf368da10791e9333f1fb8d6", "28ce291ef46bfeba381446f67e0f8ab06917adef", "9035bacb3e163fbc3d039b7ab7c74f88d48ddb49", "3623cf7267263cef2a0bfbf9b04b10fee7851ade", "17c414f8abb8567043e7fe95f48c8e6a85b43e78", "9ab4569d1971dfff0fd79e49a2427e12c0e06334", "6749d419b034cd8f176a4af15da7542b489c2aba", "20217099318328a3b2a470caf50e057fdc02c522"], "references": ["ce7dcf9f420469381ef4799700b7b4ad6bc48241", "75050fa8347fc0989a643f3debf343f1c9cab9d1", "0145bbf9eebd25655a239a52bf8532fbc93556c3", "9e022fa8effeaaff77d86be0a3d1bf50d899b5b8", "3a5a80f960ddaba0bdf4c4bf7e678cc87ffdb07f", "2d898632916468eeb79513f92d1938f6ca53839d", "52341e9dc9d1b2f4ea967357e3c5ad9d320b2228", "2aa27b532e4114019a49ca6a8f783695648da815", "4d8f5442943012bdcb0b8f79d709babcff338a88", "dbc534ec123274b8f5eb5588c6b3a9652a564a0d", "1172c5d0a5f423d4bd8064df979b2efeb5230ac4", "bbe013543e9c1d8f00499036121c363ad3ab3d7d", "89622de8b2d3f066e740aa0e559be2edb259a9f7", "0939d462723c83348d3c4fe235a112a3ebd052df", "bad5951d14294afe14dc0c52eaa8ecd543703ee9", "dbafcda95fa3e7e9b5627472d6d4fb46454828ce", "d87ceda3042f781c341ac17109d1e94a717f5f60", "d36efb9ad91e00faa334b549ce989bfae7e2907a"], "url": "https://www.semanticscholar.org/paper/ff35bed91a92df31ad281c0f59fe07e09c3f4767"}, "75050fa8347fc0989a643f3debf343f1c9cab9d1": {"id": "75050fa8347fc0989a643f3debf343f1c9cab9d1", "title": "Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "13461242", "name": "Adam Pauls", "paperCount": 27, "citationCount": 1004, "hIndex": 15}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}], "abstract": "The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2010, "reference_count": 15, "citation_count": 12, "influential_paper_citations": 1, "is_open_access": false, "citations": ["e388d24f72988f5b62cf42598e638b4f3c184fcb", "ff35bed91a92df31ad281c0f59fe07e09c3f4767", "39aaf8fbdc97b45aaf368da10791e9333f1fb8d6", "c14a0df3058dda2b01732306baf370c26d137b08", "94ab94a0672da26049b72b0116a951bf7e9c40e5", "99908574b73cb1461d7d15c527a87c2217cb7283", "e945672c19a017baa9caf3a365e197316886327b", "8eb2ba39451789f4e0d9e8e5ab97202f5a7b2c80", "89ec55e7f4c4aa567c1da52c071cec633e123d9d", "754efc7a473255e5c93b6a87eeb8ace6afe311f9"], "references": ["0145bbf9eebd25655a239a52bf8532fbc93556c3", "ce56527c77fd36c90b3e924a71fd9cd3db38e992", "26a6aaa83a74ea37b630c3fd636fad9cfebb302c", "3913b7008517c2ff0da17207da0c4c99fc5437cc", "023214084fb8b56b4027e9bcfaa3b8f3cd0db829", "973944f4c72271e0c68c91a6c5072f088a0e9d50", "49fe20074129344833ed1033bb9f207467c0550e", "3bb5a439a0d610a7eac68f73068cdd278b8c9775", "469d720411f8d8d75d2352b170dbe4611b508cff", "932a106c21a1db1e1876459c1521d27fd152caac", "9452e711ce2e7e0d4e35aaeb5ab8731de62a5809", "65d34977d9055f42e51dc1e7d9b4ca2f36c17537", "4614650c3bb3e835c80612d3bca9586f81db95a3", "d36efb9ad91e00faa334b549ce989bfae7e2907a"], "url": "https://www.semanticscholar.org/paper/75050fa8347fc0989a643f3debf343f1c9cab9d1"}, "f21b7b109a1c4822bfe0909554203befe4934d76": {"id": "f21b7b109a1c4822bfe0909554203befe4934d76", "title": "Fast, Greedy Model Minimization for Unsupervised Tagging", "authors": [{"authorId": "35014893", "name": "Sujith Ravi", "paperCount": 142, "citationCount": 3286, "hIndex": 29}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "152971314", "name": "Kevin Knight", "paperCount": 258, "citationCount": 19644, "hIndex": 70}, {"authorId": "145287425", "name": "David Chiang", "paperCount": 128, "citationCount": 7599, "hIndex": 33}], "abstract": "Model minimization has been shown to work well for the task of unsupervised part-of-speech tagging with a dictionary. In (Ravi and Knight, 2009), the authors invoke an integer programming (IP) solver to do model minimization. However, solving this problem exactly using an integer programming formulation is intractable for practical purposes. We propose a novel two-stage greedy approximation scheme to replace the IP. Our method runs fast, while yielding highly accurate tagging results. We also compare our method against standard EM training, and show that we consistently obtain better tagging accuracies on test data of varying sizes for English and Italian.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2010, "reference_count": 15, "citation_count": 13, "influential_paper_citations": 5, "is_open_access": false, "citations": ["727209a57c643472c8fc166ba3cc373936acc8d0", "e388d24f72988f5b62cf42598e638b4f3c184fcb", "a2d87894b31c8621ec3c3d72328bdfb142ea4b5a", "9e5cc076b5c5d816a0bb00c28ad1d7dd82221b5c", "c49f55ad0c47a02af2dc546140de33e8d07f91b8", "4e2e03e4ad4bb759b17768a5ae83b6d992b9fed5", "9c01a3e29f62b7e8967b03af0016e2a555ddd2c2", "6b4059132338c16eb10b8d593b0dfaf6e6bde15c", "f9ee3344215b64be39fa999db2a34386b4ef27dd", "a205d899cac91fc20b8aaa49fc997523202285dc"], "references": ["ce56527c77fd36c90b3e924a71fd9cd3db38e992", "60df78bc14265d4b87ae5993d6b5781ccf7ec5d8", "9b12788644f85878230974be9def6472cfeec59a", "3913b7008517c2ff0da17207da0c4c99fc5437cc", "3bd05be25c18887248f101505b8cefbfccc44843", "023214084fb8b56b4027e9bcfaa3b8f3cd0db829", "1ec86811a79fb02a1c551b8f418314a00f5f5a99", "469d720411f8d8d75d2352b170dbe4611b508cff", "9452e711ce2e7e0d4e35aaeb5ab8731de62a5809", "480519b1488d3457a714342d4ffe7a3e363daf70", "4614650c3bb3e835c80612d3bca9586f81db95a3", "08421c787d9a9e2d9f5ba4bda1bfc9866fa4b04f", "bdede1e17c947540b50e6e2db9e8467ddc6e7336", "d36efb9ad91e00faa334b549ce989bfae7e2907a", "ecb37a4e32d6faef4ac99b45d9ab9b2d92693985"], "url": "https://www.semanticscholar.org/paper/f21b7b109a1c4822bfe0909554203befe4934d76"}, "6f9d1977c876c056969a7d1167e37cb77875218c": {"id": "6f9d1977c876c056969a7d1167e37cb77875218c", "title": "Hassan: A Virtual Human for Tactical Questioning", "authors": [{"authorId": "144518646", "name": "D. Traum", "paperCount": 347, "citationCount": 12697, "hIndex": 54}, {"authorId": "145753983", "name": "Antonio Roque", "paperCount": 40, "citationCount": 912, "hIndex": 14}, {"authorId": "3201827", "name": "A. Leuski", "paperCount": 108, "citationCount": 2713, "hIndex": 28}, {"authorId": "1765829", "name": "P. Georgiou", "paperCount": 214, "citationCount": 4000, "hIndex": 32}, {"authorId": "3023920", "name": "Jillian Gerten", "paperCount": 14, "citationCount": 689, "hIndex": 10}, {"authorId": "2708061", "name": "Bilyana Martinovski", "paperCount": 48, "citationCount": 455, "hIndex": 11}, {"authorId": "145254843", "name": "Shrikanth S. Narayanan", "paperCount": 1203, "citationCount": 32958, "hIndex": 84}, {"authorId": "145341454", "name": "Susan Robinson", "paperCount": 16, "citationCount": 294, "hIndex": 10}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": "We present Hassan, a virtual human who engages in Tactical Questioning dialogues. We describe the tactical questioning domain, the motivation for this character, the specific architecture and present brief examples and an evaluation.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2007, "reference_count": 16, "citation_count": 43, "influential_paper_citations": 2, "is_open_access": false, "citations": ["28a07e4379ab54a2d779735c69776f58ce638576", "eec4f41b7d430729d3f59faad57f9a1e8682ead0", "c2ea2e87cfac2bf0666ce2246db5c4b44198d88b", "4cab198a9daeb328e2876aeaf1a8d52c09720eae", "03fde2b1850ce1d1afd88c6e80ded3cad8f106b7", "b80e778fcb19ea8b126bdce1dc257330c646bdb7", "697589187eeb8e61de7bd39a5d5005e20c4d7b89", "dd17c428bc5eb6296fc956e69fba731d3c196d5e", "f19ccc7bf6804879a3b5bfadb77a9202387ad268", "7c360fe80286253ade2dfaf4ea1a0362960640b9"], "references": ["abd3db64cb99c139aabb25a8bfa2770ac86c0a92", "6150cc7c29514ef3a941e559c38588075bbb9907", "cd2dd4c17e42c3de1681a1c9834803e99740db1e", "edfa9baa3ca75550b5c40901324a61358742d25f", "1c3a7081f50db04fb15264a63c613ebe3c384bc6", "dca0b348db06fca7c429734a055b056826dc0591", "cd07a6d24e9590db7041e7d60b1d1308bce76215", "f2f0ddae977c6cba5cef363a00f66036967b0dd5", "22c7a4cb18a311801aca3237dc71cf700d1e78ba"], "url": "https://www.semanticscholar.org/paper/6f9d1977c876c056969a7d1167e37cb77875218c"}, "8716cc16476da020acd609d6c6f0b5364196f839": {"id": "8716cc16476da020acd609d6c6f0b5364196f839", "title": "Evaluation of a Spoken Dialogue System for Virtual Reality Call for Fire Training", "authors": [{"authorId": "145341454", "name": "Susan Robinson", "paperCount": 16, "citationCount": 294, "hIndex": 10}, {"authorId": "145753983", "name": "Antonio Roque", "paperCount": 40, "citationCount": 912, "hIndex": 14}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "144518646", "name": "D. Traum", "paperCount": 347, "citationCount": 12697, "hIndex": 54}, {"authorId": "2068097849", "name": "Charles Hernandez", "paperCount": 1, "citationCount": 12, "hIndex": 1}, {"authorId": "115844124", "name": "Bill Millspaugh", "paperCount": 1, "citationCount": 12, "hIndex": 1}], "abstract": "Abstract : We present an evaluation of a spoken dialogue system that engages in dialogues with soldiers training in an immersive Call for Fire (CFF) simulation. We briefly describe aspects of the Joint Fires and Effects Trainer System, and the Radiobot-CFF dialogue system, which can engage in voice communications with a trainee in call for fire dialogues. An experiment is described to judge performance of the Radiobot CFF system compared with human radio operators. Results show that while the current version of the system is not quite at human-performance levels, it is already viable for training interaction and as an operator-controller aid.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2007, "reference_count": 9, "citation_count": 12, "influential_paper_citations": 1, "is_open_access": false, "citations": ["351cc4032bd693ab1a8ead12f56c32f5163df613", "e1941b38cb70ebba548537070ec726ecbb439fbc", "2779b22fb79ef32b6843c950b8361b46746af801", "92c2734c9f3feec9e9246df273001bdcd7bb7c4c", "f1055980f9cccb32e50c3fba84742851f53ab3d3", "9057fe6a86cc9cc73243d355a1e12645f0833903", "1c0becf051dd81f46ddc597f15bfe330afed2ff5", "f25e9a19b85cbdece94c85beda2ef6766cbcd69d", "b8d30b662bb4e8dc65f22e9181be3db86c629e99", "3fbff30383aeb5e38cd745d4f0215e8b05aa5c9c"], "references": ["bb84331ae06f81215a1df05019c1b7d137863934", "a3719c43090fe99beed984c0884d3c40a4646c2e", "6fc61cd0170ef66fc81babc736d51a4e8199c210", "5b1724235b196bec5b2e22000517ff92a7a45e10", "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "0059e902765146e58542a6716b69e2993793be55"], "url": "https://www.semanticscholar.org/paper/8716cc16476da020acd609d6c6f0b5364196f839"}, "bb84331ae06f81215a1df05019c1b7d137863934": {"id": "bb84331ae06f81215a1df05019c1b7d137863934", "title": "Activity-Based Dialogue Analysis as Evaluation Method.", "authors": [{"authorId": "2708061", "name": "Bilyana Martinovski", "paperCount": 48, "citationCount": 455, "hIndex": 11}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": "This paper uses linguistic activity-based dialogue analysis in order to characterize, evaluate and compare activities. We find that human-human and human-machine interaction via radio are equally efficient but offer different styles of learning and instruction.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2007, "reference_count": 8, "citation_count": 2, "influential_paper_citations": 0, "is_open_access": false, "citations": ["fb3b949a36a30357d2a24baa48f478e856300f28", "8716cc16476da020acd609d6c6f0b5364196f839"], "references": ["b4c2d7c1527f4e11b2b496d6aba8199458594294", "138c9f8bdb4359465c22dfd36f6a7ce22e23ee46", "390a07acc5e5482cd5a11ad9191e140e495edb0e", "f829fbe07e951cb18922225a4d8527db59afe6b9"], "url": "https://www.semanticscholar.org/paper/bb84331ae06f81215a1df05019c1b7d137863934"}, "cd38f0cfac361a064313a0ac0c6296a84f948b6d": {"id": "cd38f0cfac361a064313a0ac0c6296a84f948b6d", "title": "A Virtual Human for Tactical Questioning", "authors": [{"authorId": "2708061", "name": "Bilyana Martinovski", "paperCount": 48, "citationCount": 455, "hIndex": 11}, {"authorId": "144518646", "name": "D. Traum", "paperCount": 347, "citationCount": 12697, "hIndex": 54}, {"authorId": "145753983", "name": "Antonio Roque", "paperCount": 40, "citationCount": 912, "hIndex": 14}, {"authorId": "3201827", "name": "A. Leuski", "paperCount": 108, "citationCount": 2713, "hIndex": 28}, {"authorId": "1765829", "name": "P. Georgiou", "paperCount": 214, "citationCount": 4000, "hIndex": 32}, {"authorId": "3023920", "name": "Jillian Gerten", "paperCount": 14, "citationCount": 689, "hIndex": 10}, {"authorId": "152434613", "name": "Shrikanth S. Narayanan", "paperCount": 53, "citationCount": 143, "hIndex": 7}, {"authorId": "145341454", "name": "Susan Robinson", "paperCount": 16, "citationCount": 294, "hIndex": 10}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": null, "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2007, "reference_count": 4, "citation_count": 13, "influential_paper_citations": 0, "is_open_access": false, "citations": ["26435746f1b7ed2563cad2f5ac9f650f70909ff1", "aa134b16258a58bbca12f7bc2aea2320c224d8ed", "c2ea2e87cfac2bf0666ce2246db5c4b44198d88b", "1b5144df9bf96a02e46745f2e8cd334980108b2b", "f19ccc7bf6804879a3b5bfadb77a9202387ad268", "7c360fe80286253ade2dfaf4ea1a0362960640b9", "0da7063cd58343c028b3ef9484c8dc06d7035a8b", "85d9bf2f213f65a3e0e0bc0344208a9f91d9defc", "332f3f74f36140713cce7721503e692001514cac", "24aa444519fd48d5c57ea5ed2e5628bf5d5da921"], "references": ["abd3db64cb99c139aabb25a8bfa2770ac86c0a92", "dca0b348db06fca7c429734a055b056826dc0591", "f2f0ddae977c6cba5cef363a00f66036967b0dd5", "22c7a4cb18a311801aca3237dc71cf700d1e78ba"], "url": "https://www.semanticscholar.org/paper/cd38f0cfac361a064313a0ac0c6296a84f948b6d"}, "5b1724235b196bec5b2e22000517ff92a7a45e10": {"id": "5b1724235b196bec5b2e22000517ff92a7a45e10", "title": "Radiobot-CFF: a spoken dialogue system for military training", "authors": [{"authorId": "145753983", "name": "Antonio Roque", "paperCount": 40, "citationCount": 912, "hIndex": 14}, {"authorId": "3201827", "name": "A. Leuski", "paperCount": 108, "citationCount": 2713, "hIndex": 28}, {"authorId": "3145973", "name": "V. Sridhar", "paperCount": 41, "citationCount": 735, "hIndex": 13}, {"authorId": "145341454", "name": "Susan Robinson", "paperCount": 16, "citationCount": 294, "hIndex": 10}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "145254843", "name": "Shrikanth S. Narayanan", "paperCount": 1203, "citationCount": 32958, "hIndex": 84}, {"authorId": "144518646", "name": "D. Traum", "paperCount": 347, "citationCount": 12697, "hIndex": 54}], "abstract": "We describe a spoken dialogue system which can engage in Call For Fire (CFF) radio dialogues to help train soldiers in proper procedures for requesting artillery fire missions. We describe the domain, an information-state dialogue manager with a novel system of interactive information components, and provide evaluation results.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2006, "reference_count": 5, "citation_count": 19, "influential_paper_citations": 0, "is_open_access": false, "citations": ["351cc4032bd693ab1a8ead12f56c32f5163df613", "d3aca13c966bb22eed7086baeb287a64bc18c152", "2d78f555a087a34a2ec95e0e1ea55e693e07fd61", "81bb81d22a5c49e2e38008cd402820b30d3a3044", "d32d40d3ec44f9d00b3c6805fda2b48bed8e5645", "8716cc16476da020acd609d6c6f0b5364196f839", "5a7c7025c85b01aab55689ec4ee2f6493f3ac24b", "8a25716f30a1af2dd6949e6bf0a298b14de5ff82", "bbbfbf62cd3b32247572cfc42aeeb810d03c1486", "57dae61c98bca59bb74b48380bc2f3cc39a5a196"], "references": ["897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "0059e902765146e58542a6716b69e2993793be55"], "url": "https://www.semanticscholar.org/paper/5b1724235b196bec5b2e22000517ff92a7a45e10"}, "6bae9b8789a1d44e86ee7cfff260105285675246": {"id": "6bae9b8789a1d44e86ee7cfff260105285675246", "title": "Radiobot-CFF : A Spoken Dialogue S", "authors": [{"authorId": "145753983", "name": "Antonio Roque", "paperCount": 40, "citationCount": 912, "hIndex": 14}, {"authorId": "3201827", "name": "A. Leuski", "paperCount": 108, "citationCount": 2713, "hIndex": 28}, {"authorId": "145341454", "name": "Susan Robinson", "paperCount": 16, "citationCount": 294, "hIndex": 10}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2073035402", "name": "Vivek Rangarajan", "paperCount": 4, "citationCount": 20, "hIndex": 2}, {"authorId": "2085822827", "name": "I. Narayanan", "paperCount": 1, "citationCount": 1, "hIndex": 1}, {"authorId": "144518646", "name": "D. Traum", "paperCount": 347, "citationCount": 12697, "hIndex": 54}], "abstract": "We describe a spoken dialogue system which can engage in Call For Fire (CFF) radio dialogues to help train soldiers in proper procedures for requesting artillery fire missions. We describe the domain, an information-state dialogue manager with a novel system of interactive information components, and provide evaluation results.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2006, "reference_count": 5, "citation_count": 1, "influential_paper_citations": 0, "is_open_access": false, "citations": ["e45c5cdeacea3e595e56a14c934b506a12c51d79"], "references": ["897249c93f55ef1c0d2aa1e799eb67b414c6d4a6", "0059e902765146e58542a6716b69e2993793be55"], "url": "https://www.semanticscholar.org/paper/6bae9b8789a1d44e86ee7cfff260105285675246"}, "7a8fd9b1621ba7f1db89cfe32aec0a9b611a31e5": {"id": "7a8fd9b1621ba7f1db89cfe32aec0a9b611a31e5", "title": "o BNL-20062 QUANTITATION OF THE DEGREE OF OSTEOPOROSIS BY MEASURE OF TOTAL-BODY CALCIUM EMPLOYING NEUTRON ACTIVATION * by", "authors": [{"authorId": "5662238", "name": "S. Cohn", "paperCount": 302, "citationCount": 8086, "hIndex": 50}, {"authorId": "7027242", "name": "I. Zanzi", "paperCount": 103, "citationCount": 2468, "hIndex": 21}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}, {"authorId": "2056041471", "name": "S.", "paperCount": 3, "citationCount": 0, "hIndex": 0}], "abstract": "NOTICE \u2014 \u2014 \u2014 \u2014 \u2014 T7iis report was prepared as an account of work sponsored by the United Slues Government. Neither rile United Sales nor the United Slates Energy Research and Development Administration, nor any of their employees, nor any of tlieir contractors, subcontractors, or their employees, mafces any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness or usefulness of any information, apparatus, product or process disclosed, or represents \"\u2022-\u2022 '' would not", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2005, "reference_count": 9, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["cf787a2aa33c088d412b193b0da2917d49ba8d98", "edbb60dc356dce07cf03f425716053931b9b539f", "83eaab0486d72950149f786729a2644414390cee", "d3323117912da28d4e58b8d6cffc104fac763bd3", "09d9d83f45e91b60120456bebfa57737355f64f6", "b46bba6e0584906edd11b559eb9cee5c85c14263", "98db935cc27aed9b1f6067cae126d588a4896914", "62a88e289947e7ab2371486cd0d98dde064efda4"], "url": "https://www.semanticscholar.org/paper/7a8fd9b1621ba7f1db89cfe32aec0a9b611a31e5"}, "8516e3ddf8163d5002274929000770a26296a5d4": {"id": "8516e3ddf8163d5002274929000770a26296a5d4", "title": "Cs599: Structure and Dynamics of Networked Information (spring 2005) 02/09/2005: Correlation Clustering Scribes: Ashish Vaswani 1 a Simple Algorithm for Maximizing Agreements", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": "So far, we have mostly talked about communities in the sense of discovering one, or a few, densely linked subgraphs. We departed from this interpretation at the end of last lecture, when we defined the notion of the modularity of a clustering. There, we are interested in the division of a graph into disjoint partitions (or clusters) of nodes, and the quality of this clustering. Clustering of data, mostly in metric spaces, is one of the most well-studied problems in CS, mostly due to its applications to machine learning and classification. Here, we look at the relatively new concept of correlation clustering [1]. In correlation clustering, each edge of the graph is annotated with a label of \u2018+\u2019 or \u2018-\u2019, expressing that the two endpoints were observed to be similar or dissimilar, respectively. The goal is then to find a clustering that puts many \u2019+\u2019 edges inside clusters, and \u2019-\u2019 edges between clusters. However, these goals may be conflicting, as can be seen for a triangle with two edges labeled \u2019+\u2019 and one labeled \u2019-\u2019. Notice that the number of clusters is not pre-specified in this problem. This notion of clustering can be useful when we can identify if a link constitutes an endorsement, or the opposite. For instance, in many competitive scenarios (for instance, politics or sports), pages will link to other pages with the explicit goal of deriding the content. This can be frequently identified from anchor text and similar clues. In this sense, correlation clustering may help us in identifying communities with aligned interests, which compete with other communities. More formally, given the graph G = (V,E) on n vertices, we write +(i, j) if the edge between i and j is labeled \u2018+\u2019 and similarly for \u2212(i, j). The optimization problem can now be expressed in two ways:", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2005, "reference_count": 0, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": [], "url": "https://www.semanticscholar.org/paper/8516e3ddf8163d5002274929000770a26296a5d4"}, "8e39b1fe4ad92b45a76e0e3f9fe42ecea1a78ba6": {"id": "8e39b1fe4ad92b45a76e0e3f9fe42ecea1a78ba6", "title": "Cs599: Structure and Dynamics of Networked Information (spring 2005) 03/02/2005: Rank Aggregation Scribes: Nupur Kothari and Ranjit Raveendran", "authors": [{"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": "We have observed that random graphs do not model social networks well, as social networks exhibit a lot of clustering, whereas triangles or other small cycles are rare in sparse random graphs. Instead, we considered a model proposed by Watts and Strogatz [3] that superimposed random edges on top of a clustered graph, e.g., a grid graph, thus obtaining both high clustering and small diameter. However, returning to Milgram\u2019s experiment, this does not yet answer the question of how people were actually able to find these short paths. Indeed, we saw that when the random links were uniformly random, then no local rule can find these short paths. So we want to adjust the model, and in particular the random long-range link distribution, to explain how people were actually able to find paths. We assume that the underlying \u201cclustered\u201d graph is the 2-dimensional grid, and study the effect of different distributions of long-range links. Let\u2019s say Prob[v \u2192 w], the probability that v connects to w via the long-range link, is a function of dv,w, the distance between v and w. Intuitively, it seems that connections between distant individuals are less likely, so the probability should be monotone decreasing. If Prob[v \u2192 w] is an inverse exponential function of dv,w, then it decreases very rapidly, and long-range links are too unlikely. Hence, we use a polynomially decreasing function in dv,w [1], i.e., Prob[v \u2192 w] \u223c (dv,w) for a constant \u03b1 \u2265 0. To understand this distribution well, we need to calculate its normalizing constant, i.e., the \u03b3 such that Prob[v \u2192 w] = 1 \u03b3 d \u2212\u03b1. By noticing that for each d, there are \u0398(d) nodes at distance d from any given node v, we can calculate \u2211 w d \u2212\u03b1 v,w \u2248 \u2211n d=1 dd \u2212\u03b1 = \u2211n d=1 d 1\u2212\u03b1 = \uf8f2\uf8f3 \u0398(n 2\u2212\u03b1) for \u03b1 < 2 \u0398(log n) for \u03b1 = 2 \u0398(1) for \u03b1 > 2", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2005, "reference_count": 22, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["3f69ff45fc185abc918953e7b79df8675ceaa08f", "d72a4c789b904af8a5a19f8b73813c95d21c1f05", "44aea375e5482b4df10690370e8c422e123cf72a", "620cf63f101f20c03a2e530c287c2603839de15e", "4bc2ba9723e34832e35976654e56388cbbdfde17", "1bed57282935012df51ad87970cd01b90c2fb27c", "bad78a526180dd4e2a4cd6485e5dd5cb010b12f2", "ff5602a02809b8db51446bd22e75844c793958f0", "e2e073433931c4d1a739f548b7d17b6e9b2fa13e", "add4ecd63de0de0f70205a6eba19cb52af05845a", "0c6fbf40cf4fd1c8b4694e1e38bd03f8297e9cde", "d61031326150ba23f90e6587c13d99188209250e", "20a80a7356859daa4170fb4da6b87b84adbb547f", "52bacd5cd5d351ed772458aa20d71361e6063e55", "c9aece346139711b8c65c618da99cdbecb162575", "b9ec803dde773adbc2d19e5857cd63f05d4a9069", "cb06b66f4097107ef698f85c66a3244eddb14653", "1c74180188a592d20a63cedb45d53089201fe127", "bc22d1610ce680c91b4323a1899b1f22cfdf533f"], "url": "https://www.semanticscholar.org/paper/8e39b1fe4ad92b45a76e0e3f9fe42ecea1a78ba6"}, "b618c3b4d3f40a036307d00513afbe05e703135d": {"id": "b618c3b4d3f40a036307d00513afbe05e703135d", "title": "2 Schelling's Model for Segregation", "authors": [{"authorId": "2072722116", "name": "S. Bharathi", "paperCount": 5, "citationCount": 499, "hIndex": 1}, {"authorId": "40348417", "name": "Ashish Vaswani", "paperCount": 55, "citationCount": 55147, "hIndex": 26}], "abstract": "One of the first models studied explicitly in this context was Schelling\u2019s model for segregation [3]. Schelling was motivated by the question: why is it that most neighborhoods are very uniform (racially, and in other respects), even though most people profess that they would prefer to live in a diverse community? Schelling proposed the following model: Assume that roughly n 2 2 individuals live on an n\u00d7 n grid. Each node wants to make sure to not be completely isolated (the odd person in a community): formally, if less than an \u03b5 fraction of v\u2019s neighbors (in a small ball around v) are of the same color, v is unhappy and moves to a spot where it is not unhappy (say, the closest, or a random, such spot). What Schelling observed was that even with a relatively small value of \u03b5 \u2248 13 , neighborhoods end up mostly segregated: when the process quiesces, about 45 of each node\u2019s neighbors are of the same color as the node itself. While this result has been verified experimentally, it appears that no result formally proves this observation as of yet. It is also not known how the result relates with the topology of the neighborhood graph.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2005, "reference_count": 4, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["92b6aeb4e4f8e0c170f33ba274ce1830db9c9496", "c451c567916436bd557cdc4345966fec3fdac87e"], "url": "https://www.semanticscholar.org/paper/b618c3b4d3f40a036307d00513afbe05e703135d"}, "3df83a60f55c64b40e6dbcd99cf9f67894a0736e": {"id": "3df83a60f55c64b40e6dbcd99cf9f67894a0736e", "title": "Do Transformers Need Deep Long-Range Memory?", "authors": [{"authorId": "34269227", "name": "Jack W. Rae", "paperCount": 25, "citationCount": 2557, "hIndex": 20}, {"authorId": "143653164", "name": "Ali Razavi", "paperCount": 14, "citationCount": 2570, "hIndex": 10}], "abstract": "Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL \u2014 a Transformer augmented with a long-range memory of past activations \u2014 has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 24, "citation_count": 19, "influential_paper_citations": 1, "is_open_access": true, "citations": ["7e9ff94476f41041c75e253e84f487db00e9c861", "657329c633709dd1ac34a30d57341b186b1a47c2", "f75d05e759447c2aedb7097728f29f9a520d9bc1", "059bc26845de0edc264bf0ace17dfc4ff5ad5532", "1bed382373aed687c045bb65bc7541b16fc7a6be", "64da659c0687762359226b4cf455520c78acd165", "87c5b281fa43e6f27191b20a8dd694eda1126336", "b51fa25bf1ce7fd18737a36b72e80f8e5808973e", "29168348f4729d418df5acc8a5fce4f1c428a7e3", "35e31785d64574f6f5a89be81ae934c616d3753a"], "references": ["657329c633709dd1ac34a30d57341b186b1a47c2", "8890eeda67d02117a589b0ba41c69419c40c7d5e", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "f51497f463566581874c941353dd9d80069c5b77", "f47374c67ae0d45454feb2ba354d05f0da2889d7", "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "bf442ab269074665a68e4dbbe19e4efc97862541", "f4238bd2385a52413ccbacfd9e409a650235bd13", "ef523bb9437178c50d1b1e3e3ca5fb230ab37e3f", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "9405cc0d6169988371b2755e573cc28650d14dfe", "69ac3b35887eb42e8fe554619fc7255e6e95a4cb", "f0b6c1ffed9984317050d0c1dfb005cb65582f13", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "58eb3a2f0a67acf2f5c7c2cb4a22852b65314eb5", "efbd381493bb9636f489b965a2034d529cd56bcd", "784ee73d5363c711118f784428d1ab89f019daa5", "77fb0b7aef619dfac650423d4677170df2158e0d", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "9819b600a828a57e1cde047bbe710d3446b30da5", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/3df83a60f55c64b40e6dbcd99cf9f67894a0736e"}, "70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328": {"id": "70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328", "title": "Sparse GPU Kernels for Deep Learning", "authors": [{"authorId": "2066558041", "name": "Trevor Gale", "paperCount": 8, "citationCount": 1349, "hIndex": 5}, {"authorId": "143834867", "name": "M. Zaharia", "paperCount": 241, "citationCount": 50764, "hIndex": 64}, {"authorId": "39660914", "name": "C. Young", "paperCount": 77, "citationCount": 12886, "hIndex": 29}, {"authorId": "152585800", "name": "Erich Elsen", "paperCount": 33, "citationCount": 9042, "hIndex": 23}], "abstract": "Scientific workloads have traditionally exploited high levels of sparsity to accelerate computation and reduce memory requirements. While deep neural networks can be made sparse, achieving practical speedups on GPUs is difficult because these applications have relatively moderate levels of sparsity that are not sufficient for existing sparse kernels to outperform their dense counterparts. In this work, we study sparse matrices from deep learning applications and identify favorable properties that can be exploited to accelerate computation. Based on these insights, we develop high-performance GPU kernels for two sparse matrix operations widely applicable in neural networks: sparse matrix\u2013dense matrix multiplication and sampled dense\u2013 dense matrix multiplication. Our kernels reach 27% of single-precision peak on Nvidia V100 GPUs. Using our kernels, we demonstrate sparse Transformer and MobileNet models that achieve $1.2-2.1 \\times $ speedups and up to $12.8 \\times $ memory savings without sacrificing accuracy.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 56, "citation_count": 93, "influential_paper_citations": 13, "is_open_access": true, "citations": ["fdacf2a732f55befdc410ea927091cad3b791f13", "657329c633709dd1ac34a30d57341b186b1a47c2", "9227d5897abbf297a34d447e94a802a714b8eab2", "a85ba5bb3e97c999f5f6dbc78f277b107af1dba2", "b4d207a2096aee4a3764933373eef6edb574c952", "fbb80084e253ad12fa2085eabec88f3963561254", "583f353972ed917772f3f1fc62f4b7cadc8f1e81", "66d735987a31d666a6459566ae026c40ab9a1c3a", "5679ff44de6c462c6320ab497f80860d2faa14e8", "edb7a4e369d9d73d876d66b8a4a5c6b3497fbda8"], "references": ["657329c633709dd1ac34a30d57341b186b1a47c2", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "db2d3dc613169b519f1a2dd35e0473dc2e848025", "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "83d074cc5051ade0c08d66180e4a04d2c112fa97", "21da617a0f79aabf94272107184606cefe90ab75", "26384278cf5d575fc32cb92c303fb648fa0d5217", "313c6593301596d3786fd9e40569c020f09bd2c8", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "9dad5f3b491fba9496c90bb7cfe10d1c0f00fadc", "a5d2f8fe7d0c02f2f42db15f122837cab40e8604", "dbb774a68de152d9b0f80d5eb4744d2995425b5c", "03cc06909497ce5c0cb231f8f6e2bf943cf74917", "f2c882fd290d616ff96c1c5d6af4578682e26556", "1db9bd18681b96473f3c82b21edc9240b44dc329", "eedaa9eb9c6a8228c79caf560614a431736b62d5", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "572f5d18a3943dce4e14f937ef66977a01891096", "2e10560579f2bdeae0143141f26bd9f0a195b4b7", "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "56257b0804c9c2418b32337d3af0970f7b67b084", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "0a5265d5f4a2b59bde18c258ad5acd26bc680769", "3647d6d0f151dc05626449ee09cc7bce55be497e", "6584a24413a456b849a54cf4e6ce6b2ef78f4477", "34cc3ceae5c3f7c8acbb89f2bff63f9d452b00d5", "fe9bc9944984cbc6f9f12e99c1065a47d662e24c", "af2f3729c333e2d0571751afcced589692c167f4", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "d5eadd6f059d742d76441fd0a635a21694dd7392", "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "d6070e0a000df32191dab29f7971139ded11ed65", "4d376d6978dad0374edfa6709c9556b42d3594d3", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "31c36d445367ba204244bb74893c5654e31c3869", "0b544dfe355a5070b60986319a3f51fb45d1348e", "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "8d0da80d6fb01692aa2c084930e3b18048c885d0", "041b00098d4b6749309ae51da14d56cf6a48e309", "bbb1e4766fb186be61a73d8842f6b003aecda413", "e464550cdc5bfe80900afb8df94066c7ddb6b14d", "40457623fcdbaa253e2894c2f114837fde1c11e5", "0c0800259bd40b1ac96cc437629c5ea0ad729f22", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "e7297db245c3feb1897720b173a59fe7e36babb7", "d88466620ac58c55af06b49153ebc9fdb21bc0ef"], "url": "https://www.semanticscholar.org/paper/70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328"}, "055fd6a9f7293269f1b22c1470e63bd02d8d9500": {"id": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer", "authors": [{"authorId": "143808231", "name": "Nikita Kitaev", "paperCount": 16, "citationCount": 1988, "hIndex": 12}, {"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "6639036", "name": "Anselm Levskaya", "paperCount": 14, "citationCount": 4368, "hIndex": 11}], "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 24, "citation_count": 980, "influential_paper_citations": 157, "is_open_access": false, "citations": ["71b6394ad5654f5cd0fba763768ba4e523f7bbca", "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "1fa9ed2bea208511ae698a967875e943049f16b6", "05f5f8b2065a520846d89771ebaea2bb1534e9c6", "af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "fdacf2a732f55befdc410ea927091cad3b791f13", "35a9749df07a2ab97c51af4d260b095b00da7676", "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87"], "references": ["bf442ab269074665a68e4dbbe19e4efc97862541", "830995ef17cc291c13f42dfd9f462137de1d2179", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "f4238bd2385a52413ccbacfd9e409a650235bd13", "0b5d7a79205b44952e24025ce5d46e9f3aa401a1", "21da617a0f79aabf94272107184606cefe90ab75", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "fb507ada871d1e8c29e376dbf7b7879689aa89f9", "9405cc0d6169988371b2755e573cc28650d14dfe", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb", "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "7570afa31c68e24fce1342b7d67c591787219bc1", "3a6d4cd0768ae8768e733280d362bdb4d25924e7", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "be8c6c69f3e357bfad2987e45b62cff7e7474378", "c17b6f2d9614878e3f860c187f72a18ffb5aabb6", "bbd0e204f48a45735e1065c8b90b298077b73192", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "50645e3dc912d597e89d59bffb96ccc0f8e1aefa", "6e565308c8081e807709cb4a917443b737e6cdb4", "71ae756c75ac89e2d731c9c79649562b5768ff39"], "url": "https://www.semanticscholar.org/paper/055fd6a9f7293269f1b22c1470e63bd02d8d9500"}, "f51497f463566581874c941353dd9d80069c5b77": {"id": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": [{"authorId": "34269227", "name": "Jack W. Rae", "paperCount": 25, "citationCount": 2557, "hIndex": 20}, {"authorId": "13759734", "name": "Anna Potapenko", "paperCount": 19, "citationCount": 7571, "hIndex": 12}, {"authorId": "35880964", "name": "Siddhant M. Jayakumar", "paperCount": 17, "citationCount": 1211, "hIndex": 12}, {"authorId": "2542999", "name": "T. Lillicrap", "paperCount": 133, "citationCount": 60273, "hIndex": 60}], "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 47, "citation_count": 212, "influential_paper_citations": 36, "is_open_access": false, "citations": ["71b6394ad5654f5cd0fba763768ba4e523f7bbca", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "35a9749df07a2ab97c51af4d260b095b00da7676", "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "9b539d413393047b28bb7be9b195f142aaf7a80e", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "800cfb3d23115cdcd4d114234b65bbdf2080f798", "657329c633709dd1ac34a30d57341b186b1a47c2", "011a4019aa0d0ce3edfa56bb2ca1e7586eb43fb2"], "references": ["cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "bf442ab269074665a68e4dbbe19e4efc97862541", "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "f4238bd2385a52413ccbacfd9e409a650235bd13", "21da617a0f79aabf94272107184606cefe90ab75", "cd63025532a62fa245a02ec05e32ac4d23089631", "4b344351fe43544317efc9adaebe6791c4242814", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "a14af711aaa3ae83eb64d1f517b024b8c3094a8a", "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "fd4ae71916cf400bfd1490f275e91b154eb69160", "35ed258aede3df17ee20a6635364cb5fd2461049", "69ac3b35887eb42e8fe554619fc7255e6e95a4cb", "3bb63fdb4670745f8c97d8cad1a8a9603b1c16f5", "80196cdfcd0c6ce2953bf65a7f019971e2026386", "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a", "f6cbf83e1ce3b099d656d2346b261d5ef7f2b62e", "90e06703a776d4d482f8bbc04f31d816ee02ca8b", "422cbcd3bfa663cf8394aa8f71be3ede31bb3280", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "88caa4a0253a8b0076176745ebc072864eab66e1", "2d876ed1dd2c58058d7197b734a8e4d349b8f231", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "efbd381493bb9636f489b965a2034d529cd56bcd", "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "65eee67dee969fdf8b44c87c560d66ad4d78e233", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "98445f4172659ec5e891e031d8202c102135c644", "784ee73d5363c711118f784428d1ab89f019daa5", "be8c6c69f3e357bfad2987e45b62cff7e7474378", "df0402517a7338ae28bc54acaac400de6b456a46", "77fb0b7aef619dfac650423d4677170df2158e0d", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "0e6824e137847be0599bb0032e37042ed2ef5045", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "c3823aacea60bc1f2cabb9283144690a3d015db5", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "9819b600a828a57e1cde047bbe710d3446b30da5", "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "052b1d8ce63b07fec3de9dbb583772d860b7c769"], "url": "https://www.semanticscholar.org/paper/f51497f463566581874c941353dd9d80069c5b77"}, "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1": {"id": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration", "authors": [{"authorId": "14487640", "name": "Ari Holtzman", "paperCount": 30, "citationCount": 3522, "hIndex": 19}, {"authorId": "144685020", "name": "Jan Buys", "paperCount": 29, "citationCount": 1752, "hIndex": 11}, {"authorId": "39191185", "name": "Maxwell Forbes", "paperCount": 25, "citationCount": 1995, "hIndex": 15}, {"authorId": "1699545", "name": "Yejin Choi", "paperCount": 252, "citationCount": 22649, "hIndex": 71}], "abstract": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. \nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 43, "citation_count": 1210, "influential_paper_citations": 343, "is_open_access": false, "citations": ["47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "9b539d413393047b28bb7be9b195f142aaf7a80e", "56fa0b9cba4d9aee5ccc327365b3b3a721031c69", "a3b42a83669998f65df60d7c065a70d07ca95e99", "07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6", "394be105b87e9bfe72c20efe6338de10604e1a11", "62d1a3137b01a69443bebf4d92c1990ec512a6a1", "13a0d8bb38f739990c8cd65a44061c6534f17221", "657329c633709dd1ac34a30d57341b186b1a47c2"], "references": ["53a77e8f73f2ca422d6e38fa9ecc490231ac044c", "a9b80b3cffb758bea670220fa6762eb343865419", "d3231772937a2182b2377d028417245c49868dd1", "071a1d2898aa3a5628438f9f68a21a6cac4f7148", "051091195626920ca8ce91ee72d68ec37110eeec", "2c253dc6d35df2bc5932fecdfa9169a8e663dc31", "9405cc0d6169988371b2755e573cc28650d14dfe", "c334c9c8c4854459f8b164a80150253897b90cce", "a7822238f5db7d62731eaeabf9725a65f4edf893", "2001dd627fc2634ca12c240bfe4022ca337dffdd", "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6", "6db2b93a2d4007371030644173f1001c959214d2", "1b1e3f7218f1c0f0db56bf2bd9475521454693a1", "39d56e05caa642bdb292832fa5a01c5c597a0203", "86be5c90c4128ec59b1c320a16996bb5de68624e", "31b26b31f28988ebcfe7ff356e7fda7e17f1558c", "5bb8c2a054bb98aef95c108b0a29ea078d53c65e", "f9de0d4a5adefc59bfb033f162d8a4a5212882cf", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "fb39923f6a4cdb486da5b579c5f8e2c500f36a35", "13395213d47f78672ab4e81573f2b0fa0cfc8c6d", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627", "66721162f712690bb10928132d402d9bd4460c1b", "176f1d608b918eec8dc4b75e7b6e0acaba84a447", "88caa4a0253a8b0076176745ebc072864eab66e1", "d19b712f90cde698cc96ebd5fe291b410e3f0f9c", "2966ecd82505ecd55ead0e6a327a304c8f9868e3", "e235bc8ccbe85de40f406d1a1201d50aec893b2d", "1298dae5751fb06184f6b067d1503bde8037bdb7", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "651e5bcc14f14605a879303e97572a27ea8c7956", "93499a7c7f699b6630a86fad964536f9423bb6d0", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "9ee949dc55a12849afb60c2d883e0bbc11d781df", "f9a1b3850dfd837793743565a8af95973d395a4e", "ed724d3cb163f9090ea93d35c0733dda9708cf7c", "b25e5bca74d74abb1687315fa3c637bb9911554d", "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657"], "url": "https://www.semanticscholar.org/paper/cf4aa38ae31b43fd07abe13b4ffdb265babb7be1"}, "f6390beca54411b06f3bde424fb983a451789733": {"id": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers", "authors": [{"authorId": "146783606", "name": "Gon\u00e7alo M. Correia", "paperCount": 5, "citationCount": 204, "hIndex": 4}, {"authorId": "2114966", "name": "Vlad Niculae", "paperCount": 59, "citationCount": 2847, "hIndex": 17}, {"authorId": "145644643", "name": "Andr\u00e9 F. T. Martins", "paperCount": 123, "citationCount": 4454, "hIndex": 33}], "abstract": "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter \u2013 which controls the shape and sparsity of alpha-entmax \u2013 allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 40, "citation_count": 145, "influential_paper_citations": 20, "is_open_access": true, "citations": ["fdacf2a732f55befdc410ea927091cad3b791f13", "bd20069f5cac3e63083ecf6479abc1799db33ce0", "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "657329c633709dd1ac34a30d57341b186b1a47c2", "94f94e8892261d0377159379ca5a166ceae19a14", "56676aef356ebb13cba77fc9e4d70760fbc151f5", "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "234763381de73a18f49430b0238310a6853d184e"], "references": ["07a64686ce8e43ac475a8d820a8a9f1d87989583", "f4238bd2385a52413ccbacfd9e409a650235bd13", "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "3cee801d10f410f0feb1a2390776a01ba2765001", "21da617a0f79aabf94272107184606cefe90ab75", "17298b0b53c0b62b737f8c7c086b428f4f3b5057", "8eabed69bbebd83d90c7c27b731ff76edcd6b0a9", "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f", "4b344351fe43544317efc9adaebe6791c4242814", "c9552f9e2a7a7656c4c9ef9569a824dffa1fd181", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "9405cc0d6169988371b2755e573cc28650d14dfe", "49400b3a3ea01772e321e3e010b7b891c3d6cb88", "fdbdd4e0461d23905104460a02a176907d945f44", "e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1", "8b84405fb6e75d41ae35337b86916ca059201824", "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "ad796bf779c8617d1e0d8111913ac3f8eaaf6532", "520ddb38b59b8fae2209ddc7c6640462cf153eec", "03f8754ab20732ebda02ce6e65ec9bfcce17528a", "572f5d18a3943dce4e14f937ef66977a01891096", "94238dead40b12735d79ed63e29ead70730261a2", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "43428880d75b3a14257c3ee9bda054e61eb869c0", "ba6b48ef52e2432a0d6342381e0863fd82a8687b", "0076b232181e4e5be58dce8354a813ad2bbf663a", "cb0ab255c4079e2082ba6e3a807529527d96687c", "1a327709cc53ff9e52454e50a643abf4a0ac92af", "46f281810693e7c4c9741817ce2ebe021cf4be04", "c1e3a26fb88c6720f4e84b7118e6f2df7dc8efa3", "1af68821518f03568f913ab03fc02080247a27ff", "93499a7c7f699b6630a86fad964536f9423bb6d0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "7de66a09cd23f05859a95fa55616b515acab71e9", "d7da009f457917aa381619facfa5ffae9329a6e9", "b20c0758a38bd5a4083f64eff53af924499a8e29", "849a2f9fff249cb5c7356aa6b8f4a7c43ce74746", "0054d6bb763b29b6f21ee6aabf7cf39bfa13682b"], "url": "https://www.semanticscholar.org/paper/f6390beca54411b06f3bde424fb983a451789733"}, "bf442ab269074665a68e4dbbe19e4efc97862541": {"id": "bf442ab269074665a68e4dbbe19e4efc97862541", "title": "Large Memory Layers with Product Keys", "authors": [{"authorId": "1830914", "name": "Guillaume Lample", "paperCount": 33, "citationCount": 11260, "hIndex": 21}, {"authorId": "3469062", "name": "Alexandre Sablayrolles", "paperCount": 23, "citationCount": 2947, "hIndex": 13}, {"authorId": "1706809", "name": "M. Ranzato", "paperCount": 98, "citationCount": 33437, "hIndex": 54}, {"authorId": "8905591", "name": "Ludovic Denoyer", "paperCount": 135, "citationCount": 6067, "hIndex": 31}, {"authorId": "1681054", "name": "H. J\u00e9gou", "paperCount": 161, "citationCount": 20564, "hIndex": 51}], "abstract": "This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 54, "citation_count": 85, "influential_paper_citations": 10, "is_open_access": false, "citations": ["055fd6a9f7293269f1b22c1470e63bd02d8d9500", "75acc731bdd2b626edc74672a30da3bc51010ae8", "fdacf2a732f55befdc410ea927091cad3b791f13", "832fff14d2ed50eb7969c4c4b976c35776548f56", "6f68e1bb253925d8431588555d3010419f322e04", "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "19adf1af8daa9551328226fc6c0140e955bf5689", "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "f51497f463566581874c941353dd9d80069c5b77"], "references": ["2cbb8de53759e75411bc528518947a3094fbce3a", "43f2ad297941db230c089ba353efc3f281ab678c", "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "c18663fea10c8a303d045fd2c1f33cacf9b73ca3", "2d636429422559e9cac2c1db33574057cc11b9b4", "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "06ee9741730e4a2c7c8cdf643f5f34bc497a0b7c", "9405cc0d6169988371b2755e573cc28650d14dfe", "0f885fd46064d271d4404cf9bb3d758e1a6f8d55", "93b8da28d006415866bf48f9a6e06b5242129195", "14f60d36c1d2263d2898a95dd3be09609a8c66c6", "2ed4ebe1878fd8e421f24e1aac76fbdc89e9d381", "b36a5bb1707bb9c70025294b3a310138aae8327a", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "e9c9da57bbf9a968489cb90ec7252319bcab42fb", "510e26733aaff585d65701b9f1be7ca9d5afc586", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "be8c6c69f3e357bfad2987e45b62cff7e7474378", "b649a98ce77ece8cd7638bb74ab77d22d9be77e7", "123ae35aa7d6838c817072032ce5615bb891652d", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "1af68821518f03568f913ab03fc02080247a27ff", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "a5733ff08daff727af834345b9cfff1d0aa109ec", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "8250ecbaef057bdb5390ef4e4be798f1523a23f6", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "71ae756c75ac89e2d731c9c79649562b5768ff39", "4064696e69b0268003879c0bcae6527d3b786b85", "8829e3873846c6bbad5aca111e64f9d2c1b24299", "cea967b59209c6be22829699f05b8b1ac4dc092d", "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "a85e512d8845bd007b0866b4a97e8341463f8190", "b94043a133e3d07ed0b1cfc036829e619ea0ba22", "44ddac48353ead135eef4096859956eaa31be2a5", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "5b81a5c1394e227cfff8406c0c4e476fd3cc4852", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "4748d22348e72e6e06c2476486afddbc76e5eca7", "c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "99e8d34817ae10d7304521e89c5fbf908b9d856b", "2805537bec87a6177037b18f9a3a9d3f1038867b"], "url": "https://www.semanticscholar.org/paper/bf442ab269074665a68e4dbbe19e4efc97862541"}, "7d67237398986a6088c696df0bf57646c714508f": {"id": "7d67237398986a6088c696df0bf57646c714508f", "title": "Look Harder: A Neural Machine Translation Model with Hard Attention", "authors": [{"authorId": "51126440", "name": "S. Indurthi", "paperCount": 11, "citationCount": 116, "hIndex": 7}, {"authorId": "2064829685", "name": "Insoo Chung", "paperCount": 5, "citationCount": 67, "hIndex": 4}, {"authorId": "2118808548", "name": "Sangha Kim", "paperCount": 16, "citationCount": 109, "hIndex": 6}], "abstract": "Soft-attention based Neural Machine Translation (NMT) models have achieved promising results on several translation tasks. These models attend all the words in the source sequence for each target token, which makes them ineffective for long sequence translation. In this work, we propose a hard-attention based NMT model which selects a subset of source tokens for each target token to effectively handle long sequence translation. Due to the discrete nature of the hard-attention mechanism, we design a reinforcement learning algorithm coupled with reward shaping strategy to efficiently train it. Experimental results show that the proposed model performs better on long sequences and thereby achieves significant BLEU score improvement on English-German (EN-DE) and English-French (ENFR) translation tasks compared to the soft attention based NMT.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 23, "citation_count": 11, "influential_paper_citations": 0, "is_open_access": true, "citations": ["657329c633709dd1ac34a30d57341b186b1a47c2", "57f123c95ecf9d901be3a53291f53302740451e2", "66d5ebdaa712698a04a0d28b691b075471ca66bf", "ee5954bd69b624b2f6ae51b46607878993f2c4b3", "d6afd14cb0a88e811ae7b744e69e475320f28a56", "aa755de9c9b284a9f0a651b81099b5b6f2ee7659", "fbe7edb7c7bc8e6aa32450e5ebf54cac621750aa", "6f3797d3e88b5ba957ffef70621f74ebd2f4df9c", "c2b7dd6e58d8488832bece9ef4cbc1a117b01c84", "411585e950f6f3bf836e9c2c3f97be0a13e0368e"], "references": ["1af138dc72fa855cc3bc9c0b83750b461c26e29d", "f54b36edae733ab9cd7a748595947710bd28a2e3", "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb", "d7dded37a976ebc05103a4f3785969005c37af5b", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "59562be2cf8e01e8b7bb7560cef56158ea171227", "5d727286a59d7e2681b6fac5fa269e782849f007", "3fc5ed18c2294596af072df929c8ee12c71f96a2", "4216ee11823aba41ad4c2adbe50f765e86a8a04b", "fe5947422173fcda9377570587ff3d13245eef18", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "43428880d75b3a14257c3ee9bda054e61eb869c0", "c25a67ad7e8629a9d12b9e2fc356cd73af99a060", "5256b6d0ffe5b0dbcd979e2a8404326732b5ed51", "0d24a0695c9fc669e643bad51d4e14f056329dec", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "467d5d8fc766e73bfd3e9415f75479823f92c2f7", "93499a7c7f699b6630a86fad964536f9423bb6d0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "8a756d4d25511d92a45d0f4545fa819de993851d", "d7da009f457917aa381619facfa5ffae9329a6e9", "94066dc12fe31e96af7557838159bde598cb4f10"], "url": "https://www.semanticscholar.org/paper/7d67237398986a6088c696df0bf57646c714508f"}, "e0c6abdbdecf04ffac65c440da77fb9d66bb474c": {"id": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "authors": [{"authorId": "2109512754", "name": "Zhilin Yang", "paperCount": 53, "citationCount": 13117, "hIndex": 27}, {"authorId": "3422912", "name": "Zihang Dai", "paperCount": 38, "citationCount": 11660, "hIndex": 23}, {"authorId": "35729970", "name": "Yiming Yang", "paperCount": 319, "citationCount": 40548, "hIndex": 62}, {"authorId": "143712374", "name": "J. Carbonell", "paperCount": 531, "citationCount": 32730, "hIndex": 74}, {"authorId": "145124475", "name": "R. Salakhutdinov", "paperCount": 332, "citationCount": 123720, "hIndex": 101}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}], "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 44, "citation_count": 5141, "influential_paper_citations": 813, "is_open_access": false, "citations": ["8f2bca9d684005675e294b33c26481e36f528cdb", "370b680057a6e324e67576a6bf1bf580af9fdd74", "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "8bef2ad7d969f0e37d9c8b78d9c8d2997838f2cf", "b3848d32f7294ec708627897833c4097eb4d8778", "0e8c3f15c210909a361ba3378d6fe2822ae4f93e", "2fe2f849b94cf08b559226bc9d78adcaef5ef186", "05bcf9999525656cfaa59bc71f8572d771ff3776", "b161c4aaddd2983a9d4d5a240bd5ffa84b36c4e7", "f40aeae3e522ada1f6a9f326841b01ef5c8657b6"], "references": ["7a064df1aeada7e69e5173f7d4c8606f4470365b", "8c473a8adca5635c3cde5af793ed7b68afec9d77", "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "c3f89364aecd661eb032840d2fe3efd0f6d1698c", "ef6948edae12eba6f1d486b8600108b9762f36ab", "c57298fe3faf87f9f24414821b0df7ebb7634320", "3562fefb64cd63ac1a6a0adbaa83ae73dd674243", "df06490403fb4cb3d097160d09093e13777e8362", "6642ad0b2fd2bf834388b804250eb9337ceb3f88", "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "18bc1d4271abe8dd6e16179cdb06524a4f396e16", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "b5246fa284f86b544a7c31f050b3bd0defd053fd", "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "93b8da28d006415866bf48f9a6e06b5242129195", "3febb2bed8865945e7fddc99efd791887bb7e14f", "fc3384d631f5e2b2a9d66623d4d3e1d28b96dee7", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "ef9ddbc35676ce8ffc2a8067044473727839dbac", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "718e1b453fe9dce79458e0db035091db603775fb", "318e25144389e7ad26ea6350ab2243b2b304a35b", "ea738439b880ad033ff01602ea52d04b366d0d37", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "636a79420d838eabe4af7fb25d6437de45ab64e8", "3eda43078ae1f4741f09be08c4ecab6229046a5c", "2cd55ded95d5d13430edfa223ba591b514ebe8a5", "d51ed05fd05b9d222427a05a87ed88217447b44f", "05dd7254b632376973f3a1b4d39485da17814df5", "93d8d45fe8101545ae6d9fab3dbb38f904ff7b4e", "3cccc93064dae265eeb7bc76d02cdc67c942f0a5", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "0e6824e137847be0599bb0032e37042ed2ef5045", "90f72fbbe5f0a29e627db28999e01a30a9655bc6", "190e4800c67ef445e4bd0944a55debaccebcf43f"], "url": "https://www.semanticscholar.org/paper/e0c6abdbdecf04ffac65c440da77fb9d66bb474c"}, "f4238bd2385a52413ccbacfd9e409a650235bd13": {"id": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers", "authors": [{"authorId": "2265067", "name": "Sainbayar Sukhbaatar", "paperCount": 30, "citationCount": 4950, "hIndex": 16}, {"authorId": "3024698", "name": "Edouard Grave", "paperCount": 71, "citationCount": 21226, "hIndex": 38}, {"authorId": "2329288", "name": "Piotr Bojanowski", "paperCount": 55, "citationCount": 21435, "hIndex": 30}, {"authorId": "2319608", "name": "Armand Joulin", "paperCount": 96, "citationCount": 27135, "hIndex": 51}], "abstract": "We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 11, "citation_count": 189, "influential_paper_citations": 14, "is_open_access": true, "citations": ["71b6394ad5654f5cd0fba763768ba4e523f7bbca", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "fdacf2a732f55befdc410ea927091cad3b791f13", "6f68e1bb253925d8431588555d3010419f322e04", "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "f51497f463566581874c941353dd9d80069c5b77", "610b302950a19acef1c45456111dcd495f638c18"], "references": ["c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "f902ce46b2e1307fe4d0895e2a99ad8399f47d40", "6746a18b2820f757334f75bc95428b3ea58d6603", "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "93499a7c7f699b6630a86fad964536f9423bb6d0", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5"], "url": "https://www.semanticscholar.org/paper/f4238bd2385a52413ccbacfd9e409a650235bd13"}, "21da617a0f79aabf94272107184606cefe90ab75": {"id": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers", "authors": [{"authorId": "48422824", "name": "Rewon Child", "paperCount": 13, "citationCount": 17787, "hIndex": 11}, {"authorId": "145565184", "name": "Scott Gray", "paperCount": 12, "citationCount": 11670, "hIndex": 9}, {"authorId": "38909097", "name": "Alec Radford", "paperCount": 34, "citationCount": 52296, "hIndex": 24}, {"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}], "abstract": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 30, "citation_count": 835, "influential_paper_citations": 97, "is_open_access": false, "citations": ["268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "7a064df1aeada7e69e5173f7d4c8606f4470365b", "289db3be7bf77e06e75541ba93269de3d604ac72", "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "2def61f556f9a5576ace08911496b7c7e4f970a4", "05f5f8b2065a520846d89771ebaea2bb1534e9c6", "47f7ec3d0a5e6e83b6768ece35206a94dc81919c"], "references": ["7d3ab2a839b077a318022f7842225db55033b2c3", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "a7822238f5db7d62731eaeabf9725a65f4edf893", "8ba5f5106cd039a22f0d7fdf97d700e427dde282", "2da824e19bd49a2e37739421fa003818c413946f", "21b786b3f870fc7fa247c143aa41de88b1fc6141", "32bfa6418f4908f3c5eff08d76bcf33a90ce2a9c", "1db9bd18681b96473f3c82b21edc9240b44dc329", "7570afa31c68e24fce1342b7d67c591787219bc1", "d1c424c261c577958917055f72fb9e2ad0348865", "f0afdccf2903039d202085a771953a171dfd57b1", "2e10560579f2bdeae0143141f26bd9f0a195b4b7", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "87fc28cbb193a3bc100e13a4a57a8dc9ce7e31a3", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "43428880d75b3a14257c3ee9bda054e61eb869c0", "2d1b8f60f2724efd6c9344870fb60e8525157d70", "66386a946a04534275bd466862364d139790f41f", "e221e2c2ca8bd74a7b818406c8a2a342760e7d65", "df0402517a7338ae28bc54acaac400de6b456a46", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7", "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "f61e9fd5a4878e1493f7a6b03774a61c17b7e9a4", "942deb7d865b7782c03176d95e3a0d56cb71009e", "77f0a39b8e02686fd85b01971f8feb7f60971f80", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "5522764282c85aea422f1c4dc92ff7e0ca6987bc"], "url": "https://www.semanticscholar.org/paper/21da617a0f79aabf94272107184606cefe90ab75"}, "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac": {"id": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "authors": [{"authorId": "46522098", "name": "Xiaodong Liu", "paperCount": 331, "citationCount": 6385, "hIndex": 29}, {"authorId": "50462546", "name": "Pengcheng He", "paperCount": 35, "citationCount": 3347, "hIndex": 14}, {"authorId": "2109136147", "name": "Weizhu Chen", "paperCount": 115, "citationCount": 6109, "hIndex": 33}, {"authorId": "1800422", "name": "Jianfeng Gao", "paperCount": 394, "citationCount": 40273, "hIndex": 86}], "abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 40, "citation_count": 828, "influential_paper_citations": 195, "is_open_access": true, "citations": ["077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "81f5810fbbab9b7203b9556f4ce3c741875407bc", "d9f6ada77448664b71128bb19df15765336974a6", "05f5f8b2065a520846d89771ebaea2bb1534e9c6", "818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57", "1c71771c701aadfd72c5866170a9f5d71464bb88", "a33a06ddc762fb855b6954c08d5aca603080b011"], "references": ["1c71771c701aadfd72c5866170a9f5d71464bb88", "7ebed46b7f3ec913e508e6468304fcaea832eda1", "86f00f3619626bf3aa9664b17bcaebc18a4b6531", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "b5388cfc06688fe3c6937c65025442fdf9a1e6b9", "ebf7b916cf84f9e43d5948395a48e18688c5464d", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "b47381e04739ea3f392ba6c8faaf64105493c196", "8527e946ad292088db8b8e6084384a82299633fe", "83e567c2822aeda91006096a5d7ac0b34721d2a5", "f95f5e43f34e1bfb425b6491fc09558c44d2973d", "413a03a146e6f7b16c11e73243d83e6f1a6627a3", "cf8c493079702ec420ab4fc9c0fabb56b2a16c84", "6084b58d8b4b0caf3a2a7f3a1bee1cc527927e39", "93b8da28d006415866bf48f9a6e06b5242129195", "3febb2bed8865945e7fddc99efd791887bb7e14f", "8490431f3a76fbd165d108eba938ead212a2a639", "7ad66cba3b7e3abae7ef33122588512a146f7f77", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "bb35ef89addbbc28d960bc0cab70d8a29fdf6eee", "83b83ee4f27388445bdebb199cd75e5bf546dd85", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "8ff46c88964a36985f2b45933a3d47b81bd87bd0", "05dd7254b632376973f3a1b4d39485da17814df5", "d76c07211479e233f7c6a6f32d5346c983c5598f", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "c3b8367a80181e28c95630b9b63060d895de08ff", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fdb813d8b927bdd21ae1858cafa6c34b66a36268", "687bac2d3320083eb4530bf18bb8f8f721477600", "128cb6b891aee1b5df099acb48e2efecfcff689f", "bc1022b031dc6c7019696492e8116598097a8c12", "63aaf12163fe9735dfe9a69114937c4fa34f303a", "475354f10798f110d34792b6d88f31d6d5cb099e", "161ffb54a3fdf0715b198bb57bd22f910242eb49", "155345976aa505a10a45e9119f2853df4d7999d7"], "url": "https://www.semanticscholar.org/paper/658721bc13b0fa97366d38c05a96bf0a9f4bb0ac"}, "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6": {"id": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context", "authors": [{"authorId": "3422912", "name": "Zihang Dai", "paperCount": 38, "citationCount": 11660, "hIndex": 23}, {"authorId": "2109512754", "name": "Zhilin Yang", "paperCount": 53, "citationCount": 13117, "hIndex": 27}, {"authorId": "35729970", "name": "Yiming Yang", "paperCount": 319, "citationCount": 40548, "hIndex": 62}, {"authorId": "143712374", "name": "J. Carbonell", "paperCount": 531, "citationCount": 32730, "hIndex": 74}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}, {"authorId": "145124475", "name": "R. Salakhutdinov", "paperCount": 332, "citationCount": 123720, "hIndex": 101}], "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 69, "citation_count": 2158, "influential_paper_citations": 303, "is_open_access": true, "citations": ["9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "370b680057a6e324e67576a6bf1bf580af9fdd74", "5b783ba4fa0c8f7b32139cd0f2bffa19dae83d62", "7f631586a368f1762866b01ff9f43c265851d52e", "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "9933a5af7895354087baf6c96b64dc8a8973eaed", "f75cddf2d42ed01b34686704eb3504becef67442", "7d4c2c8407e0caf2f907df9954b056a42a92fd13", "75a060752996ec8b27b791a9084b171bc8b4d777"], "references": ["43f2ad297941db230c089ba353efc3f281ab678c", "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "c1f457e31b611da727f9aef76c283a18157dfa83", "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "2da824e19bd49a2e37739421fa003818c413946f", "fa0beb3f4d7f6e7e49b153af7e8a7c30f2937b60", "7b9256b9fc59404b4cfe8c60b3943f4e38360122", "717892acc8767a028218b5053ebe57a4f59685d1", "fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "69ac3b35887eb42e8fe554619fc7255e6e95a4cb", "680aafd3d51e666b297e27b93d9554cc2caf1c4d", "565ab57eede8bf6ef9c42df51216b9f85287c234", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "921196c32213a229245a9705ee4768bc941e7a26", "3febb2bed8865945e7fddc99efd791887bb7e14f", "608e4bbe7a2d6f04d68b5747d9d0778d5fce47df", "fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "01eb299fec9b9f5f499d0ce9702d5aeb77848d88", "ef9ddbc35676ce8ffc2a8067044473727839dbac", "58c6f890a1ae372958b7decf56132fe258152722", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "87a913817503379547bec61a5f010abac5b0f76b", "79baf48bd560060549998d7b61751286de062e2a", "510e26733aaff585d65701b9f1be7ca9d5afc586", "88caa4a0253a8b0076176745ebc072864eab66e1", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "7ab2166f6cdb1737e000df66d29c6538afc6811d", "67d968c7450878190e45ac7886746de867bf673d", "424aef7340ee618132cc3314669400e23ad910ba", "563783de03452683a9206e85fe6d661714436686", "efbd381493bb9636f489b965a2034d529cd56bcd", "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "65eee67dee969fdf8b44c87c560d66ad4d78e233", "63e39cdf1ad884da6bc69096bb3413b5b1100559", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "952454718139dba3aafc6b3b67c4f514ac3964af", "98445f4172659ec5e891e031d8202c102135c644", "136cf66392f1d6bf42da4cc070888996dc472b91", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "e8e76b1062918624e9904e0073e11794d7594593", "6c5325c2b67bf88f2b846cf5a6df6c2e6362d75b", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "d46b81707786d18499f911b4ab72bb10c65406ba", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "71ae756c75ac89e2d731c9c79649562b5768ff39", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "0dc9eb7d17f2def56ad930945f2521653f04c3fa", "c3823aacea60bc1f2cabb9283144690a3d015db5", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "d1275b2a2ab53013310e759e5c6878b96df643d4", "c5145b1d15fea9340840cc8bb6f0e46e8934827f", "9819b600a828a57e1cde047bbe710d3446b30da5", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "aed054834e2c696807cc8b227ac7a4197196e211", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6"}, "7d3ab2a839b077a318022f7842225db55033b2c3": {"id": "7d3ab2a839b077a318022f7842225db55033b2c3", "title": "Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling", "authors": [{"authorId": "10698483", "name": "Jacob Menick", "paperCount": 15, "citationCount": 2191, "hIndex": 13}, {"authorId": "2583391", "name": "Nal Kalchbrenner", "paperCount": 35, "citationCount": 30929, "hIndex": 25}], "abstract": "The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of sub-images of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models. To address the latter challenge, we propose to use Multidimensional Upscaling to grow an image in both size and depth via intermediate stages utilising distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 14, "citation_count": 116, "influential_paper_citations": 11, "is_open_access": false, "citations": ["289db3be7bf77e06e75541ba93269de3d604ac72", "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "6be216d93421bf19c1659e7721241ae73d483baf", "21da617a0f79aabf94272107184606cefe90ab75", "de18baa4964804cf471d85a5a090498242d2e79f", "9695824d7a01fad57ba9c01d7d76a519d78d65e7", "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "853de0e00ac5ac257a622ae678ed373b8e086404", "3e577c9bdc82cb7fed337a74f90bbc4505fdfb69"], "references": ["21b786b3f870fc7fa247c143aa41de88b1fc6141", "642c1b4a9da95ea4239708afc5929a5007a1870d", "f2c882fd290d616ff96c1c5d6af4578682e26556", "40638a7a9e0a0499af46053c6efc05ce0b088a28", "d1c424c261c577958917055f72fb9e2ad0348865", "744fe47157477235032f7bb3777800f9f2f45e52", "f89e5a8800b318fa03289b5cc67df54b956875b4", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "2d1b8f60f2724efd6c9344870fb60e8525157d70", "db0d33590dc15de2d30cf0407b7a26ae79cd51b5", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "df0402517a7338ae28bc54acaac400de6b456a46", "0936352b78a52bc5d2b5e3f04233efc56664af51"], "url": "https://www.semanticscholar.org/paper/7d3ab2a839b077a318022f7842225db55033b2c3"}, "d170bd486e4c0fe82601e322b0e9e0dde63ab299": {"id": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling", "authors": [{"authorId": "51428394", "name": "Alexei Baevski", "paperCount": 32, "citationCount": 6432, "hIndex": 18}, {"authorId": "2325985", "name": "Michael Auli", "paperCount": 97, "citationCount": 19553, "hIndex": 45}], "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 35, "citation_count": 249, "influential_paper_citations": 36, "is_open_access": false, "citations": ["268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "7a064df1aeada7e69e5173f7d4c8606f4470365b", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "49a049dc85e2380dde80501a984878341dd8efdf", "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "949fef650da4c41afe6049a183b504b3cc91f4bd", "cc27ec53160d88c25fc5096c0df65536eb780de4"], "references": ["b9de9599d7241459db9213b5cdd7059696f5ef8d", "9852ae077c7da6a9d178acaa2b44a335289507a6", "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "69ac3b35887eb42e8fe554619fc7255e6e95a4cb", "680aafd3d51e666b297e27b93d9554cc2caf1c4d", "0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b", "b887a39268ee41fea8d72f0ecd364eb72fe28a82", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "510e26733aaff585d65701b9f1be7ca9d5afc586", "88caa4a0253a8b0076176745ebc072864eab66e1", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "424aef7340ee618132cc3314669400e23ad910ba", "efbd381493bb9636f489b965a2034d529cd56bcd", "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "63e39cdf1ad884da6bc69096bb3413b5b1100559", "b022f2a277a4bf5f42382e86e4380b96340b9e86", "8f67a85a79dfe3617cb7aaaf5400391cb8fad0a1", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "759956bb98689dbcc891528636d8994e54318f85", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "1af68821518f03568f913ab03fc02080247a27ff", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "1b7db8ad49f94da9b90db89bede5f27644bb9911", "dcfade77ecd26c1b21c68021ff482dba6fef8063", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "71480da09af638260801af1db8eff6acb4e1122f", "aa7bfd2304201afbb19971ebde87b17e40242e91", "84069287da0a6b488b8c933f3cb5be759cb6237e", "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "a17745f1d7045636577bcd5d513620df5860e9e5", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "9819b600a828a57e1cde047bbe710d3446b30da5", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "4af41f4d838daa7ca6995aeb4918b61989d1ed80", "6c2b28f9354f667cd5bd07afc0471d8334430da7"], "url": "https://www.semanticscholar.org/paper/d170bd486e4c0fe82601e322b0e9e0dde63ab299"}, "b9de9599d7241459db9213b5cdd7059696f5ef8d": {"id": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention", "authors": [{"authorId": "1388360943", "name": "Rami Al-Rfou", "paperCount": 37, "citationCount": 12578, "hIndex": 22}, {"authorId": "51221461", "name": "Dokook Choe", "paperCount": 2, "citationCount": 265, "hIndex": 2}, {"authorId": "40832517", "name": "Noah Constant", "paperCount": 35, "citationCount": 4453, "hIndex": 20}, {"authorId": "51150315", "name": "Mandy Guo", "paperCount": 16, "citationCount": 849, "hIndex": 12}, {"authorId": "145024664", "name": "Llion Jones", "paperCount": 21, "citationCount": 50175, "hIndex": 15}], "abstract": "LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 53, "citation_count": 257, "influential_paper_citations": 32, "is_open_access": true, "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992", "9405cc0d6169988371b2755e573cc28650d14dfe", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "21da617a0f79aabf94272107184606cefe90ab75"], "references": ["fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "fc7e6502dace26305e3e062e426034f61a18095e", "565ab57eede8bf6ef9c42df51216b9f85287c234", "27981998aaef92952eabef2c1490b926f9150c4f", "69902406e7d08f8865f02185699978db499d25e7", "fa9decd1395cc2f39e9921f870ebc8a8ec2bd08d", "58c6f890a1ae372958b7decf56132fe258152722", "a1e34c955e5a412c71c07c5d9ef03680fd4d5add", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "87a913817503379547bec61a5f010abac5b0f76b", "c8067a25b1e69569b8d3a0bee223299b2839f416", "664ec878de4b7170712baae4a7821fc2602bba25", "58eb3a2f0a67acf2f5c7c2cb4a22852b65314eb5", "510e26733aaff585d65701b9f1be7ca9d5afc586", "88caa4a0253a8b0076176745ebc072864eab66e1", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "67d968c7450878190e45ac7886746de867bf673d", "55cf59bfbb25d6363cab87cb747648ebe8a096e5", "65eee67dee969fdf8b44c87c560d66ad4d78e233", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105", "952454718139dba3aafc6b3b67c4f514ac3964af", "98445f4172659ec5e891e031d8202c102135c644", "ca9d174c70c9102d88f2707bc395d6a384e1de1d", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "136cf66392f1d6bf42da4cc070888996dc472b91", "4db8cd9117254d21c9c828b8ba2aea58e57ee2c4", "84ca430856a92000e90cd728445ca2241c10ddc3", "f6fda11d2b31ad66dd008a65f7e708aa64a27703", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "e9c771197a6564762754e48c1daafb066f449f2e", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "4d376d6978dad0374edfa6709c9556b42d3594d3", "d14c7e5f5cace4c925abc74c88baa474e9f31a28", "71ae756c75ac89e2d731c9c79649562b5768ff39", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "0b544dfe355a5070b60986319a3f51fb45d1348e", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "f9a1b3850dfd837793743565a8af95973d395a4e", "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "9819b600a828a57e1cde047bbe710d3446b30da5", "aed054834e2c696807cc8b227ac7a4197196e211", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "1a3d22599028a05669e884f3eaf19a342e190a87"], "url": "https://www.semanticscholar.org/paper/b9de9599d7241459db9213b5cdd7059696f5ef8d"}, "c9552f9e2a7a7656c4c9ef9569a824dffa1fd181": {"id": "c9552f9e2a7a7656c4c9ef9569a824dffa1fd181", "title": "Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms", "authors": [{"authorId": "27257992", "name": "Mathieu Blondel", "paperCount": 54, "citationCount": 52900, "hIndex": 23}, {"authorId": "145644643", "name": "Andr\u00e9 F. T. Martins", "paperCount": 123, "citationCount": 4454, "hIndex": 33}, {"authorId": "2114966", "name": "Vlad Niculae", "paperCount": 59, "citationCount": 2847, "hIndex": 17}], "abstract": "This paper studies Fenchel-Young losses, a generic way to construct convex loss functions from a regularization function. We analyze their properties in depth, showing that they unify many well-known loss functions and allow to create useful new ones easily. Fenchel-Young losses constructed from a generalized entropy, including the Shannon and Tsallis entropies, induce predictive probability distributions. We formulate conditions for a generalized entropy to yield losses with a separation margin, and probability distributions with sparse support. Finally, we derive efficient algorithms, making Fenchel-Young losses appealing both in theory and practice.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 57, "citation_count": 27, "influential_paper_citations": 3, "is_open_access": false, "citations": ["657329c633709dd1ac34a30d57341b186b1a47c2", "3cee801d10f410f0feb1a2390776a01ba2765001", "f6390beca54411b06f3bde424fb983a451789733", "5eef6a00d9eab08f3071ef19ea3e4b545421e8cb", "3a5f479d15a3300a2fbfb868f80339431b452a5b", "bd575b481405fa6d6ada0277c30cb8f58a4e6d44", "55c5fec09097dee09879d52a6a30eedc2db381aa", "3ca8a63b74859c8e74ec905f6f655470b8e47b17", "fb0bd71d0921ca53c6df689c687d8cd75bad3a40", "36ffa5b1f643f59ccf8396cff9865e5474c8dae7"], "references": ["3ca8a63b74859c8e74ec905f6f655470b8e47b17", "a72e98969f4779459dd3b7f4356de3dd49996003", "bd575b481405fa6d6ada0277c30cb8f58a4e6d44", "f1fa4fd79ee58a491687a537aa876628d302ff16", "52cb75f5e385280e1c298d62d71ba5100bd81708", "ba6b48ef52e2432a0d6342381e0863fd82a8687b", "c1e3a26fb88c6720f4e84b7118e6f2df7dc8efa3", "d9bdff5eac0d0d1ebd8d09960f195b838ce16f4e", "8babd9c1243af3edc3884a4c491c26e1c3292924", "6c986b56a76a8b61f11615cd03442bb895dbeb6e", "2676540843915141bcbb046a3f89da954f43bc09", "4daec165c1f4aa1206b0d91c0b26f0287d1ef52d", "c1626528298f39c11834a66b34e21f645e46690c", "961eabeaebd7035cd7668c9917fa9c39462e1113", "9f4f30780d9aa3ec21b5fd0c2d669ceb05178c07", "9df16ee1828f8d46cfc6c817dfef540f4c1af51e", "8af74907c5d0b825c3e9419da64903f614aefd7c", "a8697665a7545e5af460a11112afdab7c2332124", "fce0a6e31471195b488caf955ef30e70d96ee56b", "f22513a1c3e6155786fc7912dfb710b71b8627b8", "c41b4357b315e1196f13fd1d966309cc0024326f", "d98d0d1900b13b87aa4ffd6b69c046beb63f0434", "ed7c7c079c8c54d3b82e016cc52a7a2c3a61f237", "64e9a50ebaa20fb34414c3fd0668bfef93e2366c", "1da3495d34bd518f72f5208bb0fe005826a315ce", "6bbd8fc39487249bd1b6886e4dc763550877b758", "4f607f03272e4d62708f5b2441355f9e005cb452", "f312852b31a76bc09b59aca6e4017a9382f9cb39", "8e6c6086ea725737aa6081a57ea68d43a24ca3b9", "d6706b6e626c15680688b0774419662f2341caee", "eb6e94817bc64429adfc786574696d6dea633939", "9761fec71c7c525c75ed27445606fc451560da24", "f69b1f3726603d4e046c15911065159681b8fa64", "2b461250c014b460e7c97b6138a3ee811f198f43", "5a7958b418bceb48a315384568091ab1898b1640", "cfc6d0c8260594ebc5dd20ee558d29b1014ed41a", "f4ba954b0412773d047dc41231c733de0c1f4926", "efe5519409c823737c75e0826ae00efa325f7721", "d5051890e501117097eeffbd8ded87694f0d8063", "385197d4c02593e2823c71e4f90a0993b703620e", "032ca6373e53a9e0691b04eee2f1809538c0e881", "1267fe36b5ece49a9d8f913eb67716a040bbcced", "b20c0758a38bd5a4083f64eff53af924499a8e29", "0363376e26eaa8070fa2f0f158c0aaacee5b4097", "90c05a30cd36269b90d772ca35471f823d4de057", "0763865ef0555d4b24c3802f8110313a3abedb6c", "8da8dcc203430464bf55980c12bf555df052505b", "44a6b76e5cbc61330663d0a9f393caf91a3a1be8", "7878499969cec365c14d129c9ea14a2967653407", "b0d3ae537a926245cfa6686383cd2ec4375d616d", "0a4a3b1efafb09be3f97061dba18eea2116ad4b2", "5d11aad09f65431b5d3cb1d85328743c9e53ba96", "feee6551179612b9691f021b583d8a99b81b9b86", "faa975eaeb6a45031f77d6d7344ac905f74fb962", "aecf46bca84b4854e725f1458f107c435d81ff6e", "d4143c46910f249bedbdc37caf88e4c292124c08"], "url": "https://www.semanticscholar.org/paper/c9552f9e2a7a7656c4c9ef9569a824dffa1fd181"}, "df2b0e26d0599ce3e70df8a9da02e51594e0e992": {"id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": [{"authorId": "39172707", "name": "Jacob Devlin", "paperCount": 42, "citationCount": 48279, "hIndex": 20}, {"authorId": "1744179", "name": "Ming-Wei Chang", "paperCount": 110, "citationCount": 52615, "hIndex": 39}, {"authorId": "2544107", "name": "Kenton Lee", "paperCount": 45, "citationCount": 58597, "hIndex": 29}, {"authorId": "3259253", "name": "Kristina Toutanova", "paperCount": 113, "citationCount": 55918, "hIndex": 45}], "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 59, "citation_count": 43984, "influential_paper_citations": 13932, "is_open_access": false, "citations": ["370b680057a6e324e67576a6bf1bf580af9fdd74", "2b49156cf855dbb39768ae0ba7d7cb9263d17e5c", "220b915c3f86925c7dc05a59e613f53e485ad87d", "44732fe24025c2f17777028bb49bb4a09c31af2a", "1c2e1c73e3f43688e4f22d29ae5a6463721690f9", "1c83f3f9789df43bf937ae2618721e2da83dcc06", "c1ba680d6a8adaa4f19df6f730fda2b344924e8a", "27f5d30a6963de8cfd5004114e57d1e2f6650fe0", "75c4f57e8028efa26c0739253459a2d6c2ff55bf", "e5d5ebbb0e25425621dae26a188146dfe398375a"], "references": ["b9de9599d7241459db9213b5cdd7059696f5ef8d", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "27e98e09cf09bc13c913d01676e5f32624011050", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "421fc2556836a6b441de806d7b393a35b6eaea58", "93b8da28d006415866bf48f9a6e06b5242129195", "3febb2bed8865945e7fddc99efd791887bb7e14f", "8c1b00128e74f1cd92aede3959690615695d5101", "bc1d609520290e0460c49b685675eb5a57fa5935", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "3c78c6df5eb1695b6a399e346dde880af27d1016", "e0222a1ae6874f7fff128c3da8769ab95963da04", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "59761abc736397539bdd01ad7f9d91c8607c0457", "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "05dd7254b632376973f3a1b4d39485da17814df5", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "26e743d5bd465f49b9538deaf116c15e61b7951f", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "0e6824e137847be0599bb0032e37042ed2ef5045", "081651b38ff7533550a3adfc1c00da333a8fe86c", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "687bac2d3320083eb4530bf18bb8f8f721477600", "128cb6b891aee1b5df099acb48e2efecfcff689f", "0f8468de03ee9f12d693237bec87916311bf1c24", "dac72f2c509aee67524d3321f77e97e8eff51de6", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "db8885a0037fe47d973ade79d696586453710233", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "57458bc1cffe5caa45a885af986d70f723f406b4", "843959ffdccf31c6694d135fad07425924f785b1", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "944e1a7b2c5c62e952418d7684e3cade89c76f87", "475354f10798f110d34792b6d88f31d6d5cb099e", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "3de5d40b60742e3dfa86b19e7f660962298492af", "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd"], "url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992"}, "21b786b3f870fc7fa247c143aa41de88b1fc6141": {"id": "21b786b3f870fc7fa247c143aa41de88b1fc6141", "title": "Glow: Generative Flow with Invertible 1x1 Convolutions", "authors": [{"authorId": "1726807", "name": "Diederik P. Kingma", "paperCount": 33, "citationCount": 136875, "hIndex": 27}, {"authorId": "6515819", "name": "Prafulla Dhariwal", "paperCount": 27, "citationCount": 20237, "hIndex": 14}], "abstract": "Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at this https URL", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 29, "citation_count": 1865, "influential_paper_citations": 397, "is_open_access": false, "citations": ["633e2fbfc0b21e959a244100937c5853afca4853", "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "de18baa4964804cf471d85a5a090498242d2e79f", "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "501c02c7caa7fc2c7077405299b4cbe7d294b170", "21549620b6c67a3a9bf10d23e4e16863bf901076", "3e577c9bdc82cb7fed337a74f90bbc4505fdfb69", "370b680057a6e324e67576a6bf1bf580af9fdd74", "88dd6594c9ddd4c4bb7f9b407b162e283907f4f3"], "references": ["1db9bd18681b96473f3c82b21edc9240b44dc329", "f6cbf83e1ce3b099d656d2346b261d5ef7f2b62e", "744fe47157477235032f7bb3777800f9f2f45e52", "e9882bd009c414d4c6d153a5cf340b2d23213d0f", "3a6d4cd0768ae8768e733280d362bdb4d25924e7", "585bf7bea8fa5267738bc465611d6f197e0f87dd", "2d1b8f60f2724efd6c9344870fb60e8525157d70", "6a97d2668187965743d1b825b306defccbabbb4c", "09879f7956dddc2a9328f5c1472feeb8402bcbcf", "df0402517a7338ae28bc54acaac400de6b456a46", "0936352b78a52bc5d2b5e3f04233efc56664af51", "77f0a39b8e02686fd85b01971f8feb7f60971f80", "3d2c6941a9b4608ba52b328369a3352db2092ae0", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "4dcdae25a5e33682953f0853ee4cf7ca93be58a9", "0f899b92b7fb03b609fee887e4b6f3b633eaf30d", "4d376d6978dad0374edfa6709c9556b42d3594d3", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "dc8301b67f98accbb331190dd7bd987952a692af", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "5d90f06bb70a0a3dced62413346235c02b1aa086", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "56f50f070a861923707d1faba7e9144ea99be6e8"], "url": "https://www.semanticscholar.org/paper/21b786b3f870fc7fa247c143aa41de88b1fc6141"}, "520ddb38b59b8fae2209ddc7c6640462cf153eec": {"id": "520ddb38b59b8fae2209ddc7c6640462cf153eec", "title": "Sparse and Constrained Attention for Neural Machine Translation", "authors": [{"authorId": "8805254", "name": "Chaitanya Malaviya", "paperCount": 18, "citationCount": 1555, "hIndex": 12}, {"authorId": "2054692770", "name": "Pedro Ferreira", "paperCount": 6, "citationCount": 56, "hIndex": 2}, {"authorId": "145644643", "name": "Andr\u00e9 F. T. Martins", "paperCount": 123, "citationCount": 4454, "hIndex": 33}], "abstract": "In neural machine translation, words are sometimes dropped from the source or generated repeatedly in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 26, "citation_count": 54, "influential_paper_citations": 5, "is_open_access": true, "citations": ["88e83776313effc1564044d7bf19972981815e3c", "657329c633709dd1ac34a30d57341b186b1a47c2", "17298b0b53c0b62b737f8c7c086b428f4f3b5057", "3cee801d10f410f0feb1a2390776a01ba2765001", "f6390beca54411b06f3bde424fb983a451789733", "6181dce4fa5236c3f6cc322d27bec428e1b9cc46", "3d61a34611c6171f203286119f76ec52f8016580", "6cba18bd36967eefa35a03b8f26c9664bbc11c02", "ab456c1ed181c5c48a34adb61395d4806a0ba949", "c0f709acf38eb27702b0fbce1215db0ebaa2de2b"], "references": ["15e81c8d1c21f9e928c72721ac46d458f3341454", "252571243aa4c0b533aa7fc63f88d07fd844e7bb", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627", "1ada06eb9d7d8f757ccdd3ae6056492c88b62fa9", "aab5002a22b9b4244a8329b140bd0a86021aa2d1", "f958d4921951e394057a1c4ec33bad9a34e5dad1", "31fc1b0fd5ec43863f1a502f6fc3df2cc71b6e6f", "8dde8967c8bf1c97a5614c70beb0eeeaf32d2e7c", "734d0e41828b53a748336884078777cca127dc27", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "f997c2f1f668b942c4cccd425bc192df651ed516", "c1e3a26fb88c6720f4e84b7118e6f2df7dc8efa3", "33108287fbc8d94160787d7b2c7ef249d3ad6437", "1af68821518f03568f913ab03fc02080247a27ff", "93499a7c7f699b6630a86fad964536f9423bb6d0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "26adb749fc5d80502a6d889966e50b31391560d3", "1f9b0dda97b9f787a64ad2b2c006e83b9cdf042f", "7b5e31257f01aba987f16e175a3e49e00a5bd3bb", "ab7b5917515c460b90451e67852171a531671ab8", "416e7bb25c51e352330c6e58f5b06da0c58190d3", "639bb2d08557584d28c19777b0c6732e498e82a5"], "url": "https://www.semanticscholar.org/paper/520ddb38b59b8fae2209ddc7c6640462cf153eec"}, "54a13bcc9613dcaa76fb25fbe96572f376cfcca9": {"id": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost", "authors": [{"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "144872294", "name": "Mitchell Stern", "paperCount": 24, "citationCount": 2351, "hIndex": 14}], "abstract": "In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 13, "citation_count": 400, "influential_paper_citations": 72, "is_open_access": false, "citations": ["3cfb319689f06bf04c2e28399361f414ca32c4b3", "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "75acc731bdd2b626edc74672a30da3bc51010ae8", "9b539d413393047b28bb7be9b195f142aaf7a80e", "d08463bd665589d04619f04dbde84183ffcf2e63", "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "600be3dde18d1059c6b56170bd04ee65ce79a848"], "references": ["0162b26ab4099cbfdf7d6bc6ae4bf5107e3569a3", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "510e26733aaff585d65701b9f1be7ca9d5afc586", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "c1d31d01837136222c5a501ea1cb6fa091d521fe", "84069287da0a6b488b8c933f3cb5be759cb6237e", "8729441d734782c3ed532a7d2d9611b438c0a09a", "413c1142de9d91804d6d11c67ff3fed59c9fc279", "d9211bc2f53b93bc2501aef8d8d929eac60451ad", "29bae9472203546847ec1352a604566d0f602728", "8112c4305b88d85199267e9e03d3a0aca4432059"], "url": "https://www.semanticscholar.org/paper/54a13bcc9613dcaa76fb25fbe96572f376cfcca9"}, "680aafd3d51e666b297e27b93d9554cc2caf1c4d": {"id": "680aafd3d51e666b297e27b93d9554cc2caf1c4d", "title": "An Analysis of Neural Language Modeling at Multiple Scales", "authors": [{"authorId": "3375440", "name": "Stephen Merity", "paperCount": 18, "citationCount": 3449, "hIndex": 9}, {"authorId": "2844898", "name": "N. Keskar", "paperCount": 44, "citationCount": 5814, "hIndex": 20}, {"authorId": "2166511", "name": "R. Socher", "paperCount": 202, "citationCount": 106675, "hIndex": 76}], "abstract": "Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 34, "citation_count": 164, "influential_paper_citations": 26, "is_open_access": false, "citations": ["c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "6cf1d69e447e9687dbd2d92572f44bddbabd8192", "9a618cca0d2fc78db1be1aed70517401cb3f3859", "520ec00dc35475e0554dbb72f27bd2eeb6f4191d", "e5a95a679774e069e1e36d96f92bac6b93027118", "901e5f840daa0609a80232ec363d7c77de25b611", "7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7", "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "657329c633709dd1ac34a30d57341b186b1a47c2"], "references": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "ef9ddbc35676ce8ffc2a8067044473727839dbac", "fa9decd1395cc2f39e9921f870ebc8a8ec2bd08d", "58c6f890a1ae372958b7decf56132fe258152722", "f0b6c1ffed9984317050d0c1dfb005cb65582f13", "fc18e99f918d8906ec44be3f7d90d8f9ebabae96", "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "87a913817503379547bec61a5f010abac5b0f76b", "664ec878de4b7170712baae4a7821fc2602bba25", "510e26733aaff585d65701b9f1be7ca9d5afc586", "88caa4a0253a8b0076176745ebc072864eab66e1", "2d876ed1dd2c58058d7197b734a8e4d349b8f231", "2d7782c225e0fc123d6e227f2cb253e58279ac73", "67d968c7450878190e45ac7886746de867bf673d", "424aef7340ee618132cc3314669400e23ad910ba", "563783de03452683a9206e85fe6d661714436686", "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "65eee67dee969fdf8b44c87c560d66ad4d78e233", "63e39cdf1ad884da6bc69096bb3413b5b1100559", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105", "5b4cef5ce21753a38234740b3d5bf5ff0f7d9b3a", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "08c13be14da51f2ed531ffe980bb993e45042e41", "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025", "9819b600a828a57e1cde047bbe710d3446b30da5", "b3e89f05876d47b9bd6ece225aaeee457a6824e8", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "59ce9cdbde13affc05a6c1f48a51ee7b0fcb154b", "26bc0449360d7016f684eafae5b5d2feded32041", "1a3d22599028a05669e884f3eaf19a342e190a87"], "url": "https://www.semanticscholar.org/paper/680aafd3d51e666b297e27b93d9554cc2caf1c4d"}, "7570afa31c68e24fce1342b7d67c591787219bc1": {"id": "7570afa31c68e24fce1342b7d67c591787219bc1", "title": "Generating Wikipedia by Summarizing Long Sequences", "authors": [{"authorId": "35025299", "name": "Peter J. Liu", "paperCount": 29, "citationCount": 12347, "hIndex": 14}, {"authorId": "144413479", "name": "Mohammad Saleh", "paperCount": 10, "citationCount": 1480, "hIndex": 6}, {"authorId": "38627717", "name": "Etienne Pot", "paperCount": 9, "citationCount": 622, "hIndex": 5}, {"authorId": "2065067542", "name": "Ben Goodrich", "paperCount": 10, "citationCount": 802, "hIndex": 6}, {"authorId": "35474601", "name": "Ryan Sepassi", "paperCount": 8, "citationCount": 2097, "hIndex": 7}, {"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}], "abstract": "We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 21, "citation_count": 538, "influential_paper_citations": 85, "is_open_access": false, "citations": ["9405cc0d6169988371b2755e573cc28650d14dfe", "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6", "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "055fd6a9f7293269f1b22c1470e63bd02d8d9500"], "references": ["032274e57f7d8b456bd255fe76b909b2c1d7458e", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "510e26733aaff585d65701b9f1be7ca9d5afc586", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "832fc9327695f7425d8759c6aaeec0fa2d7b0a90", "05dd7254b632376973f3a1b4d39485da17814df5", "7a67159fc7bc76d0b37930b55005a69b51241635", "604764133befe7a0aaa692919545846197e6e065", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "5082a1a13daea5c7026706738f8528391a1e6d59", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "d2946a868682e4141beabc288d79253ae254c6e1", "ecced0e4b7d1a1751473637ebb8451bcb286acc2", "676b1549adae511164c1b5343f10260fd42035b4", "60b05f32c32519a809f21642ef1eb3eaf3848008", "7b95d389bc6affe6a127d53b04bcfd68138f1a1a", "b3bf6373ff41a115197cb5b30e57830c16130c2c", "eb82d3035849cd23578096462ba419b53198a556"], "url": "https://www.semanticscholar.org/paper/7570afa31c68e24fce1342b7d67c591787219bc1"}, "d1c424c261c577958917055f72fb9e2ad0348865": {"id": "d1c424c261c577958917055f72fb9e2ad0348865", "title": "PixelSNAIL: An Improved Autoregressive Generative Model", "authors": [{"authorId": "41192764", "name": "Xi Chen", "paperCount": 779, "citationCount": 28171, "hIndex": 61}, {"authorId": "3414570", "name": "Nikhil Mishra", "paperCount": 11, "citationCount": 1414, "hIndex": 6}, {"authorId": "22222033", "name": "Mostafa Rohaninejad", "paperCount": 5, "citationCount": 1300, "hIndex": 4}, {"authorId": "1689992", "name": "P. Abbeel", "paperCount": 582, "citationCount": 83411, "hIndex": 130}], "abstract": "Autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions, which offer better access to earlier parts of the sequence than conventional RNNs. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this note, we describe the resulting model and present state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \\times 32$ ImageNet (3.80 bits per dim). Our implementation is available at this https URL", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 38, "citation_count": 174, "influential_paper_citations": 25, "is_open_access": false, "citations": ["289db3be7bf77e06e75541ba93269de3d604ac72", "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "6be216d93421bf19c1659e7721241ae73d483baf", "21da617a0f79aabf94272107184606cefe90ab75", "de18baa4964804cf471d85a5a090498242d2e79f", "853de0e00ac5ac257a622ae678ed373b8e086404", "c8b25a128f4bfd0c79de82c174dd403b2ef6eeb1", "3e577c9bdc82cb7fed337a74f90bbc4505fdfb69", "1db9bd18681b96473f3c82b21edc9240b44dc329"], "references": ["1db9bd18681b96473f3c82b21edc9240b44dc329", "8899094797e82c5c185a0893896320ef77f60e64", "7e9c1e0d247b20a0683f4797d9ea248c3b53d424", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "2d1b8f60f2724efd6c9344870fb60e8525157d70", "66386a946a04534275bd466862364d139790f41f", "2c740e574eea66fdcf473e15ed2c228baef2eccd", "120adede81dc6b3bfcb4cd276f0b73cc58a96c1a", "590c9a1ff422b03477f7830b20609f212c85aa13", "42e9055ec712ec9c7f0a79d963ea034a72dc7fa8", "b01871c114b122340209562972ff515b86b16ccf", "6a97d2668187965743d1b825b306defccbabbb4c", "09879f7956dddc2a9328f5c1472feeb8402bcbcf", "98445f4172659ec5e891e031d8202c102135c644", "df0402517a7338ae28bc54acaac400de6b456a46", "0936352b78a52bc5d2b5e3f04233efc56664af51", "f05d8eacc1469439bb04f2768fd68878c982e636", "bda89e0d181eda7e49ea831225eda86d075e111c", "66101233ea7cb27d66a9ef9ad42a51a72d25d9d6", "3d2c6941a9b4608ba52b328369a3352db2092ae0", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "ed032736652ac7e1f36ea17bd253cd1bfdcc3864", "d82b55c35c8673774a708353838918346f6c006f", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "0c3b69b5247ef18fd5bab1109d87a04184ea8f4b", "0f899b92b7fb03b609fee887e4b6f3b633eaf30d", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "90f72fbbe5f0a29e627db28999e01a30a9655bc6", "dc8301b67f98accbb331190dd7bd987952a692af", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "484ad17c926292fbe0d5211540832a8c8a8e958b", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "309494da0769345cb35ca0b7b0aae8143eee85a2", "32f078a7478d1ec2169599500a4507aceaccdda7", "1542fe458849b1ce26306f68200f6c71ae2ed147", "605402e235bd62437baf3c9ebefe77fb4d92ee95", "6dc61f37ecc552413606d8c89ffbc46ec98ed887"], "url": "https://www.semanticscholar.org/paper/d1c424c261c577958917055f72fb9e2ad0348865"}, "f0afdccf2903039d202085a771953a171dfd57b1": {"id": "f0afdccf2903039d202085a771953a171dfd57b1", "title": "Monotonic Chunkwise Attention", "authors": [{"authorId": "145039780", "name": "C. Chiu", "paperCount": 66, "citationCount": 6676, "hIndex": 31}, {"authorId": "2402716", "name": "Colin Raffel", "paperCount": 92, "citationCount": 19459, "hIndex": 38}], "abstract": "Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of a model using an offline soft attention mechanism. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention-based model.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 36, "citation_count": 193, "influential_paper_citations": 23, "is_open_access": false, "citations": ["21da617a0f79aabf94272107184606cefe90ab75", "8355c58a3e9b6254b88a7b5e684b612d71ea6019", "ee3c825e6bd1cd5bc3a8bb62a3dbab13ff1c5b73", "657329c633709dd1ac34a30d57341b186b1a47c2", "f0ead45e8c1e4cc390ff6603bc0738b8c57f99ec", "ff413cae44ca5e14281ebe4659b8627c349e8493", "a8427ce5aee6d62800c725588e89940ed4910e0d", "56e3ce0ff4cbd05e404214d19ae264fe6c457a16", "45c0e9d42624d02752b2c5adce6032db1deb2940", "c035cf0e1231eb196968d7255ab55827e932ec7a"], "references": ["4216ee11823aba41ad4c2adbe50f765e86a8a04b", "6cc68e8adf34b580f3f37d1bd267ee701974edde", "c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186", "76faaf292c6d9dc29d3a99300a7fdd7a35d6d107", "668db48c6a79826456341680ee1175dfc4cced71", "a072c2a400f62f720b68dc54a662fb1ae115bf06", "7dbb2d983ab95da04e5d47c87ddd2cd9a8f20786", "a32763adef1ef22cc27d4d67ef7ac1490d23ce0b", "d838994020a794b857a4cd356bfbbf7b52da7473", "9e2d9365ec2b940f4ba5d31f8a9950a184c2d2ed", "da797d293203ebe7c95edb98184955a9b92990a4", "f61da0efbb38bd3e6b9a9855809f5288b829f1f0", "3056add22b20e3361c38c0472d294a79d4031cb4", "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "a0d864d73189101a0bffc6656aa907f3b2193cfa", "6b904d6e84c98c6ce22ce6923224b205a2a24ee1", "7fe83e1a713ccb5ba19bce9ca933f958843916bb", "d59b01eb08500519e85424a02917340ee8e4dd2c", "87119572d1065fb079e1dee8fcdb6c4811143f96", "93499a7c7f699b6630a86fad964536f9423bb6d0", "b624504240fa52ab76167acfe3156150ca01cf3b", "5259755f9c100e220ffaa7e08439c5d34be7757a", "f10e071292d593fef939e6ef4a59baf0bb3a6c2b", "4d376d6978dad0374edfa6709c9556b42d3594d3", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "5d0fc052d75298ed0536a525c262b7915b304513", "96494e722f58705fa20302fe6179d483f52705b4", "1ff9332f2b62ccdded3f46320fd3661d99155652", "8648dbfff9662fa9c62a95622712dd2951b5b3a3"], "url": "https://www.semanticscholar.org/paper/f0afdccf2903039d202085a771953a171dfd57b1"}, "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035": {"id": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training", "authors": [{"authorId": "38909097", "name": "Alec Radford", "paperCount": 34, "citationCount": 52296, "hIndex": 24}, {"authorId": "144958935", "name": "Karthik Narasimhan", "paperCount": 64, "citationCount": 6759, "hIndex": 21}], "abstract": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classi\ufb01cation. Although large unlabeled text corpora are abundant, labeled data for learning these speci\ufb01c tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative \ufb01ne-tuning on each speci\ufb01c task. In contrast to previous approaches, we make use of task-aware input transformations during \ufb01ne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speci\ufb01cally crafted for each task, signi\ufb01cantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 74, "citation_count": 4425, "influential_paper_citations": 777, "is_open_access": false, "citations": ["9695824d7a01fad57ba9c01d7d76a519d78d65e7", "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "8f2bca9d684005675e294b33c26481e36f528cdb", "370b680057a6e324e67576a6bf1bf580af9fdd74", "4990f7542f0600e0501a7e7a931b32eb7cb804d5", "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe", "b3848d32f7294ec708627897833c4097eb4d8778", "a09cbcaac305884f043810afc4fa4053099b5970", "004b97aea43f9f62cc49dec20f449abfbae28811"], "references": ["928f9dccb806a3278d20d82cc53781c5f44e2bb1", "6084b58d8b4b0caf3a2a7f3a1bee1cc527927e39", "93b8da28d006415866bf48f9a6e06b5242129195", "7b29f45df975ed1e4c3864b6ab4483f11086aa76", "d66ad3628c11c45bde5d4b65b9c1109a95d364d4", "72c2cc507bc7203bcb4eaf6a3df6e9e8f8514e31", "3febb2bed8865945e7fddc99efd791887bb7e14f", "afc2850945a871e72c245818f9bc141bd659b453", "bc1d609520290e0460c49b685675eb5a57fa5935", "7570afa31c68e24fce1342b7d67c591787219bc1", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "f14dc3dba4a58fd380cce41e44552fd5f7812a4c", "e3d772986d176057aca2f5e3eb783da53b559134", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "cf8c493079702ec420ab4fc9c0fabb56b2a16c84", "45dfef0cc1ed96558c1c650432ce39d6a1050b6a", "4e013e6c800666c5bc611ca820ae437a7139cbb6", "2cdc28b4f34410ff70099ae845daaaa25813f0e9", "caa8a41d58e386c56f56d46bbe79df9cb1087338", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "3f1802d3f4f5f6d66875dac09112f978f12e1e1e", "ac17cfa150d802750b46220084d850cfdb64d1c1", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "636a79420d838eabe4af7fb25d6437de45ab64e8", "97394554eb5a74c3160c6bd743fcd3e4bd6cbe28", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "62f3d3015cee122bd147d7d878c85f70cc15680d", "85f94d8098322f8130512b4c6c4627548ce4a6cc", "8ff46c88964a36985f2b45933a3d47b81bd87bd0", "a07609c2ed39d049d3e59b61408fb600c6ab0950", "57ea3bc5ad8d67909b086a92a801a5e5f2d17035", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "05dd7254b632376973f3a1b4d39485da17814df5", "2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "1af68821518f03568f913ab03fc02080247a27ff", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "0e6824e137847be0599bb0032e37042ed2ef5045", "d1505c6123c102e53eb19dff312cb25cea840b72", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "a14045a751f5d8ed387c8630a86a3a2861b90643", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "687bac2d3320083eb4530bf18bb8f8f721477600", "0e5fa90e28fab414c8ef3ac6ca937c6195c2860e", "10ce58375314f90f83ca3bb88840cbc67bf8050f", "97474c55c834584b71f006557aed70e09eb6eb47", "b0c5f72790cca220541ea4809d1e43b4bdad1124", "bc1022b031dc6c7019696492e8116598097a8c12", "ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "db8885a0037fe47d973ade79d696586453710233", "843959ffdccf31c6694d135fad07425924f785b1", "57458bc1cffe5caa45a885af986d70f723f406b4", "7ece4e8d31f872d928369ac2cf58a616a7182112", "34ddd8865569c2c32dec9bf7ffc817ff42faaa01", "43c8a545f7166659e9e21c88fe234e0323855216", "932c2a02d462abd75af018125413b1ceaa1ee3f4", "d5f5110d65eda0d2df7329582a232a86bf9a3a65", "8978cf7574ceb35f4c3096be768c7547b28a35d0", "03bafef700d35112a9926dd1b2be91a4aa6984a4", "8eefd28eb47e72794bb0355d8abcbebaac9d8ab1", "31b4c03d721dc10b87c178277c1d369f91db8f0e", "a007f46b3303bdb50e705b441c367e595666538c", "475354f10798f110d34792b6d88f31d6d5cb099e"], "url": "https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"}, "252571243aa4c0b533aa7fc63f88d07fd844e7bb": {"id": "252571243aa4c0b533aa7fc63f88d07fd844e7bb", "title": "Learning What\u2019s Easy: Fully Differentiable Neural Easy-First Taggers", "authors": [{"authorId": "145644643", "name": "Andr\u00e9 F. T. Martins", "paperCount": 123, "citationCount": 4454, "hIndex": 33}, {"authorId": "3422710", "name": "Julia Kreutzer", "paperCount": 40, "citationCount": 810, "hIndex": 16}], "abstract": "We introduce a novel neural easy-first decoder that learns to solve sequence tagging tasks in a flexible order. In contrast to previous easy-first decoders, our models are end-to-end differentiable. The decoder iteratively updates a \u201csketch\u201d of the predictions over the sequence. At its core is an attention mechanism that controls which parts of the input are strategically the best to process next. We present a new constrained softmax transformation that ensures the same cumulative attention to every word, and show how to efficiently evaluate and backpropagate over it. Our models compare favourably to BILSTM taggers on three sequence tagging tasks.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2017, "reference_count": 47, "citation_count": 24, "influential_paper_citations": 1, "is_open_access": true, "citations": ["88e83776313effc1564044d7bf19972981815e3c", "657329c633709dd1ac34a30d57341b186b1a47c2", "0b02da8ba8c024850d4ee3ca3d1b0bddf6bf54f1", "b34e44d48c35750456d50d225c026536596b83fa", "ab456c1ed181c5c48a34adb61395d4806a0ba949", "4d5f904f923e5e031fb500a9e9ef7699ea9283de", "520ddb38b59b8fae2209ddc7c6640462cf153eec", "f4937d10b090ea21d9b6f54c7001a808e22d3d67", "aca487321c75a0ef15ae2083cd3c70b3e520986f", "9a7def005efb5b4984886c8a07ec4d80152602ab"], "references": ["480d545ac4a4ffff5b1bc291c2de613192e35d91", "7dbb2d983ab95da04e5d47c87ddd2cd9a8f20786", "1a327709cc53ff9e52454e50a643abf4a0ac92af", "4988a269e9f61c6fd1da502e34648b93dfd1a54d", "467d5d8fc766e73bfd3e9415f75479823f92c2f7", "e9ff2d0274bcb76bdea4ab8ae1d2c972f6e83c74", "acd87e4f672f0b92ea4164414c213560c23bee52", "4be0dd53aa1c751219fa6f19fed8a6324f6d2766", "eec3a236ecd185712ce65fb336141f8656eea13d", "24158c9fc293c8a998ac552b1188404a877da292", "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "94960fcdf0ea3b346fca77ae8c63ae7943eb0d28", "c1e3a26fb88c6720f4e84b7118e6f2df7dc8efa3", "4dabd6182ce2681c758f654561d351739e8df7bf", "350e78cfc83471276008a4a8023236accff6da50", "f0a4000286dcd6ec3fd955ebab988bae3eac4a4d", "af88ce6116c2cd2927a4198745e99e5465173783", "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99", "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "b36b7f7c68923d14ba2859b5d28a1124616a8c89", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "c3823aacea60bc1f2cabb9283144690a3d015db5", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "c48b7d4af1d783234004eb9f65fa11685c503afd", "ab0378836e5123d94fd5d46773c80d3e6c7742e0", "2466d675f9a8b07cc8295bf075a4bab7e8b7c5b2", "8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731", "654357faa0b3d847c3f7122b337c6ff6b2070bfe", "9c0ddf74f87d154db88d79c640578c1610451eec", "79ab3c49903ec8cb339437ccf5cf998607fc313e", "3ce0f00d6c949192107f1bd6a167c03e1fb7393a", "3c9d9f3c6f7508f4e29730924529dc993c27cddc", "d98d0d1900b13b87aa4ffd6b69c046beb63f0434", "6503a3d9fb204c2a08ecfcfe6ba5b815fc65a030", "e85ea7311e8fcc6c0a603ca39a74b99109e185e9", "30bc0ca0b965a7e01f1c4cf20684fd654f975e4a", "78e31b99dd72b8f66d9f00b7dcfc33eb48d52547", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "eb42a490cf4f186d3383c92963817d100afd81e2", "502858989368adae91e0f4a643ebc2aa6efd5738", "617111ed3746c4304c87b188ba155b160e9f082e", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/252571243aa4c0b533aa7fc63f88d07fd844e7bb"}, "2d7782c225e0fc123d6e227f2cb253e58279ac73": {"id": "2d7782c225e0fc123d6e227f2cb253e58279ac73", "title": "Improving Neural Language Models with a Continuous Cache", "authors": [{"authorId": "3024698", "name": "Edouard Grave", "paperCount": 71, "citationCount": 21226, "hIndex": 38}, {"authorId": "2319608", "name": "Armand Joulin", "paperCount": 96, "citationCount": 27135, "hIndex": 51}, {"authorId": "1746841", "name": "Nicolas Usunier", "paperCount": 116, "citationCount": 15412, "hIndex": 39}], "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 55, "citation_count": 259, "influential_paper_citations": 49, "is_open_access": false, "citations": ["9405cc0d6169988371b2755e573cc28650d14dfe", "c1f457e31b611da727f9aef76c283a18157dfa83", "921196c32213a229245a9705ee4768bc941e7a26", "fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "88caa4a0253a8b0076176745ebc072864eab66e1", "58c6f890a1ae372958b7decf56132fe258152722", "ed46493d568030b42f0154d9e5bf39bbd07962b3", "15d6f3d815d0ff176fafb14a3f46e5723ebac723"], "references": ["efbd381493bb9636f489b965a2034d529cd56bcd", "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "77fb0b7aef619dfac650423d4677170df2158e0d", "b1e20420982a4f923c08652941666b189b11b7fe", "aa5b35dcf8b024f5352db73cc3944e8fad4f3793", "f2e50e2ee4021f199877c8920f1f984481c723aa", "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "be3a65ef15f79ebb8296e6a0e8d1a9cb5c0f3638", "94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f", "6c5325c2b67bf88f2b846cf5a6df6c2e6362d75b", "d1505c6123c102e53eb19dff312cb25cea840b72", "9653d5c2c7844347343d073bbedd96e05d52f69b", "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "9665247ea3421929f9b6ad721f139f11edb1dbb8", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "adfcf065e15fd3bc9badf6145034c84dfb08f204", "c3823aacea60bc1f2cabb9283144690a3d015db5", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "d1275b2a2ab53013310e759e5c6878b96df643d4", "77dfe038a9bdab27c4505444931eaa976e9ec667", "413c1142de9d91804d6d11c67ff3fed59c9fc279", "9819b600a828a57e1cde047bbe710d3446b30da5", "09c76da2361d46689825c4efc37ad862347ca577", "04df7d50ffc522d752116070a5cea0d3a16405b2", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "7ac0550daef2f936c4280aca87ff8e9c7e7baf69", "a90c1ca6c335de94721d7445bb01b723c3d9a840", "22d45dadde6b5837eff11dc031045754bc5901c3", "b888cae7e6e288b108f9d119fc23b84b4d447029", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "404fffebcdb9b597489f62735d8ce59eff41f623", "076fa8d095c37c657f2aff39cf90bc2ea883b7cb", "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "ab7b5917515c460b90451e67852171a531671ab8", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "606df60d518db088986e74fad1f357ea6e5312f2", "6e58b5f825df9fb0b00465a66598f302c30b080a", "eadf7d20852caa92310d0cb582269b94226b1e58", "0687165a9f0360bde0469fd401d966540e0897c3", "26bc0449360d7016f684eafae5b5d2feded32041", "1a3d22599028a05669e884f3eaf19a342e190a87", "be1fed9544830df1137e72b1d2396c40d3e18365", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "343c8af478f7703459b0e390e888efe723f15e31", "491566891addc26134c617ab026f5548de39401a", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58"], "url": "https://www.semanticscholar.org/paper/2d7782c225e0fc123d6e227f2cb253e58279ac73"}, "efbd381493bb9636f489b965a2034d529cd56bcd": {"id": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models", "authors": [{"authorId": "3375440", "name": "Stephen Merity", "paperCount": 18, "citationCount": 3449, "hIndex": 9}, {"authorId": "2228109", "name": "Caiming Xiong", "paperCount": 212, "citationCount": 17287, "hIndex": 57}, {"authorId": "40518045", "name": "James Bradbury", "paperCount": 18, "citationCount": 2754, "hIndex": 10}, {"authorId": "2166511", "name": "R. Socher", "paperCount": 202, "citationCount": 106675, "hIndex": 76}], "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 29, "citation_count": 1190, "influential_paper_citations": 282, "is_open_access": false, "citations": ["9405cc0d6169988371b2755e573cc28650d14dfe", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "d0086b86103a620a86bc918746df0aa642e2a8a3", "05f5f8b2065a520846d89771ebaea2bb1534e9c6", "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "fe8907302f9d14233cd03cc2948a1c4e2a50bdb6", "2e3002f131e1815bda7a10303eff97f79dea01ec", "15d6f3d815d0ff176fafb14a3f46e5723ebac723", "af1f7739283bdbd2b7a94903041f6d6afd991907", "8f6602b6ebe2962dacf0b563e73852183e628ddf"], "references": ["e44da7d8c71edcc6e575fa7faadd5e75785a7901", "7dba53e72c182e25e98e8f73a99d75ff69dda0c2", "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105", "aa5b35dcf8b024f5352db73cc3944e8fad4f3793", "e957747f4f8600940be4c5bb001aa70c84e53a53", "ba30df190664193514d1d309cb673728ed48f449", "f2e50e2ee4021f199877c8920f1f984481c723aa", "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "452059171226626718eb677358836328f884298e", "9653d5c2c7844347343d073bbedd96e05d52f69b", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "533ee188324b833e059cb59b654e6160776d5812", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "2a76c2121eee30af82a24058b4e149f05bcda911", "84069287da0a6b488b8c933f3cb5be759cb6237e", "d1275b2a2ab53013310e759e5c6878b96df643d4", "9819b600a828a57e1cde047bbe710d3446b30da5", "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "99e8d34817ae10d7304521e89c5fbf908b9d856b", "04df7d50ffc522d752116070a5cea0d3a16405b2", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "076fa8d095c37c657f2aff39cf90bc2ea883b7cb", "0b44fcbeea9415d400c5f5789d6b892b6f98daff"], "url": "https://www.semanticscholar.org/paper/efbd381493bb9636f489b965a2034d529cd56bcd"}, "be8c6c69f3e357bfad2987e45b62cff7e7474378": {"id": "be8c6c69f3e357bfad2987e45b62cff7e7474378", "title": "Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes", "authors": [{"authorId": "34269227", "name": "Jack W. Rae", "paperCount": 25, "citationCount": 2557, "hIndex": 20}, {"authorId": "2323922", "name": "Jonathan J. Hunt", "paperCount": 22, "citationCount": 9265, "hIndex": 10}, {"authorId": "1841008", "name": "Ivo Danihelka", "paperCount": 24, "citationCount": 9448, "hIndex": 18}, {"authorId": "3367786", "name": "Tim Harley", "paperCount": 15, "citationCount": 9222, "hIndex": 12}, {"authorId": "33666044", "name": "A. Senior", "paperCount": 140, "citationCount": 43010, "hIndex": 58}, {"authorId": "89504302", "name": "Greg Wayne", "paperCount": 49, "citationCount": 7965, "hIndex": 28}, {"authorId": "1753223", "name": "A. Graves", "paperCount": 83, "citationCount": 83500, "hIndex": 54}, {"authorId": "2542999", "name": "T. Lillicrap", "paperCount": 133, "citationCount": 60273, "hIndex": 60}], "abstract": "Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows \u2014 limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs 1,000 x faster and with 3,000x less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 22, "citation_count": 138, "influential_paper_citations": 14, "is_open_access": false, "citations": ["29e944711a354c396fad71936f536e83025b6ce0", "007112213ece771be72cbecfd59f048209facabd", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803", "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "6a0d253053ce8646e49904efe0e062bcc30d8257", "f51497f463566581874c941353dd9d80069c5b77", "4ff28221f1d734d65ff78587219f3c2fc17c4c43", "657329c633709dd1ac34a30d57341b186b1a47c2", "0735bc67a2149d5c6f38a2e97b2ef8678666d577"], "references": ["784ee73d5363c711118f784428d1ab89f019daa5", "3904315e2eca50d0086e4b7273f7fd707c652230", "35b91b365ceb016fb3e022577cec96fb9b445dc5", "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "815c84ab906e43f3e6322f2ca3fd5e1360c64285", "5259755f9c100e220ffaa7e08439c5d34be7757a", "f10e071292d593fef939e6ef4a59baf0bb3a6c2b", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "c3823aacea60bc1f2cabb9283144690a3d015db5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "04903c5e874fb490d6d6e950d3a80460ed0ab0ea", "a85e512d8845bd007b0866b4a97e8341463f8190", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "3449b65008b27f6e60a73d80c1fd990f0481126b", "98b37a4540441c2c2bdd62be1313fd2d087e0187", "a81f31dce2341b638c2f0fe83a93bc74b905f55f", "22cc0ac7fdf06de9a8edb58fd1a1518c0c04d376", "219101fe724232acc330ff0910152931538f85c7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/be8c6c69f3e357bfad2987e45b62cff7e7474378"}, "97fb4e3d45bb098e27e0071448b6152217bd35a5": {"id": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization", "authors": [{"authorId": "2503659", "name": "Jimmy Ba", "paperCount": 80, "citationCount": 125590, "hIndex": 32}, {"authorId": "51131802", "name": "J. Kiros", "paperCount": 24, "citationCount": 5110, "hIndex": 11}, {"authorId": "1695689", "name": "Geoffrey E. Hinton", "paperCount": 398, "citationCount": 383672, "hIndex": 147}], "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feedforward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 33, "citation_count": 3867, "influential_paper_citations": 211, "is_open_access": false, "citations": ["7519a1e9e7371df79bd8a21cee871feb0ec597a5", "be0fbb810583930c071d0b9b2c5187fe260783f5", "adb272fbdea3631059cf88ab764bb6c2ce29f965", "7d4c2c8407e0caf2f907df9954b056a42a92fd13", "5e4f03f68c6867d850f457dc5cc36738e5dff6c1", "867d80c8779e1d301a5fc6e267e263f7e4c4c5c7", "7c597874535c1537d7ddff3b3723015b4dc79d30", "a7266bc2229ef8da4baf769eab925646601dfaad", "3f9f7f690e003176316d0ee56fbcbfed4b6b0948", "28a1a6e0b84747ed8bee294edc3606d4451e3c01"], "references": ["952454718139dba3aafc6b3b67c4f514ac3964af", "6b570069f14c7588e066f7138e1f21af59d62e61", "3d2c6941a9b4608ba52b328369a3352db2092ae0", "8ff840a40d3f1557c55c19d4d636da77103168ce", "b27e791e843c924ef052981b79490ab59fc0433d", "46b8cbcdff87b842c2c1d4a003c831f845096ba7", "f95adc1d8daaa07a0c956826ec274ca9e2515ddc", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "0e6824e137847be0599bb0032e37042ed2ef5045", "d1505c6123c102e53eb19dff312cb25cea840b72", "6fe02ad979baad659f04c3376a77dbb2cb4699a5", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "4d376d6978dad0374edfa6709c9556b42d3594d3", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "eb42cf88027de515750f230b23b1a057dc782108", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "cea967b59209c6be22829699f05b8b1ac4dc092d", "11ec56898a9e7f401a2affe776b5297bd4e25025", "0b544dfe355a5070b60986319a3f51fb45d1348e", "71b7178df5d2b112d07e45038cb5637208659ff7", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "3127190433230b3dc1abd0680bb58dced4bcd90e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "31868290adf1c000c611dfc966b514d5a34e8d23", "32f078a7478d1ec2169599500a4507aceaccdda7", "9eb7daa88879f283ae05e359d6c502a320b833c9", "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "1cff7cc15555c38607016aaba24059e76b160adb", "cdcf7cb29f37ac0546961ea8a076075b9cc1f992", "167e1359943b96b9e92ee73db1df69a1f65d731d", "5a767a341364de1f75bea85e0b12ba7d3586a461"], "url": "https://www.semanticscholar.org/paper/97fb4e3d45bb098e27e0071448b6152217bd35a5"}, "0936352b78a52bc5d2b5e3f04233efc56664af51": {"id": "0936352b78a52bc5d2b5e3f04233efc56664af51", "title": "Conditional Image Generation with PixelCNN Decoders", "authors": [{"authorId": "3422336", "name": "A\u00e4ron van den Oord", "paperCount": 57, "citationCount": 23700, "hIndex": 36}, {"authorId": "2583391", "name": "Nal Kalchbrenner", "paperCount": 35, "citationCount": 30929, "hIndex": 25}, {"authorId": "2311318", "name": "Lasse Espeholt", "paperCount": 12, "citationCount": 6611, "hIndex": 9}, {"authorId": "2645384", "name": "K. Kavukcuoglu", "paperCount": 102, "citationCount": 115978, "hIndex": 67}, {"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "1753223", "name": "A. Graves", "paperCount": 83, "citationCount": 83500, "hIndex": 54}], "abstract": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2016, "reference_count": 32, "citation_count": 1798, "influential_paper_citations": 187, "is_open_access": false, "citations": ["289db3be7bf77e06e75541ba93269de3d604ac72", "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "1623d6ffb6efd94d21537db2b96b91a196842aef", "35b966347dae2f0d496ea713edf03a68211838a5", "5df999a1077524c9178b43c84f80492f8aaa5a7e", "de18baa4964804cf471d85a5a090498242d2e79f", "501c02c7caa7fc2c7077405299b4cbe7d294b170", "853de0e00ac5ac257a622ae678ed373b8e086404", "3a6213f716bf0405b7e4af0531fa066f0a547eae"], "references": ["b0ee82c08fda544eb159eca43f7609990a75e052", "6e90fd78e8a3b98af3954aae5209703aa966603e", "3cccc93064dae265eeb7bc76d02cdc67c942f0a5", "bda89e0d181eda7e49ea831225eda86d075e111c", "0811597b0851b7ebe21aadce7cb4daac4664b44f", "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "0875fc92cce33df5cf7df169590dbf0ca00d2652", "39e0c341351f8f4a39ac890b96217c7f4bde5369", "5b791cd374c7109693aaddee2c12d659ae4e3ec0", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6", "e4257bc131c36504a04382290cbc27ca8bb27813", "b92aa7024b87f50737b372e5df31ef091ab54e62", "47900aca2f0b50da3010ad59b394c870f0e6c02e", "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "5aa26299435bdf7db874ef1640a6c3b5a4a2c394", "2dcef55a07f8607a819c21fe84131ea269cc2e3c", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "dc8301b67f98accbb331190dd7bd987952a692af", "ad2469bff83c9d5a55dd81040d369c8af29be132", "e193734562261e42e207de81a44b8f874c9faf0a", "0523e14247d74c4505cd5e32e1f0495f291ec432", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "484ad17c926292fbe0d5211540832a8c8a8e958b", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "f4278dac2abafb66a0a824f1d59e72b67e7540a1", "8948bea1e2436e51316f131170923cb5b7d870db", "32f078a7478d1ec2169599500a4507aceaccdda7", "1c6d990c80e60aa0b0059415444cdf94b3574f0f", "c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22"], "url": "https://www.semanticscholar.org/paper/0936352b78a52bc5d2b5e3f04233efc56664af51"}, "7fe83e1a713ccb5ba19bce9ca933f958843916bb": {"id": "7fe83e1a713ccb5ba19bce9ca933f958843916bb", "title": "A Neural Transducer", "authors": [{"authorId": "3111912", "name": "N. Jaitly", "paperCount": 103, "citationCount": 31348, "hIndex": 48}, {"authorId": "3089810", "name": "David Sussillo", "paperCount": 53, "citationCount": 5662, "hIndex": 31}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}, {"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}, {"authorId": "1751569", "name": "Samy Bengio", "paperCount": 379, "citationCount": 51213, "hIndex": 85}], "abstract": "Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 19, "citation_count": 45, "influential_paper_citations": 2, "is_open_access": false, "citations": ["09e2c7adbed37440d4a339852cfa34e5b660f768", "b13e9d23983273c0c67b91ae70c55d4c3f745b8b", "657329c633709dd1ac34a30d57341b186b1a47c2", "76faaf292c6d9dc29d3a99300a7fdd7a35d6d107", "f0afdccf2903039d202085a771953a171dfd57b1", "f0afdccf2903039d202085a771953a171dfd57b1", "ca781fb293c2521d0737899d252d47a97eca0d58", "a32763adef1ef22cc27d4d67ef7ac1490d23ce0b", "9a6b904c3e52d8665119f8e8352381f46970faeb", "7703a2c5468ecbee5b62c048339a03358ed5fe19"], "references": ["bf85a0cd645ad68919c0706741ab568a60a58af2", "878ba5458e9e51f0b341fd9117fa0b43ef4096d3", "dc555e8156c956f823587ebbff018863e6d2a95e", "b624504240fa52ab76167acfe3156150ca01cf3b", "5247a6e3a60ff0381355e66bfc313bf27512ae0c", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "f10e071292d593fef939e6ef4a59baf0bb3a6c2b", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "47d2dc34e1d02a8109f5c04bb6939725de23716d", "c3823aacea60bc1f2cabb9283144690a3d015db5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "2319a491378867c7049b3da055c5df60e1671158", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "31868290adf1c000c611dfc966b514d5a34e8d23"], "url": "https://www.semanticscholar.org/paper/7fe83e1a713ccb5ba19bce9ca933f958843916bb"}, "a418524a3576afff4dc2178ed169e692915bd46b": {"id": "a418524a3576afff4dc2178ed169e692915bd46b", "title": "An Online Sequence-to-Sequence Model Using Partial Conditioning", "authors": [{"authorId": "3111912", "name": "N. Jaitly", "paperCount": 103, "citationCount": 31348, "hIndex": 48}, {"authorId": "2827616", "name": "Quoc V. Le", "paperCount": 223, "citationCount": 120217, "hIndex": 109}, {"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}, {"authorId": "3089810", "name": "David Sussillo", "paperCount": 53, "citationCount": 5662, "hIndex": 31}, {"authorId": "1751569", "name": "Samy Bengio", "paperCount": 379, "citationCount": 51213, "hIndex": 85}], "abstract": "Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a new model that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, our method computes the next-step distribution conditioned on the partial input sequence observed and the partial sequence generated. It accomplishes this goal using an encoder recurrent neural network (RNN) that computes features at the same frame rate as the input, and a transducer RNN that operates over blocks of input steps. The transducer RNN extends the sequence produced so far using a local sequence-to-sequence model. During training, our method uses alignment information to generate supervised targets for each block. Approximate alignment is easily available for tasks such as speech recognition, action recognition in videos, etc. During inference (decoding), beam search is used to find the most likely output sequence for an input sequence. This decoding is performed online - at the end of each block, the best candidates from the previous block are extended through the local sequence-to-sequence model. On TIMIT, our online method achieves 19.8% phone error rate (PER). For comparison with published sequence-to-sequence methods, we used a bidirectional encoder and achieved 18.7% PER compared to 17.6% from the best reported sequence-to-sequence model. Importantly, unlike sequence-to-sequence our model is minimally impacted by the length of the input. On artificially created longer utterances, it achieves 20.9% with a unidirectional model, compared to 20% from the best bidirectional sequence-to-sequence models.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 35, "citation_count": 102, "influential_paper_citations": 9, "is_open_access": false, "citations": ["c6b61535f1544835cca3851ceb34222ebc5b4377", "9d3480e46cc506b73d5291387c6452998690fdd3", "b13e9d23983273c0c67b91ae70c55d4c3f745b8b", "657329c633709dd1ac34a30d57341b186b1a47c2", "f0ead45e8c1e4cc390ff6603bc0738b8c57f99ec", "ca781fb293c2521d0737899d252d47a97eca0d58", "eac48f406c46527f5ca821de7fe8d62d6db56a27", "56e3ce0ff4cbd05e404214d19ae264fe6c457a16", "9a6b904c3e52d8665119f8e8352381f46970faeb", "04187519dc8c468f2b5b17442413ada7830068e5"], "references": ["b59d91e0699d4e1896a15bae13fd180bdaf77ea5", "bf85a0cd645ad68919c0706741ab568a60a58af2", "878ba5458e9e51f0b341fd9117fa0b43ef4096d3", "94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f", "dc555e8156c956f823587ebbff018863e6d2a95e", "9fca2af9a0e3f2c5c3ed47abb3ebd21b7265ac2b", "b624504240fa52ab76167acfe3156150ca01cf3b", "5247a6e3a60ff0381355e66bfc313bf27512ae0c", "85315b64a4c73cb86f156ef5b0a085d6ebc8a65d", "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99", "5259755f9c100e220ffaa7e08439c5d34be7757a", "f10e071292d593fef939e6ef4a59baf0bb3a6c2b", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "47d2dc34e1d02a8109f5c04bb6939725de23716d", "c3823aacea60bc1f2cabb9283144690a3d015db5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "0b544dfe355a5070b60986319a3f51fb45d1348e", "2319a491378867c7049b3da055c5df60e1671158", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "31868290adf1c000c611dfc966b514d5a34e8d23", "9a9f4bf3bfe133e1c70f6b60654c238b677c66d0", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85"], "url": "https://www.semanticscholar.org/paper/a418524a3576afff4dc2178ed169e692915bd46b"}, "93499a7c7f699b6630a86fad964536f9423bb6d0": {"id": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation", "authors": [{"authorId": "1821711", "name": "Thang Luong", "paperCount": 33, "citationCount": 9075, "hIndex": 14}, {"authorId": "143950636", "name": "Hieu Pham", "paperCount": 31, "citationCount": 11107, "hIndex": 15}, {"authorId": "144783904", "name": "Christopher D. Manning", "paperCount": 517, "citationCount": 149000, "hIndex": 135}], "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT\u201915 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 17, "citation_count": 6431, "influential_paper_citations": 665, "is_open_access": true, "citations": ["dc9b9c5a76df331409890eea0d01e3cf3459a526", "f1321f2df5bc686d3adfba8eae06a6c12cb88ef8", "5f636c3da9c5aaec597676a43d52a06f4d7fa181", "c48fdbbd2c671c8fc9495799c063091eba5e20d6", "92abe93c277970db9ed5caa1229e8bd0357372ff", "492a655a67e6ec7423a968cedb70eec0cdbc8e98", "49b5202408c901dead51754886e7e65e2c1f6d03", "8f6c652a392995bd047a2f7b94474ab1e6e23ff0", "1d3900c44eb9840205927dcfbde2d370a574c0b9", "6c1f6ede9b786e1ffdaab91bf346817f61706912"], "references": ["a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "1956c239b3552e030db1b78951f64781101125ed", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "47d2dc34e1d02a8109f5c04bb6939725de23716d", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "8a756d4d25511d92a45d0f4545fa819de993851d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "8e4fb17fff38a7834af5b4eaafcbbde02bf00975", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "52805ca2a7f5f6e73dc90ff20f1ca2f198dd031b", "f4f6bfacb4cd508df62540f5aa9ba30cd83dd127", "a4b828609b60b06e61bea7a4029cc9e1cad5df87", "d7da009f457917aa381619facfa5ffae9329a6e9"], "url": "https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0"}, "a2e2970dd99b86fd6198c0b756c6cd4d52e34c3b": {"id": "a2e2970dd99b86fd6198c0b756c6cd4d52e34c3b", "title": "Clustering is Efficient for Approximate Maximum Inner Product Search", "authors": [{"authorId": "3082019", "name": "Alex Auvolat", "paperCount": 16, "citationCount": 404, "hIndex": 8}, {"authorId": "145467703", "name": "Pascal Vincent", "paperCount": 78, "citationCount": 25463, "hIndex": 39}], "abstract": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 27, "citation_count": 34, "influential_paper_citations": 9, "is_open_access": false, "citations": ["657329c633709dd1ac34a30d57341b186b1a47c2", "435553998fbef790b5bed3491a8f634d9ec5cfa2", "f2171bc79fb72693f60c8e6b0dd547f7d4f850ac", "d3c6c635b9cfd8890c7244d3db4be53d45944963", "865488b467250ceb85bb59d35b8d4fb4f10ae421", "965b0591e4f63ec9841b6e0a95303e8189e792e6", "1bf64f0961da08ea0f9941bd899e916a385e9540", "168ebafd229491250edf9e58aa7333d080c72cfe", "9f753f67da834e59f9a5c8cdf9a88ee84c496b2d", "d6be0882f9af03445a1c8e06e6495079f3869713"], "references": ["e15fdad9f7d160e11e9a313bd80ebe99952eff08", "94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f", "8250ecbaef057bdb5390ef4e4be798f1523a23f6", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "61a5de284be0243ff811c4ffb96c788cd9dd6999", "5b0a88bdec473552c6a386cd94fdac53c74b79a8", "d8aad5d9c2c336c550ca9621b73c7688f184d50b", "6dbffa57b3c6c5645cf701b9b444984a4b61bb57", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "c5bce0ff47e945d00de85170ac4fd21b6c2ae1be", "330da625c15427c6e42ccfa3b747fb29e5835bf0", "855d0f722d75cc56a66a00ede18ace96bafee6bd", "f87786cc77c8a0d85fd6a1d746d9e78583c1e94d", "dcd2755f4e7b5ed96571ec6a741b172a217cabe2", "aa1762a629b31d254450e37ce8baa235d729d82b", "bc1022b031dc6c7019696492e8116598097a8c12", "d5fdc3c0b2049a025091179a73e0e4174105fcd4", "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025", "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "9a6824ffa600f06aca27b9dd54fa8d75a5cf4a16", "c19fbefdeead6a4154a22a9c8551a18b1530033a", "3f1e54ed3bd801766e1897d53a9fc962524dd3c2", "6c2b28f9354f667cd5bd07afc0471d8334430da7", "2352d9105de31032538900dfb2ce7c95f6402963", "9241ea3d8cb85633d314ecb74b31567b8e73f6af", "ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed"], "url": "https://www.semanticscholar.org/paper/a2e2970dd99b86fd6198c0b756c6cd4d52e34c3b"}, "b624504240fa52ab76167acfe3156150ca01cf3b": {"id": "b624504240fa52ab76167acfe3156150ca01cf3b", "title": "Attention-Based Models for Speech Recognition", "authors": [{"authorId": "2292403", "name": "J. Chorowski", "paperCount": 50, "citationCount": 9610, "hIndex": 19}, {"authorId": "3335364", "name": "Dzmitry Bahdanau", "paperCount": 47, "citationCount": 50418, "hIndex": 23}, {"authorId": "1862138", "name": "Dmitriy Serdyuk", "paperCount": 19, "citationCount": 6334, "hIndex": 14}, {"authorId": "1979489", "name": "Kyunghyun Cho", "paperCount": 326, "citationCount": 88663, "hIndex": 80}, {"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}], "abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1,2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMET phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 33, "citation_count": 2068, "influential_paper_citations": 125, "is_open_access": false, "citations": ["fcecc4ef2c32dbedda61648febb39a0f905c367e", "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "657329c633709dd1ac34a30d57341b186b1a47c2", "79c5ace95f0bcd33c1e02b7e83a2e0cdadb6b50a", "32fa1612b63bfc554e57a914c87ee46f4f6428cb", "a8427ce5aee6d62800c725588e89940ed4910e0d", "1de68f12db136526116c6ac7064fd13965f2c966", "3a223a5174717a193bb7e22c825d801273ad7cb2", "30de7fecb146356d7ae6cafa072997674601e0f5", "e98cb70e420148de2b6b52922dae631a6ee7ff74"], "references": ["59b81ff81da55efc724c84ddc9d3ffd8e57a8d0e", "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "a583af2696030bcf5f556edc74573fbee902be0b", "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "71ae756c75ac89e2d731c9c79649562b5768ff39", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "24741d280869ad9c60321f5ab6e5f01b7852507d", "47d2dc34e1d02a8109f5c04bb6939725de23716d", "c3823aacea60bc1f2cabb9283144690a3d015db5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "8a756d4d25511d92a45d0f4545fa819de993851d", "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "8dc1d5c47b8af57cbff36632318b4302706df6a3", "836acf6fc99ebf81d219e2b67f7ab25efc29a6a4", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "8729441d734782c3ed532a7d2d9611b438c0a09a", "855d0f722d75cc56a66a00ede18ace96bafee6bd", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "31868290adf1c000c611dfc966b514d5a34e8d23", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "bc6dff14a130c57a91d5a21339c23471faf1d46f", "5a9ef216bf11f222438fff130c778267d39a9564", "3a1a2cff2b70fb84a7ca7d97f8adcc5855851795", "63936fa32f9e75ab2a864daae6791ce02112183d", "9eab007a8c0af72f1fdffe2aef210d790fb79d79", "96494e722f58705fa20302fe6179d483f52705b4", "162d958ff885f1462aeda91cd72582323fd6a1f4", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "47128bb3ce4ed00691c0d7d58c02791c3e963ab7", "b3db94f62118e192ef0465ca9edafcd6c074c137"], "url": "https://www.semanticscholar.org/paper/b624504240fa52ab76167acfe3156150ca01cf3b"}, "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca": {"id": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "title": "DRAW: A Recurrent Neural Network For Image Generation", "authors": [{"authorId": "144717963", "name": "Karol Gregor", "paperCount": 36, "citationCount": 7908, "hIndex": 20}, {"authorId": "1841008", "name": "Ivo Danihelka", "paperCount": 24, "citationCount": 9448, "hIndex": 18}, {"authorId": "1753223", "name": "A. Graves", "paperCount": 83, "citationCount": 83500, "hIndex": 54}, {"authorId": "1748523", "name": "Danilo Jimenez Rezende", "paperCount": 77, "citationCount": 17440, "hIndex": 39}, {"authorId": "1688276", "name": "Daan Wierstra", "paperCount": 63, "citationCount": 59256, "hIndex": 44}], "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 36, "citation_count": 1744, "influential_paper_citations": 136, "is_open_access": false, "citations": ["d07284a6811f1b2745d91bdb06b040b57f226882", "de95601d9e3b20ec51aa33e1f27b1880d2c44ef2", "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "4eebe0d12aefeedf3ca85256bc8aa3b4292d47d9", "b2504b0b2a7e06eab02a3584dd46d94a3f05ffdf", "5156381d63bb3e873533b08f203cb56c2d79b6c9", "51cdeeef710d1c84e10beadc8480c137ffe8d328", "329b84a919bfd1771be5bd14fa81e7b3f74cc961", "10bb4ef7a6719ea132e00f0ab5680919a4131d99"], "references": ["7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "6ef259c2f6d50373abfec14fcb8fa924f7b7af0b", "7198597c62f6b6433d9ed6d5b44f887bd05d3c56", "3445cc781ebdcf65840bd6314bc0c8c634f1ef5e", "b712ccc32236afe9f8a13d9871911b30aa069e2b", "c3823aacea60bc1f2cabb9283144690a3d015db5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "8a756d4d25511d92a45d0f4545fa819de993851d", "c175081733d98e63fa68a70405a35804c318afc5", "018300f5f0e679cee5241d9c69c8d88e00e8bf31", "484ad17c926292fbe0d5211540832a8c8a8e958b", "0ca6cccbfcf3df972a470c7fe18f7eaed9420cd6", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "b3d8dffb73bc93de239998548386c84177caa2ad", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "705fd4febe2fff810d2f72f48dcda20826eca77a", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "72829d537f0ec8b1cc0ced2f278bb56ce89f1b0c", "32f078a7478d1ec2169599500a4507aceaccdda7", "02227c94dd41fe0b439e050d377b0beb5d427cda", "0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3", "85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "5d90f06bb70a0a3dced62413346235c02b1aa086", "4341446db90f569e689cddf2b08a5093a5bb83ae", "08d0ea90b53aba0008d25811268fe46562cfb38c", "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e", "11540131eae85b2e11d53df7f1360eeb6476e7f4", "eeab7935e67458ee0f89c58c4a0b9cf485675695", "162d958ff885f1462aeda91cd72582323fd6a1f4", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "605402e235bd62437baf3c9ebefe77fb4d92ee95"], "url": "https://www.semanticscholar.org/paper/a2785f66c20fbdf30ec26c0931584c6d6a0f4fca"}, "4d8f2d14af5991d4f0d050d22216825cac3157bd": {"id": "4d8f2d14af5991d4f0d050d22216825cac3157bd", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "authors": [{"authorId": "2117101253", "name": "Ke Xu", "paperCount": 8, "citationCount": 8114, "hIndex": 3}, {"authorId": "2503659", "name": "Jimmy Ba", "paperCount": 80, "citationCount": 125590, "hIndex": 32}, {"authorId": "3450996", "name": "Ryan Kiros", "paperCount": 24, "citationCount": 16219, "hIndex": 14}, {"authorId": "1979489", "name": "Kyunghyun Cho", "paperCount": 326, "citationCount": 88663, "hIndex": 80}, {"authorId": "1760871", "name": "Aaron C. Courville", "paperCount": 252, "citationCount": 98036, "hIndex": 82}, {"authorId": "145124475", "name": "R. Salakhutdinov", "paperCount": 332, "citationCount": 123720, "hIndex": 101}, {"authorId": "1804104", "name": "R. Zemel", "paperCount": 238, "citationCount": 42826, "hIndex": 69}, {"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}], "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2015, "reference_count": 53, "citation_count": 8044, "influential_paper_citations": 713, "is_open_access": false, "citations": ["f1321f2df5bc686d3adfba8eae06a6c12cb88ef8", "0167e98f6d2e4c44b505c0f74f91425f62dfc62c", "19b3b074d38b250d024920732ae51a8ffa0996dd", "347278d90b63937bd157b76ccd53f5e3296de225", "16dcfa217296a6b0571ce12fa6eca36e694b2d24", "abed09d4722b3259b68cb42141d532126076dab2", "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "a7266bc2229ef8da4baf769eab925646601dfaad", "003a11f401e286ccfd3a699d8f55db5cf81fd540", "50557623b7b5908c892b0e8f54a520b6f9d3e65f"], "references": ["55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "5f425b7abf2ed3172ed060df85bb1885860a297e", "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745", "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "f01fc808592ea7c473a69a6e7484040a435f36d9", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "f4af49a1ead3c81cc5d023878cb67c5646dd8a04", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "59927ded86ab4f7253fc32efb351e5a13e746ead", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "8a756d4d25511d92a45d0f4545fa819de993851d", "fad611e35b3731740b4d8b754241e77add5a70b9", "0b544dfe355a5070b60986319a3f51fb45d1348e", "26adb749fc5d80502a6d889966e50b31391560d3", "327d3df8ea2020882827d6bace1e26c9d24309c2", "71b7178df5d2b112d07e45038cb5637208659ff7", "44040913380206991b1991daf1192942e038fe31", "17a5868dc7abfa9495af6b1ae71042a006238ebc", "484ad17c926292fbe0d5211540832a8c8a8e958b", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "0ca6cccbfcf3df972a470c7fe18f7eaed9420cd6", "533ee188324b833e059cb59b654e6160776d5812", "34f25a8704614163c4095b3ee2fc969b60de4698", "5cb6700d94c6118ee13f4f4fecac99f111189812", "3f6a4556769e819242d669d073b895f1e45a706f", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "082fb4bc8e52c973ddfb13aee5558d1a8bf1fd9e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "855d0f722d75cc56a66a00ede18ace96bafee6bd", "2a0d0f6c5a69b264710df0230696f47c5918e2f2", "2e2089ae76fe914706e6fa90081a79c8fe01611e", "355de7460120ddc1150d9ce3756f9848983f7ff4", "72829d537f0ec8b1cc0ced2f278bb56ce89f1b0c", "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11", "fbdbe747c6aa8b35b981d21e475ff1506a1bae66", "0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3", "4c915c1eecb217c123a36dc6d3ce52d12c742614", "53e66b6934516a9859573f4866f81f04bce977ae", "4d92df4a844c94fbb31b95157488e4b562b4f681", "00e64fb34f407f5939612481ebc93a44d571c9c7", "44d2abe2175df8153f465f6c39b68b76a0d40ab9"], "url": "https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd"}, "c3823aacea60bc1f2cabb9283144690a3d015db5": {"id": "c3823aacea60bc1f2cabb9283144690a3d015db5", "title": "Neural Turing Machines", "authors": [{"authorId": "1753223", "name": "A. Graves", "paperCount": 83, "citationCount": 83500, "hIndex": 54}, {"authorId": "89504302", "name": "Greg Wayne", "paperCount": 49, "citationCount": 7965, "hIndex": 28}, {"authorId": "1841008", "name": "Ivo Danihelka", "paperCount": 24, "citationCount": 9448, "hIndex": 18}], "abstract": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 44, "citation_count": 1763, "influential_paper_citations": 222, "is_open_access": false, "citations": ["c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "832fff14d2ed50eb7969c4c4b976c35776548f56", "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803", "929bef0066bad871ba971b673c053112d055d29f", "3a7895b17db0cda7bbf86bcda52c46a3e03b6ded", "c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3", "ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96", "8a5d0579590465494c9aba58a857af43b190b6a6", "cfadc984973aa0ee446157310c9045627de7d7d1", "020bb2ba5f3923858cd6882ba5c5a44ea8041ab6"], "references": ["c94b4e665199ff78aca4d3b56e18baddc2c51b75", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "c943afb901840397b72d78d81b65d78e8137121d", "9664ab17170e0442f35e27d8b3fac7398aa12d08", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "25badc676197a70aaf9911865eb03469e402ba57", "27e38351e48fe4b7da2775bf94341738bc4da07e", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "965a6451ba5eca2dc17d0bacac4cabb551edb050", "5ce004245beab1302dde2c08fc93454c3cbc18d0", "425931e434f6b370cc6cdd2db58873843def7d7f", "bdecdb4a23f1381b4ca87ec7e4129e50214ffca7", "17452b482179fd45aa958b1b8b440fa4c117be6d", "1ad7341f89cc3a09507a7e15a109954394d358a4", "6e785a402a60353e6e22d6883d3998940dcaea96", "de218b3b04ebc223f0740a809f8e1275b182bdc7", "eda6ad74daa02349feba9c63dc2cf22eece60f90", "cf7d7684600d3ebe916ca093eda123a9dad41459", "86c97493c2c7245afefd0336824eb48669923ddd", "c3577312cb178cc93459bda92e37076e1fa9af88", "1a8cc98ad8f023a9078a5ace98e777a88239854d", "044b2c29e0a54dc689786bd4d029b9ba6e355d58", "a6383f155fa9d3e9b15092bfefbf613f982eb263", "aed054834e2c696807cc8b227ac7a4197196e211", "8d46fb7cdeab9be071b0bc134aee5ef942d704b2", "d8a56b528822db211a5ad6dbe9e7d36fb77e486a", "a5eb96540ef53b49eac2246d6b13635fe6e54451", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "97901b8b7ece99faea15a301186f4a85a5160310", "a49498e51840165d55b6badd4b52e34d17860bc0", "7eb5d2f86438d1e9c72bff87894209f1ce84717a", "30110856f45fde473f1903f686aa365cf70ed4c7", "9438172bfbb74a6a4ea4242b180d4335bb1f18b7", "6a835df43fdc2f79126319f6fa033bb42147c6f6", "f70d2ff213a964621cb080f141ceed6359b84199", "4ade4934db522fe6d634ff6f48887da46eedb4d1", "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7", "98b4d4e24aab57ab4e1124ff8106909050645cfa", "4023ae0ba18eed43a97e8b8c9c8fcc9a671b7aa3"], "url": "https://www.semanticscholar.org/paper/c3823aacea60bc1f2cabb9283144690a3d015db5"}, "8829e3873846c6bbad5aca111e64f9d2c1b24299": {"id": "8829e3873846c6bbad5aca111e64f9d2c1b24299", "title": "Deep Sequential Neural Network", "authors": [{"authorId": "8905591", "name": "Ludovic Denoyer", "paperCount": 135, "citationCount": 6067, "hIndex": 31}, {"authorId": "1741426", "name": "P. Gallinari", "paperCount": 417, "citationCount": 8436, "hIndex": 45}], "abstract": "Neural Networks sequentially build high-level features through their successive layers. We propose here a new neural network model where each layer is associated with a set of candidate mappings. When an input is processed, at each layer, one mapping among these candidates is selected according to a sequential decision process. The resulting model is structured according to a DAG like architecture, so that a path from the root to a leaf node defines a sequence of transformations. Instead of considering global transformations, like in classical multilayer networks, this model allows us for learning a set of local transformations. It is thus able to process data with different characteristics through specific sequences of such local transformations, increasing the expression power of this model w.r.t a classical multilayered network. The learning algorithm is inspired from policy gradient techniques coming from the reinforcement learning domain and is used here instead of the classical back-propagation based gradient descent techniques. Experiments on different datasets show the relevance of this approach.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 19, "citation_count": 51, "influential_paper_citations": 6, "is_open_access": false, "citations": ["510e26733aaff585d65701b9f1be7ca9d5afc586", "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "e16cfe727e27be27115d0f842375c46e7e3f384b", "657329c633709dd1ac34a30d57341b186b1a47c2", "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "fc9d5be5e3f14b4c4b145b6c4bd96a9182f39fd2", "fba71eefd060e30f3516fdd46df9a191cd0aaaf7", "bf442ab269074665a68e4dbbe19e4efc97862541", "adec3702d810f1dfd0e5727406b16713be0a476f", "50f0e8ac834435c3f296ef8d3627f303623020ad"], "references": ["8a756d4d25511d92a45d0f4545fa819de993851d", "327d3df8ea2020882827d6bace1e26c9d24309c2", "9b6204e5ef66f82bee891b1336eb5ae1f64e8c99", "0d3233d858660aff451a6c2561a05378ed09725a", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "08fbf92e45d59bc85921fa91c35d2d4763738679", "c0b1589b029db1d0cbf37f24648f13c2c9281a3a", "4adae76303ff634ae66797221f0ba783983e5253", "ac850bbddef863bc88622df4d423420ba033365b", "398c296d0cc7f9d180f84969f8937e6d3a413796", "f2a0fbba89f0d18ea0abd29639d4e43babe59cf3", "bbbe546c3b20cf685f8c4594ef16870f830e3748", "b7deffb347eeaf4c7d0014e10cdd772e5f327dc1", "92d009217b100882376ae5c90217da2e92471ad7", "bcee7c85d237b79491a773ef51e746bbbcf48e35", "f6d8a7fc2e2d53923832f9404376512068ca2a57", "cd7f63611d59ae32fb0a029f978d4b0c1168adf3", "6f70bb581325440cddbbcf4ba0e7120357d5c7d9"], "url": "https://www.semanticscholar.org/paper/8829e3873846c6bbad5aca111e64f9d2c1b24299"}, "d862bda57dd0a51a1bff42b0e5fbeaaaa0c0d14f": {"id": "d862bda57dd0a51a1bff42b0e5fbeaaaa0c0d14f", "title": "Balanced K-Means for Clustering", "authors": [{"authorId": "39563202", "name": "Mikko I. Malinen", "paperCount": 10, "citationCount": 253, "hIndex": 6}, {"authorId": "1703213", "name": "P. Fr\u00e4nti", "paperCount": 284, "citationCount": 6655, "hIndex": 41}], "abstract": null, "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 29, "citation_count": 108, "influential_paper_citations": 14, "is_open_access": true, "citations": ["657329c633709dd1ac34a30d57341b186b1a47c2", "665b38fe83b1bc155de3319a72ca4e6a6696f9f2", "1824de2296b9fa8a2ac65131213e8359ddd85deb", "fd1201b44d28f6c2d722c8c4faada6bb82d79ebf", "3b9efab2114a3456dccb9d625ca732100d18ba74", "3b7ee35ff4e7921434e41e1c074cc9fc811a3e01", "bfbe59f99f487fd8d298fa30e3d46fa4af495f2a", "1db384da76b13b75f4907bc1e81a5aa7e3ffb229", "c4f43f582b579097779675b472b75272c43c67ec", "686df0720dc54c476dcb4d69584ef3c530d85f7f"], "references": ["c4f5018b9e491c80e031d616e8fa166c6104bdb7", "7d0579df611da08a1627ecfe4cd3ec2a5add4134", "4eb77524821a484df8925a4d26daca1ea53b1ccf", "1b0119589b8f7e6d98d896d33bf2e6765c22ac3e", "4ff88feaa00074edd8c20d28f6cf0ed04f15c5ae", "6fb6ae51f0282bf7453e22c5796dbe3fef9a940d", "7e2e8060c04643ad157164ba0580f8ddeb770b7e", "d55b47c5c8773f688dd009d6b45720109a9c81e0", "5fe5a0d9af66326e19f008281707ccef93ebc542", "0ff0768a06042b868e50f22eff118fc0c2e0f8b7", "59341ef60c31a903796d619e5a527da96c4bc359", "18c3a48168e1e53f656cc17dddbbd4efb30d3306", "5e0c61b7ee4a2de183a197f32c5013ad109531fa", "c37a9ced7fb89e999e66f41eb862934dd2d7791e", "f22046b49561c37de844de62f2c70077e2aeee92", "877de49fbfef9e47bb4da829d156f451e007b73a", "1b443db0552343e4d02ad24112dc0667a12335a7", "e91865ab4b7b1b14e81327094a3793292e50d7df", "c82e5c28fb271280a628588663b9eb66b262b6b0", "939c8053f190c5bcfc6b7d4b15c8f09a6b6cfeff", "a7af95f64fa19ebdf29e25e17071c6c0607a3392", "d7d385f45c096082812deb1623e5af2c2915b4a9", "ecadeb93378d7911c2f7b9bd83a8af55d7fa9e06", "efb1cf7e8151b6d4fb4d77722e0c343fbb0c73c4", "36278bf6919c6dced7d16dc0c02d725e1ed178f8", "13b3662b22d036b9946165c5982798cd4c78922e", "ad54c929e36b806c9c8dd999e8e3bdd8ee06f871", "d8bb567103352bf5746c61a445d7c3700430b793", "ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed"], "url": "https://www.semanticscholar.org/paper/d862bda57dd0a51a1bff42b0e5fbeaaaa0c0d14f"}, "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d": {"id": "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "title": "Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning", "authors": [{"authorId": "1979489", "name": "Kyunghyun Cho", "paperCount": 326, "citationCount": 88663, "hIndex": 80}, {"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}], "abstract": "Many state-of-the-art results obtained with deep networks are achieved with the largest models that could be trained, and if more computation power was available, we might be able to exploit much larger datasets in order to improve generalization ability. Whereas in learning algorithms such as decision trees the ratio of capacity (e.g., the number of parameters) to computation is very favorable (up to exponentially more parameters than computation), the ratio is essentially 1 for deep neural networks. Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation required, by activating some parameters and computation \"on-demand\", on a per-example basis. In this note, we propose a novel parametrization of weight matrices in neural networks which has the potential to increase up to exponentially the ratio of the number of parameters to computation. The proposed approach is based on turning on some parameters (weight matrices) when specific bit patterns of hidden unit activations are obtained. In order to better control for the overfitting that might result, we propose a parametrization that is tree-structured, where each node of the tree corresponds to a prefix of a sequence of sign bits, or gating units, associated with hidden units.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2014, "reference_count": 28, "citation_count": 28, "influential_paper_citations": 2, "is_open_access": false, "citations": ["fdacf2a732f55befdc410ea927091cad3b791f13", "510e26733aaff585d65701b9f1be7ca9d5afc586", "3e70bbe6c4cd98d66599db709e32b748f184a2d4", "657329c633709dd1ac34a30d57341b186b1a47c2", "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "016368185723d0ec99aafa4b5927300590d0647f", "bf442ab269074665a68e4dbbe19e4efc97862541", "837ac4ed6825502f0460caec45e12e734c85b113", "fb01415a0decfa3f3d6339930e95028ae1ff4170", "6428fa299a76ccce775b17ea6f3401ef0cfccf04"], "references": ["8dcb21c00f2d42b16d86a91dec5deab53b87cc6d", "c965bac486a714d47a6362248f0a959c77622738", "018300f5f0e679cee5241d9c69c8d88e00e8bf31", "c12fefe264e42e77f1275ce56fb3e905347761a3", "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864", "2319a491378867c7049b3da055c5df60e1671158", "62c76ca0b2790c34e85ba1cce09d47be317c7235", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "72e93aa6767ee683de7f001fa72f1314e40a8f35", "3127190433230b3dc1abd0680bb58dced4bcd90e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e33cbb25a8c7390aec6a398e36381f4f7770c283", "5bdfd78fb2285b9306e93bd3a4b534d19bf55f06", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "f8c8619ea7d68e604e40b814b40c72888a755e95", "522e90b9fccfd3c1c0603359eb04757d770c1ab5", "be9a17321537d9289875fe475b71f4821457b435", "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "3e708b0cd19818e0c7408765dd922835661f8a24", "9c34be4a906a86b2ebc09078d80247227cd54453", "0eb2e4a205a628ab059cab41d3b772f614ad29f2", "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "e60ff004dde5c13ec53087872cfcdd12e85beb57", "7924c809b75c1ff9cdb3aac3ecd4d8a49d738e9e", "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "6a923c9f89ed53b6e835b3807c0c1bd8d532687b"], "url": "https://www.semanticscholar.org/paper/c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d"}, "44ddac48353ead135eef4096859956eaa31be2a5": {"id": "44ddac48353ead135eef4096859956eaa31be2a5", "title": "Learning Factored Representations in a Deep Mixture of Experts", "authors": [{"authorId": "2060028", "name": "D. Eigen", "paperCount": 20, "citationCount": 10737, "hIndex": 11}, {"authorId": "1706809", "name": "M. Ranzato", "paperCount": 98, "citationCount": 33437, "hIndex": 54}, {"authorId": "1701686", "name": "Ilya Sutskever", "paperCount": 101, "citationCount": 254762, "hIndex": 63}], "abstract": "Mixtures of Experts combine the outputs of several \"expert\" networks, each of which specializes in a different part of the input space. This is achieved by training a \"gating\" network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (\"where\") experts at the first layer, and class-specific (\"what\") experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2013, "reference_count": 10, "citation_count": 173, "influential_paper_citations": 16, "is_open_access": false, "citations": ["118fae4b4d07453561f1eded88654f812c7c61ec", "7cfa5c97164129ce3630511f639040d28db1d4b7", "fdacf2a732f55befdc410ea927091cad3b791f13", "510e26733aaff585d65701b9f1be7ca9d5afc586", "af8a8dcb74561d52d904f7bc4afcc747e079b702", "ef9ddbc35676ce8ffc2a8067044473727839dbac", "004acfec16c36649408c561faa102dd9de76f085", "3e70bbe6c4cd98d66599db709e32b748f184a2d4", "657329c633709dd1ac34a30d57341b186b1a47c2", "aaed2a884af95852580fdedda4ea768f2effeb46"], "references": ["62c76ca0b2790c34e85ba1cce09d47be317c7235", "72d32c986b47d6b880dad0c3f155fe23d2939038", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "a4eb9f4fad5c5a1935c6d0532e2c765ee29b0b37", "5a47ba057a858f8c024d2518cc3731fc7eb40de1", "0fe51325d0ad3ab7b774fe07043bc6d36e24f66f", "7bb0f5f20883db8c69b53ba4e52eded325b25f43", "f6d8a7fc2e2d53923832f9404376512068ca2a57", "c8d90974c3f3b40fa05e322df2905fc16204aa56"], "url": "https://www.semanticscholar.org/paper/44ddac48353ead135eef4096859956eaa31be2a5"}, "62c76ca0b2790c34e85ba1cce09d47be317c7235": {"id": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation", "authors": [{"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}, {"authorId": "2065623360", "name": "Nicholas L\u00e9onard", "paperCount": 7, "citationCount": 4021, "hIndex": 4}, {"authorId": "1760871", "name": "Aaron C. Courville", "paperCount": 252, "citationCount": 98036, "hIndex": 82}], "abstract": "Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2013, "reference_count": 19, "citation_count": 1778, "influential_paper_citations": 286, "is_open_access": false, "citations": ["47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "1882f194cb43828852cc052887671e55a80f945a", "501c02c7caa7fc2c7077405299b4cbe7d294b170", "b44bb1762640ed72091fd5f5fdc20719a6dc24af", "370b680057a6e324e67576a6bf1bf580af9fdd74", "31b38a19d87711489786ad54a5a00d5f0b2ead43", "657329c633709dd1ac34a30d57341b186b1a47c2", "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa", "04e283adccf66742130bde4a4dedcda8f549dd7e"], "references": ["5df0a0e9ceec70a9321b0555288222bf53216342", "72d32c986b47d6b880dad0c3f155fe23d2939038", "b7b915d508987b73b61eccd2b237e7ed099a2d29", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "1366de5bb112746a555e9c0cd00de3ad8628aea8", "f8c8619ea7d68e604e40b814b40c72888a755e95", "67107f78a84bdb2411053cb54e94fa226eea6d8e", "a538b05ebb01a40323997629e171c91aa28b8e2f", "cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a", "843959ffdccf31c6694d135fad07425924f785b1", "0f6089fb276a8ab926b735b9043263362bf19985", "4c915c1eecb217c123a36dc6d3ce52d12c742614", "ae3fe34be9230c98b04d68b4621c89b7dbc2d717", "4d92df4a844c94fbb31b95157488e4b562b4f681", "b13813b49f160e1a2010c44bd4fb3d09a28446e3", "f7410cd1afeba276f4479e8b5f04f12530b48d83", "052b1d8ce63b07fec3de9dbb583772d860b7c769"], "url": "https://www.semanticscholar.org/paper/62c76ca0b2790c34e85ba1cce09d47be317c7235"}, "4c0d2d4269895eb367d7b0d38d9de3e99bf3f3ae": {"id": "4c0d2d4269895eb367d7b0d38d9de3e99bf3f3ae", "title": "Sparse Nonnegative Matrix Factorization for Clustering", "authors": [{"authorId": "143882848", "name": "Jingu Kim", "paperCount": 20, "citationCount": 1417, "hIndex": 13}, {"authorId": "1685928", "name": "Haesun Park", "paperCount": 242, "citationCount": 11548, "hIndex": 51}], "abstract": "Properties of Nonnegative Matrix Factorization (NMF) as a clustering method are studied by relating its formulation to other methods such as K-means clustering. We show how interpreting the objective function of K-means as that of a lower rank approximation with special constraints allows comparisons between the constraints of NMF and K-means and provides the insight that some constraints can be relaxed from K-means to achieve NMF formulation. By introducing sparsity constraints on the coefficient matrix factor in NMF objective function, we in term can view NMF as a clustering method. We tested sparse NMF as a clustering method, and our experimental results with synthetic and text data shows that sparse NMF does not simply provide an alternative to K-means, but rather gives much better and consistent solutions to the clustering problem. In addition, the consistency of solutions further explains how NMF can be used to determine the unknown number of clusters from data. We also tested with a recently proposed clustering algorithm, Affinity Propagation, and achieved comparable results. A fast alternating nonnegative least squares algorithm was used to obtain NMF and sparse NMF.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2008, "reference_count": 19, "citation_count": 221, "influential_paper_citations": 19, "is_open_access": false, "citations": ["c014c79fa37cb8dc4565d5db9c4879d98d5f904b", "944ef766160f1f937ab0e9b136b92547c8c9c9d8", "5fa927469e070edd17202f356cc50e332970c509", "b13266190d4cce43d3a38af683dd5939efd3155d", "92b18501fb7989ffa277805c42e0cd396dd2423b", "657329c633709dd1ac34a30d57341b186b1a47c2", "a3ccf7fa5c130c8bcd20cbcd356ad7a47cdd4296", "f5cb8683245eba3e874fe53e1cca17e66d06423d", "f88a033bdde22d99529b7527077da426c513e883", "50877a0e110adcb71ed6d98878a655e362fc4578"], "references": ["55d0e5e82095ce4f9a79591221c3a8bdc78e3ac1", "dd5b76e3b750dab03ec7f4c40df99b173100392d", "ca839260a9e13f21fff53b7227d6d363cf3d4735", "a74534569c5532ea62da572e3b617dedc825b262", "4fc1bf60275c419eeb85e28793305a789dca4717", "b02f1e8bfbf5bcf449f8ebbc6d69f864789b1d1c", "ede3af3637977988b8cbb330c294183e919b7d5a", "8467c65182dcbd3ea76aa31c2074930b5e3c1e05", "eef58d85880ab19737f4880fcff19fdc7ec1db99", "66a6dde6a6a20f77ce52cb2464a52777837bd81e", "29c143fc694199ce6259e6e3f423e57a616e0bf5", "db53b8465a2d6b9dcba0a9c2ec7f10712c826306", "01cbff216f2888f96151fb490338af40a09a0c30", "29bae9472203546847ec1352a604566d0f602728", "34306886f231d629c0b72d4c52ab0e9b1515b42f", "3b4b6b3b6f13f2de00a213a822e7c005e034adae", "6ff715c841b0e7d0642649ecfb22a5c033414230", "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "c1eebff5e1b41283d45b0aa2d3770a2021efa672"], "url": "https://www.semanticscholar.org/paper/4c0d2d4269895eb367d7b0d38d9de3e99bf3f3ae"}, "4fc1bf60275c419eeb85e28793305a789dca4717": {"id": "4fc1bf60275c419eeb85e28793305a789dca4717", "title": "On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering", "authors": [{"authorId": "1737469", "name": "C. Ding", "paperCount": 290, "citationCount": 32377, "hIndex": 68}, {"authorId": "143644849", "name": "Xiaofeng He", "paperCount": 229, "citationCount": 8956, "hIndex": 37}], "abstract": "Current nonnegative matrix factorization (NMF) deals with X = FG type. We provide a systematic analysis and extensions of NMF to the symmetric W = HH , and the weighted W = HSH . We show that (1) W = HH is equivalent to Kernel K-means clustering and the Laplacian-based spectral clustering. (2) X = FG is equivalent to simultaneous clustering of rows and columns of a bipartite graph. Algorithms are given for computing these symmetric NMFs.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2005, "reference_count": 32, "citation_count": 812, "influential_paper_citations": 57, "is_open_access": false, "citations": ["e791c35e79d2e61a9760d841e8a1aa860c072de3", "fddc613a2a208f241b38bc41f62b4fc4db7217c7", "edde9337e61139fa3b24aac3dd685d33d6184650", "1e1d1c9a3cc9991bf64b075be899bff7440f895a", "35ac2d4689123c13b9441a6305356eb8d8804e69", "efa13986a1a4df6fcc379e3b40701da07b057576", "2c024a6b7d5ab679f88202e413d5399dc8414257", "fb25566db4ff4d676dc4aec99a0122966e3e90b0", "0bc4ffe87343f3fb37c0c5c7c2115e74490c2d05", "e4e35f7150db8c3828dd6089c6dc0dc7f0b56bdd"], "references": ["eda90bd43f4256986688e525b45b833a3addab97", "12e46e8bebeb36875d19bc6d61cde3531bb39ca5", "1aa082404c2bac38b0553456308b66028eb196e6", "dbdb70ce52a3314bad946329ef7d399f0986fc97", "05ee6526b5a03017507d763c6c6aeb4391182aba", "66a6dde6a6a20f77ce52cb2464a52777837bd81e", "19b945138e4e6f70ecfaef7d6b1292ab687b3a00", "db53b8465a2d6b9dcba0a9c2ec7f10712c826306", "6ebb015ac7f7872ecadd75b837e859621abd0751", "01cbff216f2888f96151fb490338af40a09a0c30", "f2ab48a33534a944972e48ce4165bf6f84fa6206", "29bae9472203546847ec1352a604566d0f602728", "b94c7ff9532ab26c3aedbee3988ec4c7a237c173", "753aaa1a02412b5d2d916b5b6197f8bb5a52f59e", "36278bf6919c6dced7d16dc0c02d725e1ed178f8"], "url": "https://www.semanticscholar.org/paper/4fc1bf60275c419eeb85e28793305a789dca4717"}, "1b443db0552343e4d02ad24112dc0667a12335a7": {"id": "1b443db0552343e4d02ad24112dc0667a12335a7", "title": "Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres", "authors": [{"authorId": "144205696", "name": "A. Banerjee", "paperCount": 252, "citationCount": 23325, "hIndex": 49}, {"authorId": "34724702", "name": "Joydeep Ghosh", "paperCount": 536, "citationCount": 30353, "hIndex": 67}], "abstract": "Competitive learning mechanisms for clustering, in general, suffer from poor performance for very high-dimensional (>1000) data because of \"curse of dimensionality\" effects. In applications such as document clustering, it is customary to normalize the high-dimensional input vectors to unit length, and it is sometimes also desirable to obtain balanced clusters, i.e., clusters of comparable sizes. The spherical kmeans (spkmeans) algorithm, which normalizes the cluster centers as well as the inputs, has been successfully used to cluster normalized text documents in 2000+ dimensional space. Unfortunately, like regular kmeans and its soft expectation-maximization-based version, spkmeans tends to generate extremely imbalanced clusters in high-dimensional spaces when the desired number of clusters is large (tens or more). This paper first shows that the spkmeans algorithm can be derived from a certain maximum likelihood formulation using a mixture of von Mises-Fisher distributions as the generative model, and in fact, it can be considered as a batch-mode version of (normalized) competitive learning. The proposed generative model is then adapted in a principled way to yield three frequency-sensitive competitive learning variants that are applicable to static data and produced high-quality and well-balanced clusters for high-dimensional data. Like kmeans, each iteration is linear in the number of data points and in the number of clusters for all the three algorithms. A frequency-sensitive algorithm to cluster streaming data is also proposed. Experimental results on clustering of high-dimensional text data sets are provided to show the effectiveness and applicability of the proposed techniques.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2004, "reference_count": 59, "citation_count": 100, "influential_paper_citations": 10, "is_open_access": false, "citations": ["11de3f9770b08484b28597deb17714fb107caafe", "aa6ee637e9f86148dadbc849983159ea24d2d204", "8755ab591341f05999808a755bc727d6b9217034", "657329c633709dd1ac34a30d57341b186b1a47c2", "de3ddfdb3c8c995cff057e010db6ac70e06ef5b5", "9a6824ffa600f06aca27b9dd54fa8d75a5cf4a16", "dd309f8e1bea5a96817a0334d603400d6428081b", "70a8bcf0bd5cdc79c42b20083340de3f4c674aef", "d862bda57dd0a51a1bff42b0e5fbeaaaa0c0d14f", "b9e94fabb8f9b7ea9b20b092721cac1af1258c38"], "references": ["495bbf69d338e33c3217c272da9eb4fec0a66212", "155d81d9f764de34c7234f0e3dc37a5bd297edee", "99d3f99514e3c6284b2094acf9a0eeded51759b8", "ccd64c17c811d0d60672ac1a4a4bbb222debfa5d", "d899da3b799f5b0d8a9512ef7ec97f7e749e3887", "f4f3a10d96e0b6d134e7e347e1727b7438d4006f", "621c2bd43c1ba86ca12bd44a27b5062db5a87f8f", "36e2130daa0a2837264eb36893fdb4096dc88ef6", "939c8053f190c5bcfc6b7d4b15c8f09a6b6cfeff", "1cb4265110d1aa009fddbba5ec9e400befc411c0", "5f4ef51ae312a815db6946751219bdcfee443d45", "764fb39acf057b52f9e1221f630e701ff56ea3b9", "1b7df0c67df8ce681d7c36dd9d9424a91514db64", "5eb75ac359fbd32c378a783741d7543186fe58d8", "ecadeb93378d7911c2f7b9bd83a8af55d7fa9e06", "e20ad680666a4e02a1b0a99d0b6c974f7390bc64", "76b68dd26e0e2cabcbd376deb76e998eeb323d7a", "8948ed4d2866da6c47120210b6adf981159164f2", "e45c2420e6dc59ba6d357fb0c996ebf43c861560", "4ce8bc485df9ac987f18d99c7af1d95f9cbea6b2", "dddd774bc73acf5ed3d360e291b3265bf069b8b9", "9f87a11a523e4680e61966e36ea2eac516096f23", "0e08260df8ecfd084293efd8bd516eca9e7c1a1c", "385197d4c02593e2823c71e4f90a0993b703620e", "d5051890e501117097eeffbd8ded87694f0d8063", "56931d2df145459fa60b2e7863afbf06a4230a5a", "3a5fa1ea14cea5e55e4e1f844f78332fccefa285", "6ecb68885ba2a9db2bf5b30a9b7a8cca03c5f265", "c4c631b245971eb5fde9cbd6d3575e4ac4e41c49", "c073564b48dd5671284d7ade46e78c49fabd6577", "62f4d89a3c1441b47170c7e1380137fb388d0799", "2f3876251cc2f03a06190ef0a47544042da416aa", "e96d22faa4a167a6f7cb8ea4e3aa5bb2ca117bf7", "961e2156d523e3901c491cc2a1f65764c976fc44", "7c5a3401299fe9b98731f92e52e4208164ff9a78", "1210f40af80dc660dc9acf11cad3226ac805f948", "934a841acb7fa0dde3c2f8968d0fb810224cd850", "6b13011ff172b1f6321a4de018e841733565f671", "1b52f9413fea90069a305ba98dd0566b47a7d609", "b027d77a20e713e7244db0f92e73dc5370e94a82", "38e32ba4311f4cf5a133b919baaa8c33fc7ecbdc", "7dbdb4209626fd92d2436a058663206216036e68", "63f5a3a89a94fd1c672317f816cc49bdbdb0697d", "c3da7bdcbf7cba94ce64abc3aff0f9b947a5cab9", "a03c9127dc782129a793d731fe73f33a7a844ad8", "aa4bddbd10eafd8e1b54338517eedfee408f03ae", "8684d6daae039c76a48146c768d01550b64b2f1f", "5aca8d0b21dd0b05191100f2f345bdc7bb1f2fe9", "bb4bad84a2fd896edfa4f5c22061b2913fec500d", "6c192b91f4b4ac35ae8385fa190fdfc146f419b8", "37e44d1de8003d8394d158ec6afd1ff0e87e595b", "d36efb9ad91e00faa334b549ce989bfae7e2907a", "50a42ed2f81b9fe150883a6c89194c88a9647106", "f059510bc8c276587cc50b984b5f0d0897719c58", "3d438966ad4c51d38e87fac7d4c4475837a5e8f3"], "url": "https://www.semanticscholar.org/paper/1b443db0552343e4d02ad24112dc0667a12335a7"}, "6fb07b90b7fd2785ffec0da1069e75c53f7313c2": {"id": "6fb07b90b7fd2785ffec0da1069e75c53f7313c2", "title": "Algorithms for Non-negative Matrix Factorization", "authors": [{"authorId": "2115651440", "name": "Daniel D. Lee", "paperCount": 26, "citationCount": 21851, "hIndex": 14}, {"authorId": "144924970", "name": "H. Seung", "paperCount": 201, "citationCount": 39279, "hIndex": 67}], "abstract": "Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the Expectation-Maximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2000, "reference_count": 17, "citation_count": 7971, "influential_paper_citations": 1327, "is_open_access": false, "citations": ["657329c633709dd1ac34a30d57341b186b1a47c2", "c9b30f6bda58c2ec2a994a56e387221a5765a1ba", "13c830c9287e3d6464fef3d479f4e1d49ce372d7", "a3ef5a321876738a6b257de5e1eebc4a8aa5b907", "a2881fa91a7dd0a9360eb3cdfd0ffbf6421c5e41", "3443efc855cebd17d1512d1a703b6e9ee2e4da8b", "3b2b0547af85be326302198a40cf434614c14f96", "4e7b11e07aaebe17de0ff3a720a6f9f6b70c7bc2", "21a424eea89ebb5cfe95e9db8401b3881f06afad", "0e324d586b31ecb60258f91481e7a849ee4dd028"], "references": ["29bae9472203546847ec1352a604566d0f602728", "552e26d425c6cd6d79e428716adcd2897ee62e0e", "f7463aad3b5182820995101602788ea4c9bb4d9f", "6982720b38510a887d79013e03ec2f9a5147edf2", "98eed3f082351c4821d1edb315846207a8fefbe9", "4cf7e8e452c31ac2adb736ec006eccd3a26bea36", "8e7cb3277364dd536f5482f693425cbbcaddbc7f", "4dc175e8f6e7ca5c40ffd6fb9c6b92323bf7daf2", "ff1152582155acaa0e9d0ccbc900a4641504256d", "baf4491be1f4c1de7ecb03bf81325f6f09bda9c6", "c564aa7639a08c280423489e52b6e32055c9aa7f", "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f", "16b4fd36ff5ca603fed13a326054cd0373ea442c", "a41fd7e04f18ee369c0a48a1e83b05eb6b1fded3", "d36efb9ad91e00faa334b549ce989bfae7e2907a", "30ad4474c7d6e9fd68b3e0fa2db235f1c8bc32f0", "eb5f518e9fdd764835f719bfef59b1276cb562b3"], "url": "https://www.semanticscholar.org/paper/6fb07b90b7fd2785ffec0da1069e75c53f7313c2"}, "2352d9105de31032538900dfb2ce7c95f6402963": {"id": "2352d9105de31032538900dfb2ce7c95f6402963", "title": "Convergence Properties of the K-Means Algorithms", "authors": [{"authorId": "52184096", "name": "L. Bottou", "paperCount": 194, "citationCount": 90249, "hIndex": 70}, {"authorId": "1751762", "name": "Yoshua Bengio", "paperCount": 842, "citationCount": 430407, "hIndex": 189}], "abstract": "This paper studies the convergence properties of the well known K-Means clustering algorithm. The K-Means algorithm can be described either as a gradient descent algorithm or by slightly extending the mathematics of the EM algorithm to this hard threshold case. We show that the K-Means algorithm actually minimizes the quantization error using the very fast Newton algorithm.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 1994, "reference_count": 12, "citation_count": 489, "influential_paper_citations": 30, "is_open_access": false, "citations": ["848a25a41cba56ca180ca79c6ba3470cc3b8f143", "26f178dbb00630ce19cccb9840ea12dbe31801be", "f354310098e09c1e1dc88758fca36767fd9d084d", "b452a856a3e3d4d37b1de837996aa6813bedfdcf", "6a97adbfaeecd5c1eeb3ae9c76a3842d4858cc06", "3f397e7dab0e253e0859d18bb5711b5471c657fe", "7736b3cba7b2a30e5e817f8cbb6fa02c4e7c7cdf", "aeb7dba8de54e2d8a965db5f0d325e6f7ffccfea", "7b0db6135b8dd3e2a9efa86163e91c0cd0fdf660", "a27e8212035e26e68128c893e5b0e0148ddd177d"], "references": ["4cd32bf75a88c1987e9b666969f2c422fbf91bd5", "fedbdcdead16c7b5cdb6dcc68bc5f4af3085f64b", "59fa47fc237a0781b4bf1c84fedb728d20db26a1", "cde2937cb41cf461f624ded2012743ac4624eca8", "713f55820406c9540428ae5ec2a0428010d6800c", "015e5c48abbd59309e6986aaa94550e40562f100", "10055eb6f2f711a36d9aa8f759d3b3f01ebddb5d", "d36efb9ad91e00faa334b549ce989bfae7e2907a", "ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed"], "url": "https://www.semanticscholar.org/paper/2352d9105de31032538900dfb2ce7c95f6402963"}, "71b6394ad5654f5cd0fba763768ba4e523f7bbca": {"id": "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "title": "Longformer: The Long-Document Transformer", "authors": [{"authorId": "46181066", "name": "Iz Beltagy", "paperCount": 46, "citationCount": 5406, "hIndex": 22}, {"authorId": "39139825", "name": "Matthew E. Peters", "paperCount": 64, "citationCount": 15616, "hIndex": 26}, {"authorId": "2527954", "name": "Arman Cohan", "paperCount": 73, "citationCount": 5139, "hIndex": 24}], "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 59, "citation_count": 1231, "influential_paper_citations": 324, "is_open_access": false, "citations": ["39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "fdacf2a732f55befdc410ea927091cad3b791f13", "35a9749df07a2ab97c51af4d260b095b00da7676", "7e9ff94476f41041c75e253e84f487db00e9c861", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "800cfb3d23115cdcd4d114234b65bbdf2080f798", "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "1f133158a8973fb33fea188f20517cd7e69bfe7f"], "references": ["657329c633709dd1ac34a30d57341b186b1a47c2", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "63857190aaf5aab1d94b54bb257b7b03b8cb5a50", "56676aef356ebb13cba77fc9e4d70760fbc151f5", "c6c547e5ecbb8e26bae02fd6f37154145ae0053b", "ac87ce22d3b8cd2668793d93ce7b361cda7193c0", "8f0e74dfc845318b84fe8a8166db0957c6e2c24e", "40c71c9ab559c3a39f1d20f221c8c02e6f394c4b", "748629cb0b8e5a5708e1c6605f71b36eb525a3ce", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "f51497f463566581874c941353dd9d80069c5b77", "2c88d7486f9871cb741ba3c7076b8adbb7fd5b68", "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "5534d774a06039e13b72876c21d39949132b512b", "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "6c8503803760c5c7790f72437d0f8b874334e6f0", "0feea94f89d395436bf41bd10c797447eecbc128", "5665805becad6c87b194b260f2270d86d560bd3f", "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "1be21e96eaac56f626e7b41c1f332b6b46131608", "42e66d52cfa212bafc96dcf4b2abe1c5c403e53c", "d78aed1dac6656affa4a04cbf225ced11a83d103", "127ffe6d21b75bd41dd808e3313bc392b9428346", "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "95a251513853c6032bdecebd4b74e15795662986", "fc089a09074c84979d1f34e89341318a5bc26d3d", "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "f4238bd2385a52413ccbacfd9e409a650235bd13", "21da617a0f79aabf94272107184606cefe90ab75", "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "ef523bb9437178c50d1b1e3e3ca5fb230ab37e3f", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "9405cc0d6169988371b2755e573cc28650d14dfe", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "22655979df781d222eaf812b0d325fa9adf11594", "d7b6753a2d4a2b286c396854063bde3a91b75535", "e6566ece21f6637c515fe9969f9d1ec6cca6d36c", "853d4d94651c6d9f8ed4d114e1eb21f15f786daa", "3febb2bed8865945e7fddc99efd791887bb7e14f", "df013a17ab84d5403361da4538a04d574f58be83", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "3c78c6df5eb1695b6a399e346dde880af27d1016", "7d5cf22c70484fe217936c66741fb73b2a278bde", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "104715e1097b7ebee436058bfd9f45540f269845", "a07609c2ed39d049d3e59b61408fb600c6ab0950", "df0402517a7338ae28bc54acaac400de6b456a46", "942deb7d865b7782c03176d95e3a0d56cb71009e", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "0e6824e137847be0599bb0032e37042ed2ef5045", "cea967b59209c6be22829699f05b8b1ac4dc092d", "f8cdf754fb7c08caf6e2f82b176819230910be5b", "649d03490ef72c5274e3bccd03d7a299d2f8da91"], "url": "https://www.semanticscholar.org/paper/71b6394ad5654f5cd0fba763768ba4e523f7bbca"}, "39ca8f8ff28cc640e3b41a6bd7814ab85c586504": {"id": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "authors": [{"authorId": "2578924", "name": "Xizhou Zhu", "paperCount": 31, "citationCount": 4647, "hIndex": 15}, {"authorId": "145499378", "name": "Weijie Su", "paperCount": 9, "citationCount": 2256, "hIndex": 4}, {"authorId": "152309485", "name": "Lewei Lu", "paperCount": 10, "citationCount": 2287, "hIndex": 5}, {"authorId": "2183101614", "name": "Bin Li", "paperCount": 81, "citationCount": 2679, "hIndex": 20}, {"authorId": "93768810", "name": "Xiaogang Wang", "paperCount": 87, "citationCount": 8063, "hIndex": 35}, {"authorId": "3304536", "name": "Jifeng Dai", "paperCount": 68, "citationCount": 19356, "hIndex": 32}], "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 48, "citation_count": 1308, "influential_paper_citations": 302, "is_open_access": false, "citations": ["7519a1e9e7371df79bd8a21cee871feb0ec597a5", "658a017302d29e4acf4ca789cb5d9f27983717ff", "7d1ff4ac2390759cbe60dd46b2b9bcabd4a90db4", "1e88d5afe19aea324d33541f60a90b7036894c32", "800cfb3d23115cdcd4d114234b65bbdf2080f798", "152954530e24e701a9a1132eba602504e718a9f2", "a824c6e214dd0118f70af8bb05d67d94a858d076", "0357156aef567fb5b709222894ddea1ce5d4e721", "dd2819016c6bf244c39b3e6707b60389bbdbcd21", "f75cddf2d42ed01b34686704eb3504becef67442"], "references": ["7e5709d81558d3ef4265de29ea75931afeb1f2dd", "657329c633709dd1ac34a30d57341b186b1a47c2", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "6f68e1bb253925d8431588555d3010419f322e04", "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "56676aef356ebb13cba77fc9e4d70760fbc151f5", "baed71eed57ad462f3ab138d4b1700a738cd5414", "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "3230e2d6b4671cc03974af2219c6d3270e6fac70", "0de83cb12a3db3849fde4aaaa3016aac055fef0b", "34a4e6818d680875ff0bef9a76de0376118446d1", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "448529da2bf004cf79084401ad3cbd6b511e4969", "41c67d04be2d1632c0d3b0880c21c9fe797cdab8", "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "72564a69bf339ff1d16a639c86a764db2321caab", "a3b0009cd0dc71a557fb92b748f4f56411b7e2fe", "366244acdd930e488ae224ab6e2a92dc24aa7e06", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "21da617a0f79aabf94272107184606cefe90ab75", "b5375995ab8d679a581ffcc2f2e8d3777d60324b", "18d41e3bd94cf38736e37580912c3b4ba56f08d5", "e2751a898867ce6687e08a5cc7bdb562e999b841", "4b344351fe43544317efc9adaebe6791c4242814", "5132500b23d2da47129b3f4f68dd30947a29e502", "987b2db58fbe0bda771f11a046cd23de1ce92b39", "d243d007f9675325cbe5bc7852446851ed3d9bcb", "7536bce1007a765fd097a7cc8ea62208a8c89b85", "f550013aebb9e675987fb9a393f399cb44723ce1", "5cc22f65bf4e5c0aa61f3139da832d3a946e15cf", "1db9bd18681b96473f3c82b21edc9240b44dc329", "7570afa31c68e24fce1342b7d67c591787219bc1", "8899094797e82c5c185a0893896320ef77f60e64", "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "4a73a1840945e87583d89ca0216a2c449d50a4a3", "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "71b7178df5d2b112d07e45038cb5637208659ff7", "d2c733e34d48784a37d717fe43d9e93277a8c53e"], "url": "https://www.semanticscholar.org/paper/39ca8f8ff28cc640e3b41a6bd7814ab85c586504"}, "3fbf6339273c50b04e886fa9bd4ad18c952a683d": {"id": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers", "authors": [{"authorId": "1805203", "name": "K. Choromanski", "paperCount": 120, "citationCount": 2852, "hIndex": 27}, {"authorId": "52314889", "name": "Valerii Likhosherstov", "paperCount": 20, "citationCount": 718, "hIndex": 7}, {"authorId": "35363891", "name": "David Dohan", "paperCount": 25, "citationCount": 3672, "hIndex": 14}, {"authorId": "32725720", "name": "Xingyou Song", "paperCount": 30, "citationCount": 1008, "hIndex": 10}, {"authorId": "3071104", "name": "Andreea Gane", "paperCount": 12, "citationCount": 920, "hIndex": 6}, {"authorId": "2227764", "name": "Tam\u00e1s Sarl\u00f3s", "paperCount": 45, "citationCount": 3991, "hIndex": 24}, {"authorId": "2052793706", "name": "Peter Hawkins", "paperCount": 5, "citationCount": 936, "hIndex": 4}, {"authorId": "29827891", "name": "Jared Davis", "paperCount": 39, "citationCount": 1541, "hIndex": 12}, {"authorId": "1579862074", "name": "Afroz Mohiuddin", "paperCount": 8, "citationCount": 1112, "hIndex": 4}, {"authorId": "40527594", "name": "Lukasz Kaiser", "paperCount": 70, "citationCount": 71247, "hIndex": 29}, {"authorId": "2636941", "name": "David Belanger", "paperCount": 58, "citationCount": 2901, "hIndex": 20}, {"authorId": "2654847", "name": "Lucy J. Colwell", "paperCount": 64, "citationCount": 3656, "hIndex": 26}, {"authorId": "145689461", "name": "Adrian Weller", "paperCount": 140, "citationCount": 4425, "hIndex": 29}], "abstract": "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 63, "citation_count": 573, "influential_paper_citations": 103, "is_open_access": false, "citations": ["c8b25fab5608c3e033d34b4483ec47e68ba109b7", "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "dc0102a51a9d33e104a4a3808a18cf17f057228c", "6c22336873706b1cf5205ac6bd2432aa69d97821", "a96b482cab45823bc1a095baecb5d5b05ffd9284", "586615b95d9942a79e711b043044a16b561dc8af", "37187ceb6008d49e1758bab0d4f86bf39aa175cf", "1cb5a1fce0b65b616e69cc5ffd4e43e03d259e97"], "references": ["7e9ff94476f41041c75e253e84f487db00e9c861", "6bca62bac3f2226b88a525b995d87ceef86c0332", "657329c633709dd1ac34a30d57341b186b1a47c2", "18a93dc1558bf9d7534d0b416633cebaf75c1145", "2a074e2ac78ebb6158457aa4045fd92aeaba1ac1", "6f68e1bb253925d8431588555d3010419f322e04", "2b364917b0c51e91fcf2ab9c1d66a14ed4b44c03", "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "0147893b1aac35b7dad24d5342638292b39b362e", "0170fc76e934ee643f869df18fb617d5357e8b4e", "248b58b3d024158607002ef483df7a8701b5a8fb", "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "8cf62055fa0faab9c325f4b30415f5b0dc285434", "c5f7074a264356c9a022a8dff24df79d1db8c3d3", "bb2bc99f8220fc681320c541940c99ae30b286d6", "765866ecb5fe6a225d4e791498caf6a8351c16c7", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "f51497f463566581874c941353dd9d80069c5b77", "336868be817536e7c7fc88c391a2860cd869ea2b", "91024d7f8f749c74e90c69d05b03644bb75edbf2", "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "d78aed1dac6656affa4a04cbf225ced11a83d103", "b6e4cf90c1e1831f139bf8ad86f09e78824880ce", "5b3a37c1bd06bf3b0062e327a04b4a7af13d4e6f", "0de0a44b859a3719d11834479112314b4caba669", "a039ea239e37f53a2cb60c68e0a1967994353166", "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "26e44b8106a5145126c59b6d0c3af326337447f9", "21da617a0f79aabf94272107184606cefe90ab75", "27ac832ee83d8b5386917998a171a0257e2151e2", "2c7c9b405362277e390b09636c20ed274844c01f", "d7fc80056c0a8231fc58ee86353b42ee338464da", "49bc7fb789fec84878440da374c11a7e936b6139", "2b4696bf4bc923a139e8508086f854ca52b690d0", "9ea92ebeb7462f2db346cfa3281ad7497b1063d6", "ad655c25e052fa4eeed53421344aca6f239c4c9d", "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "fb507ada871d1e8c29e376dbf7b7879689aa89f9", "bed7c155b843fda8a1c994ce71e9176e43b20f77", "a7822238f5db7d62731eaeabf9725a65f4edf893", "b5246fa284f86b544a7c31f050b3bd0defd053fd", "bb669de2fce407df2f5cb2f8c51dedee3f467e04", "8b354d76813bd5375e7e5c8d17f630bec5936a01", "962ba753d7ea794f4085f96ef191e9016aff31ac", "1db9bd18681b96473f3c82b21edc9240b44dc329", "597b1dedb58ddb5ee5c35938c1a2574c7a11e0f6", "33998aff64ce51df8dee45989cdca4b6b1329ec4", "e399090e4dd760e8f09bd3d4de61b13b057c7740", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "5394da74498e00597295d18cd0557bd47e3fc341", "32e934094c4d17fe4d734b2e169ba5e3cd0ee05e", "455afd748e8834ef521e4b67c7c056d3c33429e2", "f63e917638553414526a0cc8550de4ad2d83fe7a", "0e6824e137847be0599bb0032e37042ed2ef5045", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "31181e73befea410e25de462eccd0e74ba8fea0b", "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "1ff9332f2b62ccdded3f46320fd3661d99155652"], "url": "https://www.semanticscholar.org/paper/3fbf6339273c50b04e886fa9bd4ad18c952a683d"}, "7e9ff94476f41041c75e253e84f487db00e9c861": {"id": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "authors": [{"authorId": "144447820", "name": "Yi Tay", "paperCount": 117, "citationCount": 6376, "hIndex": 39}, {"authorId": "3226635", "name": "M. Dehghani", "paperCount": 93, "citationCount": 11795, "hIndex": 27}, {"authorId": "2786352", "name": "Samira Abnar", "paperCount": 20, "citationCount": 778, "hIndex": 11}, {"authorId": "2714199", "name": "Yikang Shen", "paperCount": 15, "citationCount": 298, "hIndex": 6}, {"authorId": "11774695", "name": "Dara Bahri", "paperCount": 44, "citationCount": 1303, "hIndex": 12}, {"authorId": "1899637431", "name": "Philip Pham", "paperCount": 8, "citationCount": 404, "hIndex": 3}, {"authorId": "30586030", "name": "J. Rao", "paperCount": 36, "citationCount": 945, "hIndex": 15}, {"authorId": "2119062135", "name": "Liu Yang", "paperCount": 9, "citationCount": 284, "hIndex": 4}, {"authorId": "2884561", "name": "Sebastian Ruder", "paperCount": 63, "citationCount": 14219, "hIndex": 36}, {"authorId": "47193990", "name": "Donald Metzler", "paperCount": 43, "citationCount": 1248, "hIndex": 17}], "abstract": "Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at this https URL.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 54, "citation_count": 237, "influential_paper_citations": 81, "is_open_access": false, "citations": ["c8b25fab5608c3e033d34b4483ec47e68ba109b7", "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "1f133158a8973fb33fea188f20517cd7e69bfe7f", "78a0fb70b79116eb8d42c5951ced4f9efba513f0", "9933a5af7895354087baf6c96b64dc8a8973eaed", "f864d4d2267abba15eb43db54f58286aef78292b"], "references": ["7e5709d81558d3ef4265de29ea75931afeb1f2dd", "2def61f556f9a5576ace08911496b7c7e4f970a4", "466f7cfe7442738ee974b12939b4c2e32ee098bf", "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "7c5c149699a0ba54b52cd5b9e291077f4a1f9d13", "657329c633709dd1ac34a30d57341b186b1a47c2", "18a93dc1558bf9d7534d0b416633cebaf75c1145", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "3df83a60f55c64b40e6dbcd99cf9f67894a0736e", "6f68e1bb253925d8431588555d3010419f322e04", "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "76a9f336481b39515d6cea2920696f11fb686451", "f90f41416534efbd13a368d6383865b252584aec", "56676aef356ebb13cba77fc9e4d70760fbc151f5", "baed71eed57ad462f3ab138d4b1700a738cd5414", "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "34a4e6818d680875ff0bef9a76de0376118446d1", "832fff14d2ed50eb7969c4c4b976c35776548f56", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "bac7cd6175d838e1a0dea3603226a520516fb33a", "366244acdd930e488ae224ab6e2a92dc24aa7e06", "79c93274429d6355959f1e4374c2147bb81ea649", "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "17dbd7b72029181327732e4d11b52a08ed4630d0", "157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3", "a81874b4a651a740fffbfc47ef96515e8c7f782f", "b0da9efe9378efd98551e2416a3af0e4219a90ff", "21da617a0f79aabf94272107184606cefe90ab75", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f", "9405cc0d6169988371b2755e573cc28650d14dfe", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "22655979df781d222eaf812b0d325fa9adf11594", "0d3c46a3cbfe06cec259fec954b6ff6df6c1a566", "8b354d76813bd5375e7e5c8d17f630bec5936a01", "1db9bd18681b96473f3c82b21edc9240b44dc329", "7570afa31c68e24fce1342b7d67c591787219bc1", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "7d5cf22c70484fe217936c66741fb73b2a278bde", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "58eb3a2f0a67acf2f5c7c2cb4a22852b65314eb5", "d51ed05fd05b9d222427a05a87ed88217447b44f", "77fb0b7aef619dfac650423d4677170df2158e0d", "687bac2d3320083eb4530bf18bb8f8f721477600", "e01eae8dea6fbaa1ae7fc83535053932268df430", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "24459451bc8a4a378b02463db4e11490191219ed", "5d90f06bb70a0a3dced62413346235c02b1aa086"], "url": "https://www.semanticscholar.org/paper/7e9ff94476f41041c75e253e84f487db00e9c861"}, "de18baa4964804cf471d85a5a090498242d2e79f": {"id": "de18baa4964804cf471d85a5a090498242d2e79f", "title": "Improved Denoising Diffusion Probabilistic Models", "authors": [{"authorId": "38967461", "name": "Alex Nichol", "paperCount": 9, "citationCount": 3983, "hIndex": 8}, {"authorId": "6515819", "name": "Prafulla Dhariwal", "paperCount": 27, "citationCount": 20237, "hIndex": 14}], "abstract": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/ improved-diffusion.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 46, "citation_count": 353, "influential_paper_citations": 77, "is_open_access": false, "citations": ["47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "64ea8f180d0682e6c18d1eb688afdb2027c02794", "9695824d7a01fad57ba9c01d7d76a519d78d65e7", "7002ae048e4b8c9133a55428441e8066070995cb", "9cf6f42806a35fd1d410dbc34d8e8df73a29d094", "af9f365ed86614c800f082bd8eb14be76072ad16", "e2db22251792e9dd809d5ffb0feaab50a687cdb0", "8a1ea7b6e7e834d146ad782be5d63f57f806a9cc", "ad14e11bc97cc2fed0fd344e7cb7d7ce4205bfc6", "75a060752996ec8b27b791a9084b171bc8b4d777"], "references": ["90695f261c12265fb2694fe89cf390aad029a7dc", "633e2fbfc0b21e959a244100937c5853afca4853", "3e577c9bdc82cb7fed337a74f90bbc4505fdfb69", "014576b866078524286802b1d0e18628520aa886", "34bf13e58c7226d615afead0c0f679432502940e", "22c3badd79d4ee60892705b34c59807a6e828850", "685af6d2bcdff7170574643b2c5ab4fbcc36f597", "657329c633709dd1ac34a30d57341b186b1a47c2", "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "3123b814b5289315b07fdc108984422785027341", "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "853de0e00ac5ac257a622ae678ed373b8e086404", "289db3be7bf77e06e75541ba93269de3d604ac72", "1156e277fa7ec195b043161d3c5c97715da17658", "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "6469dbdff824d8abbfa7b5e03aaeeec52d4a1883", "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "965359b3008ab50dd04e171551220ec0e7f83aba", "6be216d93421bf19c1659e7721241ae73d483baf", "ca42e4d7021d4e563bbeae7db35c1ce09fe38bfa", "21da617a0f79aabf94272107184606cefe90ab75", "d383dd8ced85d7898d8b1546c514a34fb626ea16", "c8b25a128f4bfd0c79de82c174dd403b2ef6eeb1", "22aab110058ebbd198edb1f1e7b4f69fb13c0613", "7d3ab2a839b077a318022f7842225db55033b2c3", "eefa0df7c5678fa6004f8b48dbbc1c2696702fee", "21b786b3f870fc7fa247c143aa41de88b1fc6141", "d1c424c261c577958917055f72fb9e2ad0348865", "99c81b2dc2a5def38f62db269bba7765cff129e0", "231af7dc01a166cac3b5b01ca05778238f796e41", "66386a946a04534275bd466862364d139790f41f", "0936352b78a52bc5d2b5e3f04233efc56664af51", "571b0750085ae3d939525e62af510ee2cee9d5ea", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "4dcdae25a5e33682953f0853ee4cf7ca93be58a9", "2dcef55a07f8607a819c21fe84131ea269cc2e3c", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "5d90f06bb70a0a3dced62413346235c02b1aa086", "9966e890f2eedb4577e11b9d5a66380a4d9341fe"], "url": "https://www.semanticscholar.org/paper/de18baa4964804cf471d85a5a090498242d2e79f"}, "094ff971d6a8b8ff870946c9b3ce5aa173617bfb": {"id": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways", "authors": [{"authorId": "2841893", "name": "Aakanksha Chowdhery", "paperCount": 45, "citationCount": 1398, "hIndex": 18}, {"authorId": "46617804", "name": "Sharan Narang", "paperCount": 25, "citationCount": 11021, "hIndex": 16}, {"authorId": "39172707", "name": "Jacob Devlin", "paperCount": 42, "citationCount": 48279, "hIndex": 20}, {"authorId": "40377863", "name": "Maarten Bosma", "paperCount": 11, "citationCount": 1621, "hIndex": 11}, {"authorId": "2159632445", "name": "Gaurav Mishra", "paperCount": 7, "citationCount": 555, "hIndex": 6}, {"authorId": "145625142", "name": "Adam Roberts", "paperCount": 57, "citationCount": 24096, "hIndex": 31}, {"authorId": "152399055", "name": "P. Barham", "paperCount": 9, "citationCount": 1801, "hIndex": 4}, {"authorId": "3351938", "name": "Hyung Won Chung", "paperCount": 41, "citationCount": 1690, "hIndex": 20}, {"authorId": "152549864", "name": "Charles Sutton", "paperCount": 35, "citationCount": 1153, "hIndex": 14}, {"authorId": "3159346", "name": "Sebastian Gehrmann", "paperCount": 56, "citationCount": 3039, "hIndex": 25}, {"authorId": "2620528", "name": "Parker Schuh", "paperCount": 8, "citationCount": 539, "hIndex": 5}, {"authorId": "2362367", "name": "Kensen Shi", "paperCount": 14, "citationCount": 738, "hIndex": 9}, {"authorId": "2160888237", "name": "Sasha Tsvyashchenko", "paperCount": 3, "citationCount": 415, "hIndex": 2}, {"authorId": "2124977868", "name": "Joshua Maynez", "paperCount": 8, "citationCount": 430, "hIndex": 4}, {"authorId": "1484043592", "name": "Abhishek B Rao", "paperCount": 6, "citationCount": 501, "hIndex": 3}, {"authorId": "80940648", "name": "Parker Barnes", "paperCount": 6, "citationCount": 1586, "hIndex": 4}, {"authorId": "144447820", "name": "Yi Tay", "paperCount": 117, "citationCount": 6376, "hIndex": 39}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "3331141", "name": "Vinodkumar Prabhakaran", "paperCount": 74, "citationCount": 1927, "hIndex": 21}, {"authorId": "49849144", "name": "Emily Reif", "paperCount": 20, "citationCount": 1334, "hIndex": 12}, {"authorId": "2140321952", "name": "Nan Du", "paperCount": 13, "citationCount": 1314, "hIndex": 7}, {"authorId": "2044655623", "name": "B. Hutchinson", "paperCount": 7, "citationCount": 510, "hIndex": 4}, {"authorId": "2161431901", "name": "Reiner Pope", "paperCount": 3, "citationCount": 386, "hIndex": 1}, {"authorId": "2065251344", "name": "James Bradbury", "paperCount": 10, "citationCount": 20265, "hIndex": 7}, {"authorId": "2058365883", "name": "Jacob Austin", "paperCount": 9, "citationCount": 734, "hIndex": 6}, {"authorId": "2090818", "name": "M. Isard", "paperCount": 109, "citationCount": 52325, "hIndex": 53}, {"authorId": "1403749855", "name": "Guy Gur-Ari", "paperCount": 28, "citationCount": 2652, "hIndex": 20}, {"authorId": "38253388", "name": "Pengcheng Yin", "paperCount": 81, "citationCount": 2355, "hIndex": 20}, {"authorId": "2145151992", "name": "Toju Duke", "paperCount": 3, "citationCount": 658, "hIndex": 3}, {"authorId": "6639036", "name": "Anselm Levskaya", "paperCount": 14, "citationCount": 4368, "hIndex": 11}, {"authorId": "1780892", "name": "S. Ghemawat", "paperCount": 41, "citationCount": 35747, "hIndex": 19}, {"authorId": "50991767", "name": "Sunipa Dev", "paperCount": 22, "citationCount": 658, "hIndex": 7}, {"authorId": "47407464", "name": "H. Michalewski", "paperCount": 69, "citationCount": 1755, "hIndex": 15}, {"authorId": "143936294", "name": "Xavier Garc\u00eda", "paperCount": 25, "citationCount": 599, "hIndex": 9}, {"authorId": "40055795", "name": "Vedant Misra", "paperCount": 12, "citationCount": 1041, "hIndex": 8}, {"authorId": "2148473059", "name": "Kevin Robinson", "paperCount": 2, "citationCount": 486, "hIndex": 2}, {"authorId": "2096916416", "name": "L. Fedus", "paperCount": 4, "citationCount": 491, "hIndex": 3}, {"authorId": "65855107", "name": "Denny Zhou", "paperCount": 44, "citationCount": 1786, "hIndex": 15}, {"authorId": "7975935", "name": "Daphne Ippolito", "paperCount": 34, "citationCount": 1550, "hIndex": 17}, {"authorId": "150970919", "name": "D. Luan", "paperCount": 5, "citationCount": 9223, "hIndex": 4}, {"authorId": "8939217", "name": "Hyeontaek Lim", "paperCount": 39, "citationCount": 2667, "hIndex": 19}, {"authorId": "2368067", "name": "Barret Zoph", "paperCount": 46, "citationCount": 25445, "hIndex": 31}, {"authorId": "1572884723", "name": "A. Spiridonov", "paperCount": 5, "citationCount": 394, "hIndex": 3}, {"authorId": "35474601", "name": "Ryan Sepassi", "paperCount": 8, "citationCount": 2097, "hIndex": 7}, {"authorId": "35363891", "name": "David Dohan", "paperCount": 25, "citationCount": 3672, "hIndex": 14}, {"authorId": "3504647", "name": "Shivani Agrawal", "paperCount": 8, "citationCount": 400, "hIndex": 3}, {"authorId": "3175815", "name": "Mark Omernick", "paperCount": 6, "citationCount": 3767, "hIndex": 4}, {"authorId": "2555924", "name": "Andrew M. Dai", "paperCount": 71, "citationCount": 10296, "hIndex": 32}, {"authorId": "2598683", "name": "T. S. Pillai", "paperCount": 16, "citationCount": 1096, "hIndex": 9}, {"authorId": "97905921", "name": "Marie Pellat", "paperCount": 8, "citationCount": 625, "hIndex": 4}, {"authorId": "102549875", "name": "Aitor Lewkowycz", "paperCount": 38, "citationCount": 3986, "hIndex": 26}, {"authorId": "2057453483", "name": "Erica Moreira", "paperCount": 3, "citationCount": 385, "hIndex": 1}, {"authorId": "48422824", "name": "Rewon Child", "paperCount": 13, "citationCount": 17787, "hIndex": 11}, {"authorId": "2136267031", "name": "Oleksandr Polozov", "paperCount": 3, "citationCount": 393, "hIndex": 2}, {"authorId": "3844009", "name": "Katherine Lee", "paperCount": 7, "citationCount": 6045, "hIndex": 5}, {"authorId": "2198519", "name": "Zongwei Zhou", "paperCount": 20, "citationCount": 3387, "hIndex": 10}, {"authorId": "1524732527", "name": "Xuezhi Wang", "paperCount": 49, "citationCount": 2188, "hIndex": 20}, {"authorId": "4125424", "name": "Brennan Saeta", "paperCount": 12, "citationCount": 632, "hIndex": 6}, {"authorId": "2152965375", "name": "Mark D\u00edaz", "paperCount": 5, "citationCount": 564, "hIndex": 3}, {"authorId": "2345617", "name": "Orhan Firat", "paperCount": 95, "citationCount": 8131, "hIndex": 33}, {"authorId": "1754926", "name": "Michele Catasta", "paperCount": 43, "citationCount": 2739, "hIndex": 17}, {"authorId": "2111343757", "name": "Jason Wei", "paperCount": 28, "citationCount": 1141, "hIndex": 13}, {"authorId": "1398655031", "name": "K. Meier-Hellstern", "paperCount": 31, "citationCount": 3006, "hIndex": 16}, {"authorId": "2396681", "name": "D. Eck", "paperCount": 107, "citationCount": 6782, "hIndex": 40}, {"authorId": "48448318", "name": "J. Dean", "paperCount": 35, "citationCount": 6306, "hIndex": 19}, {"authorId": "1754497", "name": "Slav Petrov", "paperCount": 86, "citationCount": 12214, "hIndex": 40}, {"authorId": "22640071", "name": "Noah Fiedel", "paperCount": 13, "citationCount": 1669, "hIndex": 9}], "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning , which drastically reduces the number of task-speci\ufb01c training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM). We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly e\ufb03cient training across multiple TPU Pods. We demonstrate continued bene\ufb01ts of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the \ufb01netuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A signi\ufb01cant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2022, "reference_count": 173, "citation_count": 391, "influential_paper_citations": 54, "is_open_access": false, "citations": ["9695824d7a01fad57ba9c01d7d76a519d78d65e7", "5d0db797a45ce2453f821f7ded0b547d3fdab054", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "13a0d8bb38f739990c8cd65a44061c6534f17221", "1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe", "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "79b88230fabb59a1d368641bbc822af0f09bf262", "f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643", "5922f437512158970c417f4413bface021df5f78", "a8fc183c089bd596ccc48b3d666f8814e1b41e55"], "references": ["ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "34503c0b6a615124eaf82cb0e4a1dab2866e8980", "e47da75675b9a3fe02ef1efadca39bc8cdfcdc17", "1ed66e048bb025e75aa5ea660545285212e5341f", "011a4019aa0d0ce3edfa56bb2ca1e7586eb43fb2", "512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352", "79b88230fabb59a1d368641bbc822af0f09bf262", "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "adea6aa83b847752940129185428ea61935a0027", "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb", "f07570ed7847a7c95f35f4315afe543745f71e29", "5cbe278b65a81602a864184bbca37de91448a5f5", "7cbc2a7843411a1768ab762930707af0a3c33a19", "5d0db797a45ce2453f821f7ded0b547d3fdab054", "03488f1a193066b5ea8b9b800e119f07df5c1d9e", "b3848d32f7294ec708627897833c4097eb4d8778", "80d0116d77beeded0c23cf48946d9d10d4faee14", "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "934bdb14923510a2dedd3682a3f2c89f7e1ab364", "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "68f141724814839d556a989646194be88641b143", "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "92173d081b15824d22a9ef070e118744ceee8052", "629ae83d63f558e16b530441d765dc822d2949e1", "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "dfd104dd0ff28b1bde2fbd4c4d6d3ccb4761f639", "d64e57b9780f30f5b49bf620fdfb8584651b7f85", "e6a237ab883e503b10b73b3a411c0078c47c9830", "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "4f68e07c6c3173480053fd52391851d6f80d651b", "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "a989f5daa7c438d6b8433efbb1461f1b0af1aa8e", "57a1258571a21817d89197dc84c986861fb6e580", "509b16378deec0fb6bbec1d7aeb32a4bdeedddb1", "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a", "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "79b8ef3905a42b771248719495a2117271906445", "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3", "4506c9cd34a0b6f09fe23a3d9be0bc0962d14540", "774591fdd988eaaff3917e7c5171d044b0843e63", "13c4e5a6122f3fa2663f63e49537091da6532f35", "6b203d09193bf8cac934f9ab6e98a9184738990a", "6d9727f1f058614cada3fe296eeebd8ec4fc512a", "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "824cd8db8a68732db04f4d8b7139eb4475e59ff2", "187608bf94b2dccd25d1266ed925abf7b55dbb2e", "12b71736392209b4292471b7da0aed71ba2aa545", "4c2733d191e347753bb28afa46a1c55c65e085be", "fdacf2a732f55befdc410ea927091cad3b791f13", "346081161bdc8f18e2a4c4af7f51d35452b5cb01", "2b4929707e20e37a3a6351036ac48c20b5466a6d", "74276a37bfa50f90dfae37f767b2b67784bd402a", "687b13c44f849d23c2496996b5da83e706094db9", "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "10bb7e2c54b947fa50e7bb65b0b5c700fe998044", "1ec69f1a1a9d4ff5c5bc70db0e5087157b620570", "1882f194cb43828852cc052887671e55a80f945a", "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e", "657329c633709dd1ac34a30d57341b186b1a47c2", "681f4fbce872f138cbac9cdd92e8f6ed89ba6f8d", "0df347f5e3118fac7c351917e3a497899b071d1e", "3f97c2067cde9377e50b3160bbd7982c94abd88a", "bdd8530d72f51661f24247c9b5cd936ddb8e5c59", "f83618f13fce0e71e9127784f6ecc261dbdbf089", "3e65f572322e192fe36ae52a8a7f025b0685dfc6", "bf30aa827f77bdadfa95d5e680006aafd001503e", "f00f2d4b8ddd55aa2cc202f44053e5f97a254175", "226a962e9e2e01cafc3803018bb8bf511d549e9f", "399e7d8129c60818ee208f236c8dda17e876d21f", "725264948d7b6946259af5b8d966e996b9570f99", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "ea1f95989f808f409a3cd29b128000c04036c224", "ab5f0004c5f3317689e8457e1c8d8390ccbee522", "4b5b1154a825bca1491f0c9b09d064244a1924a9", "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "d47a682723f710395454687319bb55635e653105", "0071ee55f1749e6be586645bab08f0385ee856b4", "3cc2f69951cd24fe61be4cf32d62afbac297bc2b", "24e4d3370dc366d6b353d1d6818a0df266bb31b9", "1b04936c2599e59b120f743fbb30df2eed3fd782", "83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6", "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "d08463bd665589d04619f04dbde84183ffcf2e63", "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "04f4e55e14150b7c48b0287ba77c7443df76ed45", "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "40a6e8d8f253882c585f163b7333842d60ed6f14", "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "00c957711b12468cb38424caccdf5291bb354033", "a1b1aaeca6eb78c6fbbe2728f82612446aa03ad5", "825a609040ed9eb66a79c3efb51e2efbaa03fd78", "401dc39c2c8c910253d47980cfa3b4d2f7790d9b", "310b8117ae5ce3df8aa6304ad382525b9b46937e", "53ffd818255af798b4019e8ecc7d344f4852d647", "dc52b09089704ebd6f471177474bc29741c50023", "9e1241f017a627beca2542e378a88c642c32098b", "fe9b98b145ce9e6363fb41324dfa898990df9310", "17dbd7b72029181327732e4d11b52a08ed4630d0", "3259d52ae00e65b98391e7e6a2f672dfee721bf8", "7fcb4d25625b5a25ff99332c3ba09f9adabc7927", "8963317176fa81e185fd7a8f8cd001d7e11a4868", "6e722ac4d386489aa47703887881835ec0e1331d", "1906e3a2fda12641a42739e3fb6a8f8b1accc8dd", "1670a07b70f90cc4ddba71343e6a7ee4b5198595", "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "145b8b5d99a2beba6029418ca043585b90138d12", "d9f6ada77448664b71128bb19df15765336974a6", "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "21da617a0f79aabf94272107184606cefe90ab75", "dda6fb309f62e2557a071522354d8c2c897a2805", "26384278cf5d575fc32cb92c303fb648fa0d5217", "8ace1951c95f9bbbcf83571ff6c0579521507e2e", "c18663fea10c8a303d045fd2c1f33cacf9b73ca3", "7365f887c938ca21a6adbef08b5a520ebbd4638f", "990a7b4eceedb6e053e6386269481bdfc42a1094", "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "392ba7ae3f387f83cff7252dbc4ff14669ba4d47", "92e121c6e114fe3cfb89370df03847c66a9b4e28", "4f00db92bd7c1fd7b41d361c13797b9ff45a5b91", "44fc8d79fb8e0f8c6c6f680179b5803a789c6227", "eefa0df7c5678fa6004f8b48dbbc1c2696702fee", "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "b5246fa284f86b544a7c31f050b3bd0defd053fd", "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "807370352540f171685998aec7a5701f7110f147", "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7", "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "88bb0a28bb58d847183ec505dda89b63771bb495", "90e06703a776d4d482f8bbc04f31d816ee02ca8b", "960ba564e9e598d864dff38d2f3d0bad1b319ead", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "a26b00c607b6d6a2697f5444f14a9afc37701444", "19a632b17b2ee5f64df41bdd23755316a02fb939", "531a7f2c659787165df4fd5b4580590b953448e4", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "b123a0d46ad917b79c43c5ae981e03ed2458ed11", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "636a79420d838eabe4af7fb25d6437de45ab64e8", "554c9b3aa8c3b7504810270b18b942a0d22156e7", "510e26733aaff585d65701b9f1be7ca9d5afc586", "77fb0b7aef619dfac650423d4677170df2158e0d", "2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9", "85b68477a6e031d88b963833e15a4b4fc6855264", "d8bd531f7c8b1c0f7a4e263b6ddd9f664973ebe9", "193136b86539bd6df3f57a3685629c049a037418", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "b29447ba499507a259ae9d8f685d60cc1597d7d3", "c5145b1d15fea9340840cc8bb6f0e46e8934827f", "128cb6b891aee1b5df099acb48e2efecfcff689f", "89b945177b3fce466c7acf47a4df5514d80820b6", "60b05f32c32519a809f21642ef1eb3eaf3848008", "01a660ec8aa995a88a00bfb41cb86c022047a9db", "566eb7be43b8a2b2daff82b03711098a84859b2a"], "url": "https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb"}, "7e5709d81558d3ef4265de29ea75931afeb1f2dd": {"id": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey", "authors": [{"authorId": "144447820", "name": "Yi Tay", "paperCount": 117, "citationCount": 6376, "hIndex": 39}, {"authorId": "3226635", "name": "M. Dehghani", "paperCount": 93, "citationCount": 11795, "hIndex": 27}, {"authorId": "11774695", "name": "Dara Bahri", "paperCount": 44, "citationCount": 1303, "hIndex": 12}, {"authorId": "1680617", "name": "Donald Metzler", "paperCount": 92, "citationCount": 7900, "hIndex": 38}], "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \u201cX-former\u201d models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 104, "citation_count": 433, "influential_paper_citations": 45, "is_open_access": true, "citations": ["39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "7e9ff94476f41041c75e253e84f487db00e9c861", "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "b91de7d12ec1103f6ef9eb0720d697a9e7ecc9fe", "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "1f133158a8973fb33fea188f20517cd7e69bfe7f", "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "9933a5af7895354087baf6c96b64dc8a8973eaed", "5863d7b35ea317c19f707376978ef1cc53e3534c"], "references": ["e47da75675b9a3fe02ef1efadca39bc8cdfcdc17", "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "9d40837175577bb0009b138269b422f6d5820d00", "2d82ee05b132d4681c3bd517afc17d608fe6e525", "80d0116d77beeded0c23cf48946d9d10d4faee14", "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "66d735987a31d666a6459566ae026c40ab9a1c3a", "9933a5af7895354087baf6c96b64dc8a8973eaed", "e79d1206292bc5e67ba19737d87d4b2ea4a37105", "1f133158a8973fb33fea188f20517cd7e69bfe7f", "53c3940f35b8b45d55ed49056282e1961954513d", "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed", "01b1293ddea9bcd6df1185b0b934503de01d6561", "1a883522f3c0051d70be1f8cbdb8989a77395006", "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "e3a3e85c5a32af29e13b3561f6cf070de70651de", "e32a12b14e212506115cc6804667b3d8297917e1", "2365410a710b421b2295cdca0074946cb50bb1d4", "2def61f556f9a5576ace08911496b7c7e4f970a4", "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d", "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "9ed25f101f19ea735ca300848948ed64064b97ca", "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "fdacf2a732f55befdc410ea927091cad3b791f13", "7e9ff94476f41041c75e253e84f487db00e9c861", "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "1882f194cb43828852cc052887671e55a80f945a", "7c5c149699a0ba54b52cd5b9e291077f4a1f9d13", "0171ad4cc87cc7db25b4ec3169e293eed9a13b39", "657329c633709dd1ac34a30d57341b186b1a47c2", "ec28cb6f488a0e7f0d67f62a70a142f4601b7f7f", "5f895e84c1fea75de07b4f90da518273c2e57291", "eaa09c607780373cc809bce89b6b28b17e301f27", "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "89e2154d608cc8eced17bd7b276e278c89f8f0c1", "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "03d8cd1d4bbdc0f6f07bdd0412997606b3648e78", "cd4ffe5e014601a3d6b64121355d29a730591490", "8119a4485b7494436ca05feb9acccbf135507db1", "6f68e1bb253925d8431588555d3010419f322e04", "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "26299d5fdc5137291dc6a091573b3d18aba1d1c2", "baed71eed57ad462f3ab138d4b1700a738cd5414", "56676aef356ebb13cba77fc9e4d70760fbc151f5", "37127a02d129cb733377458d2f155997e3fd622f", "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "34a4e6818d680875ff0bef9a76de0376118446d1", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "f51497f463566581874c941353dd9d80069c5b77", "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "49e5b09480189fc9b2316a54f9d1e55cf0097c8b", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "7a064df1aeada7e69e5173f7d4c8606f4470365b", "0cbf97173391b0430140117027edcaf1a37968c7", "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "e763fdc9ae56826ff799163ea035b29bffd8ea6f", "59c7f27be8b09596714384f4a35face7cb74ad11", "a54b56af24bb4873ed0163b77df63b92bd018ddc", "366244acdd930e488ae224ab6e2a92dc24aa7e06", "f6390beca54411b06f3bde424fb983a451789733", "17dbd7b72029181327732e4d11b52a08ed4630d0", "bf442ab269074665a68e4dbbe19e4efc97862541", "830995ef17cc291c13f42dfd9f462137de1d2179", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "5240bad304d5e9dd6a7ab1e089e024119ae55567", "07a64686ce8e43ac475a8d820a8a9f1d87989583", "f4238bd2385a52413ccbacfd9e409a650235bd13", "b03c7ff961822183bab66b2e594415e585d3fd09", "21da617a0f79aabf94272107184606cefe90ab75", "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89", "2a31319e73d4486716168b65cdf7559baeda18ce", "660d3472d9c3733dedcf911187b234f2b65561b5", "29ddc1f43f28af7c846515e32cc167bc66886d0c", "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "5132500b23d2da47129b3f4f68dd30947a29e502", "512b8ef0002e0bfd0ecb5ab17d533c1762eb9786", "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "93b8da28d006415866bf48f9a6e06b5242129195", "1db9bd18681b96473f3c82b21edc9240b44dc329", "7570afa31c68e24fce1342b7d67c591787219bc1", "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83", "3861ae2a6bdd2a759c2d901a6583e63a216bc2fc", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "a456265138c088a894301c0433dae938705a9bec", "efbd381493bb9636f489b965a2034d529cd56bcd", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "f63e917638553414526a0cc8550de4ad2d83fe7a", "0c908739fbff75f03469d13d4a1a07de3414ee19", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "37fa065905442974682e4aca19c8108075d4de44", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "5d90f06bb70a0a3dced62413346235c02b1aa086", "441f33fab0614fa0696be54a046cbc692b7e70a2"], "url": "https://www.semanticscholar.org/paper/7e5709d81558d3ef4265de29ea75931afeb1f2dd"}, "63a9daf15ae2d4c1a7859d3105c9e6710903e072": {"id": "63a9daf15ae2d4c1a7859d3105c9e6710903e072", "title": "Perceiver: General Perception with Iterative Attention", "authors": [{"authorId": "2689633", "name": "Andrew Jaegle", "paperCount": 41, "citationCount": 934, "hIndex": 14}, {"authorId": "49423009", "name": "Felix Gimeno", "paperCount": 7, "citationCount": 540, "hIndex": 6}, {"authorId": "2065040247", "name": "Andrew Brock", "paperCount": 19, "citationCount": 5788, "hIndex": 15}, {"authorId": "1688869", "name": "Andrew Zisserman", "paperCount": 789, "citationCount": 241598, "hIndex": 172}, {"authorId": "1689108", "name": "Oriol Vinyals", "paperCount": 177, "citationCount": 122701, "hIndex": 85}, {"authorId": "35681810", "name": "Jo\u00e3o Carreira", "paperCount": 67, "citationCount": 12866, "hIndex": 32}], "abstract": "Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver \u2013 a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture performs competitively or beyond strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 on ImageNet without convolutions and by directly attending to 50,000 pixels. It also surpasses state-of-the-art results for all modalities in AudioSet.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 113, "citation_count": 281, "influential_paper_citations": 43, "is_open_access": false, "citations": ["c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "8f2bca9d684005675e294b33c26481e36f528cdb", "0eff37167876356da2163b2e396df2719adf7de9", "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "9933a5af7895354087baf6c96b64dc8a8973eaed", "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "847a153286d7f6f496f1ff61089831c267d68e30", "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "7fff8018bf625447df837c2fda5c58a705fbc038", "f1902f99c53781601061d794d957f77982753352"], "references": ["78ea232dbabc67ca4d6d4a7c1bbf568e9b47cb8a", "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "9ed25f101f19ea735ca300848948ed64064b97ca", "cec7872b194aadf54140578b9be52939eb1112e9", "c16835c8e535ebd9c10a550ca9455fe384a14449", "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "ad7ddcc14984caae308c397f1a589aae75d4ab71", "e75638f8dee71ea0439e139f53411a98a9c9f825", "787119e3c3f819244c82b7d97779473773e60696", "7e9ff94476f41041c75e253e84f487db00e9c861", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "466f7cfe7442738ee974b12939b4c2e32ee098bf", "7c5c149699a0ba54b52cd5b9e291077f4a1f9d13", "657329c633709dd1ac34a30d57341b186b1a47c2", "c7ecc64af234f2084ca353613463afe9f6fa7060", "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "6f68e1bb253925d8431588555d3010419f322e04", "10d11f0045dc7f217c7f01bc6cbb47929e9b8808", "5156381d63bb3e873533b08f203cb56c2d79b6c9", "a0dc3135c40e150f0271002a96b7c9680b6cac40", "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "03ea251b802fd46fe45483d40f01238a4ac9f4f7", "5bc24361d1f1ec16451d9c9531cfb45b99ea6a1f", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "54c7445f319823c7dcc948c830e75e2fa7460b33", "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "428b663772dba998f5dc6a24488fff1858a0899f", "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "43f2ad297941db230c089ba353efc3f281ab678c", "0172163109a75c0e847dd4b64c566d02afd2d63f", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "65d53938a12c77e7920b8eb3a49df249c978ba3f", "34adc3101810bc4d5ef84b4136455a89b9c86c27", "f0e287d883755757314fbc628007df2c6709c0bb", "f51497f463566581874c941353dd9d80069c5b77", "1ce1d287e825c78d381a95c0134e080bf1310611", "6bbd51697c25493ea15f4cac830e28eeac143898", "7a064df1aeada7e69e5173f7d4c8606f4470365b", "119b1526842c105ef9a5b187c13045eb580220e7", "1be579f4c120a8bf15c4df78d622549913b4d8f7", "02f55b398ff8c4d03314e99da7e815d190794471", "bc789aef715498e79a74f857fa090ece9e383bf1", "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "366244acdd930e488ae224ab6e2a92dc24aa7e06", "54416048772b921720f19869ed11c2a360589d03", "4616ea39c9c4fcaec65b1cf4769898b441511fd0", "f6390beca54411b06f3bde424fb983a451789733", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "f4238bd2385a52413ccbacfd9e409a650235bd13", "21da617a0f79aabf94272107184606cefe90ab75", "27ac832ee83d8b5386917998a171a0257e2151e2", "b0fae9fbb4e580d92395eabafe73e317ae6510e3", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "94bbc4ea271c918705876b60d98d227a0ab55a43", "512b8ef0002e0bfd0ecb5ab17d533c1762eb9786", "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "3afe627b83d4b7b4154f722d361dca9bfcc4adeb", "9405cc0d6169988371b2755e573cc28650d14dfe", "fd4ae71916cf400bfd1490f275e91b154eb69160", "9f5263cda2d58fb3dfaff5ec6db70b0d2ae53c68", "aa46c05868d6ccb404f4f6378cc11ae77f702d45", "1db9bd18681b96473f3c82b21edc9240b44dc329", "dfc504536e8434eb008680343abb77010965169e", "8899094797e82c5c185a0893896320ef77f60e64", "80d635952b283a6886487e51887f2d05ed122738", "9ae0a24f0928cab1554a6ac880f6b350f85be698", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "8674494bd7a076286b905912d26d47f7501c4046", "43428880d75b3a14257c3ee9bda054e61eb869c0", "5ba2218b708ca64ab556e39d5997202e012717d5", "0e779fd59353a7f1f5b559b9d65fa4bfe367890c", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7", "5c077b3ad4de4f2ea99561908aa9be1520f18a14", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "66d4475f0eee4b65983e06b1fbafad533eb81b2a", "dc4fc132b961c165ce5848dde224e92d00ae910a", "9e992b62070fbff77b455ff4bc2e8604e7116772", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "d25c65d261ea0e6a458be4c50c40ffe5bc508f77", "71ae756c75ac89e2d731c9c79649562b5768ff39", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "7c8a51d04522496c43db68f2582efd45eaf59fea", "6d4c9c923e9f145d1c01a2de2afc38ec23c44253", "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "82b9099ddf092463f497bd48bb112c46ca52c4d1", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "8f18b9dde7ff8a49c3807a2dc90c0941a66dc561", "0911b79ff4a7b3c4dc573a1014c0258068002251", "22c5a6f756b9adecb2c0297121e382128a33b5ef", "69e68bfaadf2dccff800158749f5a50fe82d173b", "f42b865e20e61a954239f421b42007236e671f19", "2de4eb4a2b74d2b7d7a987dfde3e0b9620012203", "7bea855e19fd13461590e4f2d44bbf7b807ce3e3", "4df17f8bbaf8e84f9ebe88c59f88b24babfac9b3", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "b94c7ff9532ab26c3aedbee3988ec4c7a237c173", "700bbcd3518ca8cb3dac50a89fc69cad3dc1a579", "c66b40e54fcb1d748aa04d82f497f24ac22d6129", "3d6ae6e9b59a871fd9259836ac9b6b7628f697f2", "33ce6c2f2d5128be710fb3ddd8f1117758b9b4a9", "db0172576316dc748aea82e8f13fb4719ac933d5"], "url": "https://www.semanticscholar.org/paper/63a9daf15ae2d4c1a7859d3105c9e6710903e072"}, "800cfb3d23115cdcd4d114234b65bbdf2080f798": {"id": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows", "authors": [{"authorId": "5620602", "name": "Xiaoyi Dong", "paperCount": 32, "citationCount": 562, "hIndex": 9}, {"authorId": "9324504", "name": "Jianmin Bao", "paperCount": 47, "citationCount": 2206, "hIndex": 20}, {"authorId": "49025801", "name": "Dongdong Chen", "paperCount": 116, "citationCount": 3567, "hIndex": 28}, {"authorId": "47528018", "name": "Weiming Zhang", "paperCount": 227, "citationCount": 5663, "hIndex": 37}, {"authorId": "1708598", "name": "Nenghai Yu", "paperCount": 556, "citationCount": 12935, "hIndex": 57}, {"authorId": "145347147", "name": "Lu Yuan", "paperCount": 95, "citationCount": 7454, "hIndex": 43}, {"authorId": "47514557", "name": "Dong Chen", "paperCount": 64, "citationCount": 4827, "hIndex": 27}, {"authorId": "143632999", "name": "B. Guo", "paperCount": 232, "citationCount": 16699, "hIndex": 66}], "abstract": "We present CSWin Transformer, an efficient and effective Transformer-based backbone for general-purpose vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute whereas local self-attention often limits the field of interactions of each token. To address this issue, we develop the Cross-Shaped Window self-attention mechanism for computing self-attention in the horizontal and vertical stripes in parallel that form a cross-shaped window, with each stripe obtained by splitting the input feature into stripes of equal width. We provide a mathematical analysis of the effect of the stripe width and vary the stripe width for different layers of the Transformer network which achieves strong modeling capability while limiting the computation cost. We also introduce Locally-enhanced Positional Encoding (LePE), which handles the local positional information better than existing encoding schemes. LePE naturally supports arbitrary input resolutions, and is thus especially effective and friendly for downstream tasks. Incorporated with these designs and a hierarchical structure, CSWin Transformer demonstrates competitive performance on common vision tasks. Specifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection task, and 52.2 mIOU on the ADE20K semantic segmentation task, surpassing previous state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and +2.0 respectively under the similar FLOPs setting. By further pretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. 11Code and pretrain model is available at https://github.com/microsoft/CSWin-Transformer", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 76, "citation_count": 213, "influential_paper_citations": 40, "is_open_access": true, "citations": ["2835951fabf12804e17d5a525b2be2bee70e7910", "be0fbb810583930c071d0b9b2c5187fe260783f5", "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "21ec90872abd986c12afe39bebe807732ffa70c9", "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "a66686e60a3eda0c606e036403cf0a07a5962595", "3e38f4b4055abecbac2e618df2ecb33554073e08", "9f1b0e4c42a5a85d4c023030557ade4419f82ecf", "9f951b58fc21926f94fc68d9b565d31cc02e8623", "c8831d0629f0eaf7f723317d71bbd60b8eb3c39f"], "references": ["68f080e0ac836ea230cb5316fbed273c70422d75", "8d3ddc27dce9c6c0fe110e4f9cb45d3b59feb04b", "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "5b68522f58b61e7235b852677337ef3725075fd9", "14c52ffa7ea9c1971d5d82ea369c946c98d056a9", "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "40f4d7fe800810288a80f84cdb357a8f4c28e880", "e775e649d815a02373eac840cf5e33a04ff85c95", "3cbe314cc5407a6c3249815b5173f22ea15173c2", "0eff37167876356da2163b2e396df2719adf7de9", "888e7c34223f52366286b332c66ddf7335dfac68", "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "2984ab83ade26639c3a82d29628d0d9e4abbebb0", "0ae67202f0584afccefa770865d14a46655d2975", "3e398bad2d8636491a1034cc938a5e024c7aa881", "654247d5b184495fca18c6aa7e840e4f4559fef0", "7fa11c51c540f6dc961c8a12b2573386570a0b27", "b4ce7f92a8b987b5e76d580bf5076e2495f06883", "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "d29430adccb805ab57b349afa8553954347b3197", "ad7ddcc14984caae308c397f1a589aae75d4ab71", "43cb4886a8056d5005702edbc51be327542b2124", "2ac7999cce9f415ee87643f56631b55ed26aa10e", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "466f7cfe7442738ee974b12939b4c2e32ee098bf", "657329c633709dd1ac34a30d57341b186b1a47c2", "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "d1f474d316b89555d335470da9b3aa4926be4695", "279696f90d13b1327ec7adb73e711e7d8f5db761", "6f68e1bb253925d8431588555d3010419f322e04", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "71b6394ad5654f5cd0fba763768ba4e523f7bbca", "34a4e6818d680875ff0bef9a76de0376118446d1", "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "0495d9df8eb84dcdab4e5536179823cd26279949", "f51497f463566581874c941353dd9d80069c5b77", "6bbd51697c25493ea15f4cac830e28eeac143898", "df67d46e78aae0d2fccfb6212d101a342259c01b", "2788a2461ed0067e2f7aaa63c449a24a237ec341", "1a0912bb76777469295bb2c059faee907e7f3258", "366244acdd930e488ae224ab6e2a92dc24aa7e06", "c2c083df88e88223e1a411e61040b94c233b1b63", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "21da617a0f79aabf94272107184606cefe90ab75", "6303bac53abd725c3b458190a6abe389a4a1e72d", "a84906dbd4d6640f918d0b6ed2a7313dda0d55f1", "5132500b23d2da47129b3f4f68dd30947a29e502", "d07284a6811f1b2745d91bdb06b040b57f226882", "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "04957e40d47ca89d38653e97f728883c0ad26e5d", "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "3647d6d0f151dc05626449ee09cc7bce55be497e", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "5694e46284460a648fe29117cbc55f6c9be3fa3c", "51db1f3c8dfc7d4077da39c96bb90a6358128111", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "eb42cf88027de515750f230b23b1a057dc782108", "71b7178df5d2b112d07e45038cb5637208659ff7", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "6dc61f37ecc552413606d8c89ffbc46ec98ed887"], "url": "https://www.semanticscholar.org/paper/800cfb3d23115cdcd4d114234b65bbdf2080f798"}, "e2db22251792e9dd809d5ffb0feaab50a687cdb0": {"id": "e2db22251792e9dd809d5ffb0feaab50a687cdb0", "title": "Variational Diffusion Models", "authors": [{"authorId": "2117266525", "name": "Diederik P. Kingma", "paperCount": 2, "citationCount": 157, "hIndex": 1}, {"authorId": "2117261512", "name": "Tim Salimans", "paperCount": 5, "citationCount": 362, "hIndex": 4}, {"authorId": "2117259766", "name": "Ben Poole", "paperCount": 2, "citationCount": 157, "hIndex": 1}, {"authorId": "2112615253", "name": "Jonathan Ho", "paperCount": 4, "citationCount": 357, "hIndex": 3}], "abstract": "Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the af\ufb01rmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for ef\ufb01cient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simpli\ufb01es to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often signi\ufb01cantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at https://github.com/google-research/vdm .", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 53, "citation_count": 159, "influential_paper_citations": 31, "is_open_access": false, "citations": ["f0b19c470c1f2b39f13258f0b57dceefca4d0a98", "136ff4a73cd7e9529f0bfe325249941128ad3187", "b28f10136cb60d7b9691c6711e043fac34d03ac4", "baf172cc59766ade32b8b8a807f68a709a237730", "65198893f70f04bd5b9baf6a6bcccbed44d20724", "479b624ee1ba3ef7dade574c007b7a6a4a7b1da1", "fd39b9a3cdd9c6d1a2753e9b0bb725cb6fb226bb", "621b90b9f7ea48da3014689f5766a35e64fba4e2", "17f2a787db8cf5104ffa71ca619d8f8092b05ca5"], "references": ["ad14e11bc97cc2fed0fd344e7cb7d7ce4205bfc6", "63d6a3cc7f2f52c9b4e224bb8b18f17b03f6de1e", "628bf6ac8fb384ca947ff4f52c2dd77f777f738d", "de18baa4964804cf471d85a5a090498242d2e79f", "9cf6f42806a35fd1d410dbc34d8e8df73a29d094", "90695f261c12265fb2694fe89cf390aad029a7dc", "633e2fbfc0b21e959a244100937c5853afca4853", "3e577c9bdc82cb7fed337a74f90bbc4505fdfb69", "014576b866078524286802b1d0e18628520aa886", "34bf13e58c7226d615afead0c0f679432502940e", "685af6d2bcdff7170574643b2c5ab4fbcc36f597", "657329c633709dd1ac34a30d57341b186b1a47c2", "853de0e00ac5ac257a622ae678ed373b8e086404", "289db3be7bf77e06e75541ba93269de3d604ac72", "a0dc3135c40e150f0271002a96b7c9680b6cac40", "1156e277fa7ec195b043161d3c5c97715da17658", "421271a93517c2c3f7ad9096b8c9455c16ae85a1", "dd6de9423afcc0f821fee2bd0363a4091c7f8cd3", "965359b3008ab50dd04e171551220ec0e7f83aba", "986c982f3f7af44d9cb597ac817bdfbef80b0835", "92f7c3cf0824732cdc1ec5a46af9a48cba3fa698", "21da617a0f79aabf94272107184606cefe90ab75", "c8b25a128f4bfd0c79de82c174dd403b2ef6eeb1", "c2ed34facd63d72e5d03ba13a6a3956ed6b2ac6c", "7d3ab2a839b077a318022f7842225db55033b2c3", "d07284a6811f1b2745d91bdb06b040b57f226882", "21b786b3f870fc7fa247c143aa41de88b1fc6141", "1db9bd18681b96473f3c82b21edc9240b44dc329", "744fe47157477235032f7bb3777800f9f2f45e52", "66386a946a04534275bd466862364d139790f41f", "6a97d2668187965743d1b825b306defccbabbb4c", "df0402517a7338ae28bc54acaac400de6b456a46", "41f1d50c85d3180476c4c7b3eea121278b0d8474", "4dcdae25a5e33682953f0853ee4cf7ca93be58a9", "6364fdaa0a0eccd823a779fcdd489173f938e91a", "2dcef55a07f8607a819c21fe84131ea269cc2e3c", "cea967b59209c6be22829699f05b8b1ac4dc092d", "484ad17c926292fbe0d5211540832a8c8a8e958b", "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "872bae24c109f7c30e052ac218b17a8b028d08a0", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "3b003beb9d8186c51b4912933b1ff75eef0a3c6c", "5d90f06bb70a0a3dced62413346235c02b1aa086", "f7f15848cd0fbb3d08f351595da833b1627de9c3", "25c9f33aceac6dcff357727cbe2faf145b01d13c"], "url": "https://www.semanticscholar.org/paper/e2db22251792e9dd809d5ffb0feaab50a687cdb0"}, "4d42859c7cb7a3ca8728f36c373c801789af2292": {"id": "4d42859c7cb7a3ca8728f36c373c801789af2292", "title": "Denoising Pretraining for Semantic Segmentation", "authors": [{"authorId": "2141568352", "name": "Emmanuel Asiedu Brempong", "paperCount": 2, "citationCount": 5, "hIndex": 2}, {"authorId": "40464924", "name": "Simon Kornblith", "paperCount": 76, "citationCount": 11941, "hIndex": 22}, {"authorId": "2117181755", "name": "Ting Chen", "paperCount": 8, "citationCount": 77, "hIndex": 3}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "46352821", "name": "Matthias Minderer", "paperCount": 14, "citationCount": 8847, "hIndex": 9}, {"authorId": "144739074", "name": "Mohammad Norouzi", "paperCount": 169, "citationCount": 26441, "hIndex": 53}], "abstract": "Semantic segmentation labels are expensive and time consuming to acquire. To improve label efficiency of semantic segmentation models, we revisit denoising autoencoders and study the use of a denoising objective for pretraining UNets. We pretrain a Transformer-based UNet as a denoising autoencoder, followed by fine-tuning on semantic segmentation using few labeled examples. Denoising pretraining outperforms training from random initialization, and even supervised ImageNet-21K pretraining of the encoder when the number of labeled images is small. A key advantage of denoising pretraining over supervised pretraining of the backbone is the ability to pretrain the decoder, which would otherwise be randomly initialized. We thus propose a novel Decoder Denoising Pretraining (DDeP) method, in which we initialize the encoder using supervised learning and pretrain only the decoder using the denoising objective. Despite its simplicity, DDeP achieves state-of-the-art results on label-efficient semantic segmentation, offering considerable gains on the Cityscapes, Pascal Context, and ADE20K datasets.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2022, "reference_count": 104, "citation_count": 3, "influential_paper_citations": 0, "is_open_access": false, "citations": ["b6be967c7200c3d126fc69586d8e17db832244c5", "a871286076818da41c053969378795972146da5f", "fb7a38326d2e31b08bbf958f223ec094ca918022"], "references": ["42f2271cebb7f272b0066c1f22d33381f139ee68", "37c9c4e7648f639c0b36f150fc6c6c90b3682f4a", "dab46dd3985d1de5cd6549319797ab3705b6a801", "0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4", "d0cefc4621a55540c77e8ade9b8ae1bfeb530f42", "8a1ea7b6e7e834d146ad782be5d63f57f806a9cc", "0bbbbdf1d6dbcc53c41a0cea7589e42cfb806e82", "49371edd617fb1684ddf351ef0857135f1af0ece", "68f080e0ac836ea230cb5316fbed273c70422d75", "64ea8f180d0682e6c18d1eb688afdb2027c02794", "786cb74290340443e9d02ffd9669f5e2a18878b5", "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "de18baa4964804cf471d85a5a090498242d2e79f", "24b8a0b02bcb7934967757fc59d273a71ba67e30", "d29430adccb805ab57b349afa8553954347b3197", "914a593b7f2e980470075a9955f1407641669a8f", "a264063c96bad0f6368e1dff6553539788ecd4ce", "787119e3c3f819244c82b7d97779473773e60696", "633e2fbfc0b21e959a244100937c5853afca4853", "6f92dcefc5f6b4346f619ae7546a8bd2d6decade", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "217dd8145b5fd7ae7eaa921fe1e98b03039904ad", "685af6d2bcdff7170574643b2c5ab4fbcc36f597", "6d78ad8f84d486a27c6d1501803408fbfa276bd6", "e516c11de1838ad9d79a36177defd50e35610f74", "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "4d2a05140dd9bafaf035a846e7bda05f956304d2", "79ef0efa7217bf85712c652b3b3640176b9e2feb", "e8271568a813ee7ce25d66c6a8cb04e71e590b75", "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "289db3be7bf77e06e75541ba93269de3d604ac72", "38f93092ece8eee9771e61c1edaf11b1293cae1b", "368c72c2298e5f8276398b2cb198702281eac4f8", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "128002775de8271e43f64574105a155a0457073c", "f3c8c755ffee6f3355cf56ea17af066bbc61c44b", "59216337a3ef6928484fb2101530956d1107337a", "88e8a250f8e28544fe7b74db72ec18a7627ea456", "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "a1b8a8df281bbaec148a897927a49ea47ea31515", "34733eaf66007516347a40ad5d9bbe1cc9dacb6b", "c7a701f64e03b8dcaa506e1d5b0e18822a55d429", "0495d9df8eb84dcdab4e5536179823cd26279949", "add2f205338d70e10ce5e686df4a690e2851bdfc", "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "9eed9e9a299bc1264b28ddfb9b06f96eb5f43175", "1cae417456711c4da184f5efcd1b7464a7a0661a", "3021b6dec80e7032fd995c0dcadf4c992b7d7506", "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "965359b3008ab50dd04e171551220ec0e7f83aba", "cde35c87aaabbc617d38f9cfaa2721a2e166d750", "9b09d296059909490096e34e9df2d95314787ad5", "461470832f30d7dcca7d58dc399190f70462ee55", "24bae1dbad640d81306d106a380261eb7d6063fb", "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "c002ceaeee6fb74bebbdc2012a877ba78f1fd4de", "a970d6b162cfdde4258cc2c356190cf516927b51", "ceb2ebef0b41e31c1a21b28c2734123900c005e2", "eae7d5b15423a148e6bb32d24bbabedfacd0e2df", "4b1c6f6521da545892f3f5dc39461584d4a27ec0", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "1d033b30f38642e4b6dd146bb8b464bfb58aad96", "b227f3e4c0dc96e5ac5426b85485a70f2175a205", "22dd101b73666bf19afa429bb61b371d7ca326f2", "fad0f098e178cf7f3065b282721509290ecf738c", "aab368284210c1bb917ec2d31b84588e3d2d7eb4", "88512be44744615f4baa8e14f600f036db4c2433", "cab372bc3824780cce20d9dd1c22d4df39ed081a", "6c7bdab6e39a6045f4700efb5b09a8c5e646e7c9", "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "1342c1e1684620c019972e2679d5131f1e8a4a13", "62f3d3015cee122bd147d7d878c85f70cc15680d", "571b0750085ae3d939525e62af510ee2cee9d5ea", "7d0effebfa4bed19b6ba41f3af5b7e5b6890de87", "c8c494ee5488fe20e0aa01bddf3fc4632086d654", "2ec8f7e0257a07d3914322b36072d1bbcd58a1e0", "8201e6e687f2de477258e9be53ba7b73ee30d7de", "8e63784bd5a24d5e3035e2a11753e65e6e56625d", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "fc1b1c9364c58ec406f494dd944b609a6a038ba6", "6364fdaa0a0eccd823a779fcdd489173f938e91a", "2dcef55a07f8607a819c21fe84131ea269cc2e3c", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "3419ccd5c94d301ee08d716d037f0c3c6a62e78e", "6270baedeba28001cd1b563a199335720d6e0fe0", "872bae24c109f7c30e052ac218b17a8b028d08a0", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "ba04a332de10e844752794c25c2dee35483d4ca3", "82635fb63640ae95f90ee9bdc07832eb461ca881", "843959ffdccf31c6694d135fad07425924f785b1", "5ee8a371fc5adc5469435020a52fb815f3b57a71", "9966e890f2eedb4577e11b9d5a66380a4d9341fe", "59dc87ac9fd2c39d7b9f8ab1a2bf43ac53891e96", "944cba683d10d8c1a902e05cd68e32a9f47b372e", "baf2c2c796470815a689ce04ce13ef6f10d61730", "76d4b7fd734f987dc87c9aafdbcc7e1af99ad8a5", "17a620afc87f5266e3fd8b3f308c883cc2c2b7c7"], "url": "https://www.semanticscholar.org/paper/4d42859c7cb7a3ca8728f36c373c801789af2292"}, "868f052bf099692d4d188675488b21dce35fdd07": {"id": "868f052bf099692d4d188675488b21dce35fdd07", "title": "Decoder Denoising Pretraining for Semantic Segmentation", "authors": [{"authorId": "2153814789", "name": "E. B. Asiedu", "paperCount": 2, "citationCount": 6, "hIndex": 2}, {"authorId": "40464924", "name": "Simon Kornblith", "paperCount": 76, "citationCount": 11941, "hIndex": 22}, {"authorId": "2117181755", "name": "Ting Chen", "paperCount": 8, "citationCount": 77, "hIndex": 3}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "46352821", "name": "Matthias Minderer", "paperCount": 14, "citationCount": 8847, "hIndex": 9}, {"authorId": "144739074", "name": "Mohammad Norouzi", "paperCount": 169, "citationCount": 26441, "hIndex": 53}], "abstract": "Semantic segmentation labels are expensive and time consuming to acquire. Hence, pretraining is commonly used to improve the label-e\ufb03ciency of segmentation models. Typically, the encoder of a segmentation model is pretrained as a classi\ufb01er and the decoder is randomly initialized. Here, we argue that random initialization of the decoder can be suboptimal, especially when few labeled examples are available. We propose a decoder pretraining approach based on denoising, which can be combined with supervised pretraining of the encoder. We \ufb01nd that decoder denoising pretraining on the ImageNet dataset strongly outperforms encoder-only supervised pretraining. Despite its simplicity, decoder denoising pretraining achieves state-of-the-art results on label-e\ufb03cient semantic segmentation and o\ufb00ers consid-erable gains on the Cityscapes, Pascal Context, and ADE20K datasets. The DeP results make", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2022, "reference_count": 99, "citation_count": 4, "influential_paper_citations": 0, "is_open_access": false, "citations": ["02a615c807a6f0f692f4d89a62f11c00314492fc", "a871286076818da41c053969378795972146da5f", "c3344bc67b581c39fa089e59be47a2db486f473a", "4be4de9df5f036c0a785852634d79b0d52926a26"], "references": ["571e8ac5adf78330518215a1e1b80011be61982f", "42f2271cebb7f272b0066c1f22d33381f139ee68", "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "37c9c4e7648f639c0b36f150fc6c6c90b3682f4a", "722ad6ac92286507437b31486f47987d6ece05c9", "0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4", "d0cefc4621a55540c77e8ade9b8ae1bfeb530f42", "8a1ea7b6e7e834d146ad782be5d63f57f806a9cc", "0bbbbdf1d6dbcc53c41a0cea7589e42cfb806e82", "1c1cc65908dd5ca3d0103d650de896e053e0e7d7", "ed1cb6957f77175f583651879cd6648299f4589b", "9653c070724e44f023e8cc3ec79f0b9e6d59480d", "68f080e0ac836ea230cb5316fbed273c70422d75", "64ea8f180d0682e6c18d1eb688afdb2027c02794", "786cb74290340443e9d02ffd9669f5e2a18878b5", "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "de18baa4964804cf471d85a5a090498242d2e79f", "24b8a0b02bcb7934967757fc59d273a71ba67e30", "d29430adccb805ab57b349afa8553954347b3197", "914a593b7f2e980470075a9955f1407641669a8f", "787119e3c3f819244c82b7d97779473773e60696", "633e2fbfc0b21e959a244100937c5853afca4853", "6f92dcefc5f6b4346f619ae7546a8bd2d6decade", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "217dd8145b5fd7ae7eaa921fe1e98b03039904ad", "685af6d2bcdff7170574643b2c5ab4fbcc36f597", "6d78ad8f84d486a27c6d1501803408fbfa276bd6", "e516c11de1838ad9d79a36177defd50e35610f74", "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "4d2a05140dd9bafaf035a846e7bda05f956304d2", "79ef0efa7217bf85712c652b3b3640176b9e2feb", "e8271568a813ee7ce25d66c6a8cb04e71e590b75", "289db3be7bf77e06e75541ba93269de3d604ac72", "3e7f5f4382ac6f9c4fef6197dd21abf74456acd1", "38f93092ece8eee9771e61c1edaf11b1293cae1b", "368c72c2298e5f8276398b2cb198702281eac4f8", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "128002775de8271e43f64574105a155a0457073c", "f3c8c755ffee6f3355cf56ea17af066bbc61c44b", "59216337a3ef6928484fb2101530956d1107337a", "88e8a250f8e28544fe7b74db72ec18a7627ea456", "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "a1b8a8df281bbaec148a897927a49ea47ea31515", "34733eaf66007516347a40ad5d9bbe1cc9dacb6b", "c7a701f64e03b8dcaa506e1d5b0e18822a55d429", "0495d9df8eb84dcdab4e5536179823cd26279949", "add2f205338d70e10ce5e686df4a690e2851bdfc", "3cfb319689f06bf04c2e28399361f414ca32c4b3", "9eed9e9a299bc1264b28ddfb9b06f96eb5f43175", "1cae417456711c4da184f5efcd1b7464a7a0661a", "3021b6dec80e7032fd995c0dcadf4c992b7d7506", "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "965359b3008ab50dd04e171551220ec0e7f83aba", "9b09d296059909490096e34e9df2d95314787ad5", "461470832f30d7dcca7d58dc399190f70462ee55", "24bae1dbad640d81306d106a380261eb7d6063fb", "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "c002ceaeee6fb74bebbdc2012a877ba78f1fd4de", "a970d6b162cfdde4258cc2c356190cf516927b51", "ceb2ebef0b41e31c1a21b28c2734123900c005e2", "eae7d5b15423a148e6bb32d24bbabedfacd0e2df", "4b1c6f6521da545892f3f5dc39461584d4a27ec0", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "1d033b30f38642e4b6dd146bb8b464bfb58aad96", "b227f3e4c0dc96e5ac5426b85485a70f2175a205", "22dd101b73666bf19afa429bb61b371d7ca326f2", "fad0f098e178cf7f3065b282721509290ecf738c", "aab368284210c1bb917ec2d31b84588e3d2d7eb4", "88512be44744615f4baa8e14f600f036db4c2433", "6c7bdab6e39a6045f4700efb5b09a8c5e646e7c9", "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "1342c1e1684620c019972e2679d5131f1e8a4a13", "62f3d3015cee122bd147d7d878c85f70cc15680d", "7d0effebfa4bed19b6ba41f3af5b7e5b6890de87", "c8c494ee5488fe20e0aa01bddf3fc4632086d654", "2ec8f7e0257a07d3914322b36072d1bbcd58a1e0", "8201e6e687f2de477258e9be53ba7b73ee30d7de", "8e63784bd5a24d5e3035e2a11753e65e6e56625d", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "fc1b1c9364c58ec406f494dd944b609a6a038ba6", "2dcef55a07f8607a819c21fe84131ea269cc2e3c", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "e15cf50aa89fee8535703b9f9512fca5bfc43327", "54e325aee6b2d476bbbb88615ac15e251c6e8214", "3419ccd5c94d301ee08d716d037f0c3c6a62e78e", "6270baedeba28001cd1b563a199335720d6e0fe0", "872bae24c109f7c30e052ac218b17a8b028d08a0", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "ba04a332de10e844752794c25c2dee35483d4ca3", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "843959ffdccf31c6694d135fad07425924f785b1", "9966e890f2eedb4577e11b9d5a66380a4d9341fe", "59dc87ac9fd2c39d7b9f8ab1a2bf43ac53891e96", "944cba683d10d8c1a902e05cd68e32a9f47b372e", "baf2c2c796470815a689ce04ce13ef6f10d61730", "76d4b7fd734f987dc87c9aafdbcc7e1af99ad8a5", "17a620afc87f5266e3fd8b3f308c883cc2c2b7c7"], "url": "https://www.semanticscholar.org/paper/868f052bf099692d4d188675488b21dce35fdd07"}, "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78": {"id": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "title": "Bottleneck Transformers for Visual Recognition", "authors": [{"authorId": "41207614", "name": "A. Srinivas", "paperCount": 38, "citationCount": 4011, "hIndex": 19}, {"authorId": "33493200", "name": "Tsung-Yi Lin", "paperCount": 37, "citationCount": 57516, "hIndex": 25}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "1789737", "name": "Jonathon Shlens", "paperCount": 108, "citationCount": 67183, "hIndex": 50}, {"authorId": "1689992", "name": "P. Abbeel", "paperCount": 582, "citationCount": 83411, "hIndex": 130}, {"authorId": "1630664874", "name": "Ashish Vaswani", "paperCount": 13, "citationCount": 611, "hIndex": 6}], "abstract": "We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt [67] evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in \"compute\"1 time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.2", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 77, "citation_count": 374, "influential_paper_citations": 54, "is_open_access": true, "citations": ["c8b25fab5608c3e033d34b4483ec47e68ba109b7", "2def61f556f9a5576ace08911496b7c7e4f970a4", "177e957f5cd93229c9794ea652c646d2557b4a69", "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "e775e649d815a02373eac840cf5e33a04ff85c95", "76d79b5956f1a1a75de52c9e32a84d03f504c97f", "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "6709d5583f658f589ae6a2184805933aceb18849", "003326a15fc4a8833785a47a741d7712474fa256"], "references": ["7e5709d81558d3ef4265de29ea75931afeb1f2dd", "9efe8dbde586d6248ecfc69f08b918012e2ac478", "cec7872b194aadf54140578b9be52939eb1112e9", "ad7ddcc14984caae308c397f1a589aae75d4ab71", "914a593b7f2e980470075a9955f1407641669a8f", "0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "5f69762f2ec3eecb6011d14f024ef0dec3eba23c", "38f93092ece8eee9771e61c1edaf11b1293cae1b", "6b85b63579a916f705a8e10a49bd8d849d91b1fc", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "54c7445f319823c7dcc948c830e75e2fa7460b33", "2709167f1c3a03fa5b970a665ea48ed243aab582", "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "34733eaf66007516347a40ad5d9bbe1cc9dacb6b", "d34aaf35f433dbb51681a91a2eed37a3926346e9", "19f3299deda9f33003a6378788806acdb089669f", "41c67d04be2d1632c0d3b0880c21c9fe797cdab8", "add2f205338d70e10ce5e686df4a690e2851bdfc", "6bbd51697c25493ea15f4cac830e28eeac143898", "611d6f7cf50d82dbef0b1233b922c2f34546d042", "97f4d09175705be4677d675fa27e55defac44800", "1cae417456711c4da184f5efcd1b7464a7a0661a", "df67d46e78aae0d2fccfb6212d101a342259c01b", "366244acdd930e488ae224ab6e2a92dc24aa7e06", "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "9ad960428d2845dbe8a9af3e988ee22a303948f2", "15a526d7d66dd7a08f6bec6aa9b39ffa6322a96e", "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "66143960c0325c70329a3869cc8052f0416b87aa", "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "27ac832ee83d8b5386917998a171a0257e2151e2", "b5375995ab8d679a581ffcc2f2e8d3777d60324b", "c41a11c0e9b8b92b4faaf97749841170b760760a", "41071dbbbcbb27af3fec70de045f19c28535f5b7", "29309743870c825f9645a4803af727402462e513", "5132500b23d2da47129b3f4f68dd30947a29e502", "9405cc0d6169988371b2755e573cc28650d14dfe", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "9f7919a5677290ab2eca4fa8056bdbbf7c0b11d6", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "5cc22f65bf4e5c0aa61f3139da832d3a946e15cf", "04957e40d47ca89d38653e97f728883c0ad26e5d", "6a0aaefce8a27a8727d896fa444ba27558b2d381", "8899094797e82c5c185a0893896320ef77f60e64", "b587ee7c802a5bd222a69090f59285e0dfdb29f1", "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36", "9e8db1519245426f3a78752a3d8360484f4626b1", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7", "1c4e9156ca07705531e45960b7a919dc473abb51", "1e9b1f6061ef779e3ad0819c2832a29168eaeb9d", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "7ffdbc358b63378f07311e883dddacc9faeeaf4b", "0c908739fbff75f03469d13d4a1a07de3414ee19", "4d376d6978dad0374edfa6709c9556b42d3594d3", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "eb42cf88027de515750f230b23b1a057dc782108", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "342786659379879f58bf5c4ff43c84c83a6a7389", "71b7178df5d2b112d07e45038cb5637208659ff7", "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "34f25a8704614163c4095b3ee2fc969b60de4698", "38f35dd624cd1cf827416e31ac5e0e0454028eca", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "82635fb63640ae95f90ee9bdc07832eb461ca881", "9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d"], "url": "https://www.semanticscholar.org/paper/16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78"}, "48ded1987241642a70206d0a49d342f67814e5df": {"id": "48ded1987241642a70206d0a49d342f67814e5df", "title": "Bottleneck Transformers for Visual Recognition-Supplementary", "authors": [{"authorId": "41207614", "name": "A. Srinivas", "paperCount": 38, "citationCount": 4011, "hIndex": 19}, {"authorId": "2115356502", "name": "Tsung-Yi Lin", "paperCount": 16, "citationCount": 257, "hIndex": 8}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "1789737", "name": "Jonathon Shlens", "paperCount": 108, "citationCount": 67183, "hIndex": 50}, {"authorId": "1689992", "name": "P. Abbeel", "paperCount": 582, "citationCount": 83411, "hIndex": 130}, {"authorId": "1630664874", "name": "Ashish Vaswani", "paperCount": 13, "citationCount": 611, "hIndex": 6}], "abstract": "For all experiments, we use L2 weight decay of 4e \u2212 5, sync-batch-norm with momentum 0.997 and epsilon 1e4. We use batch norm in the backbone, FPN, RPN head, FRCNN head and MRCNN head. We initialize backbone weights with pre-trained ImageNet checkpoints and finetune all the weights (including the batch-norm parameters) as specified in MoCo [6]. Table 1 presents the hyperparameters for models that we train with batch size 64 on 8 TPU-v3 cores (equivalent to DGX-1) which applies to most of our models, in particular, all the models that we train with image size 1024\u00d7 1024.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 22, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "cec7872b194aadf54140578b9be52939eb1112e9", "ad7ddcc14984caae308c397f1a589aae75d4ab71", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "64b9be00f4eecd465b4e8e46e2ab7624d7eaeb2b", "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "2709167f1c3a03fa5b970a665ea48ed243aab582", "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "add2f205338d70e10ce5e686df4a690e2851bdfc", "df67d46e78aae0d2fccfb6212d101a342259c01b", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "8899094797e82c5c185a0893896320ef77f60e64", "b587ee7c802a5bd222a69090f59285e0dfdb29f1", "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7", "2c03df8b48bf3fa39054345bafabfeff15bfd11d"], "url": "https://www.semanticscholar.org/paper/48ded1987241642a70206d0a49d342f67814e5df"}, "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4": {"id": "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones", "authors": [{"authorId": "1630664874", "name": "Ashish Vaswani", "paperCount": 13, "citationCount": 611, "hIndex": 6}, {"authorId": "3377142", "name": "Prajit Ramachandran", "paperCount": 15, "citationCount": 3878, "hIndex": 12}, {"authorId": "41207614", "name": "A. Srinivas", "paperCount": 38, "citationCount": 4011, "hIndex": 19}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "3135881", "name": "Blake A. Hechtman", "paperCount": 20, "citationCount": 1002, "hIndex": 13}, {"authorId": "1789737", "name": "Jonathon Shlens", "paperCount": 108, "citationCount": 67183, "hIndex": 50}], "abstract": "Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutions. 1", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 69, "citation_count": 156, "influential_paper_citations": 21, "is_open_access": true, "citations": ["2def61f556f9a5576ace08911496b7c7e4f970a4", "7a9a708ca61c14886aa0dcd6d13dac7879713f5f", "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "2835951fabf12804e17d5a525b2be2bee70e7910", "800cfb3d23115cdcd4d114234b65bbdf2080f798", "e3a3e85c5a32af29e13b3561f6cf070de70651de", "03db529f0bfae6d0b64b0feef565196327fe8d50", "2a805d0e1b067444a554c5169d189fa1f649f411", "1fb10189c500e4902cd1b5afd406f57323d21be8", "57150ca7d793d6f784cf82da1c349edf7beb6bc2"], "references": ["cec7872b194aadf54140578b9be52939eb1112e9", "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "657329c633709dd1ac34a30d57341b186b1a47c2", "54c7445f319823c7dcc948c830e75e2fa7460b33", "2709167f1c3a03fa5b970a665ea48ed243aab582", "a8c0ac6588012d91c81b83b6cbd16c40e2e5edd2", "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "0c24d55e0a57052b99c5aadf9764327e18688151", "0495d9df8eb84dcdab4e5536179823cd26279949", "1ce1d287e825c78d381a95c0134e080bf1310611", "df67d46e78aae0d2fccfb6212d101a342259c01b", "dfa553707b215910f028d2a58ab79116626cc94a", "366244acdd930e488ae224ab6e2a92dc24aa7e06", "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "21de3a36cb51adc205fad8a1d3d69118891dc3dd", "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "8c92054c26fb4c6dd7435bc99fbb8af3323eae1b", "21da617a0f79aabf94272107184606cefe90ab75", "27ac832ee83d8b5386917998a171a0257e2151e2", "c256c0fca1f0f6de240743164ae729da3a29fc81", "fb8cf663a71bf31f59557a35d36aaf8c465b50af", "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "29309743870c825f9645a4803af727402462e513", "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "1db9bd18681b96473f3c82b21edc9240b44dc329", "8899094797e82c5c185a0893896320ef77f60e64", "d0611891b9e8a7c5731146097b6f201578f47b2f", "b587ee7c802a5bd222a69090f59285e0dfdb29f1", "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "8760bc7631c0cb04e7138254e9fd6451b7def8ca", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "204a4a70428f3938d2c538a4d74c7ae0416306d8", "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36", "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "b022f2a277a4bf5f42382e86e4380b96340b9e86", "15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "51db1f3c8dfc7d4077da39c96bb90a6358128111", "77f0a39b8e02686fd85b01971f8feb7f60971f80", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "d5eadd6f059d742d76441fd0a635a21694dd7392", "4d376d6978dad0374edfa6709c9556b42d3594d3", "4d8f2d14af5991d4f0d050d22216825cac3157bd", "326d65827307862ddc3d39b84ebc662e83ff95b3", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "31c36d445367ba204244bb74893c5654e31c3869", "6bdb186ec4726e00a8051119636d4df3b94043b5", "71b7178df5d2b112d07e45038cb5637208659ff7", "a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "34f25a8704614163c4095b3ee2fc969b60de4698", "89b1f4740ae37fd04f6ac007577bdd34621f0861", "aa7bfd2304201afbb19971ebde87b17e40242e91", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "d2c733e34d48784a37d717fe43d9e93277a8c53e", "2cc157afda51873c30b195fff56e917b9c06b853", "9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d", "6dc61f37ecc552413606d8c89ffbc46ec98ed887", "8d3a318b62d2e970122da35b2a2e70a5d12cc16f", "1ff2a79066f2501453c750f40b17613324ec12b9", "c5f5311fa1f34159ab3a0a1d58da51cd0340a640", "d7fb932bca642615fcbcf3f5d26b2c26666603d3"], "url": "https://www.semanticscholar.org/paper/91e8117e7ebc966bc76de2cb52ec717d2acdb1a4"}, "cea1f8894cb73f6b1ece7f465bad18fb4b79168d": {"id": "cea1f8894cb73f6b1ece7f465bad18fb4b79168d", "title": "Simple and Efficient ways to Improve REALM", "authors": [{"authorId": "143820870", "name": "Vidhisha Balachandran", "paperCount": 18, "citationCount": 209, "hIndex": 6}, {"authorId": "1630664874", "name": "Ashish Vaswani", "paperCount": 13, "citationCount": 611, "hIndex": 6}, {"authorId": "2073587169", "name": "Yulia Tsvetkov", "paperCount": 27, "citationCount": 355, "hIndex": 7}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}], "abstract": "Dense retrieval has been shown to be effective for Open Domain Question Answering, surpassing sparse retrieval methods like BM25. One such model, REALM, (Guu et al., 2020) is an end-to-end dense retrieval system that uses MLM based pretraining for improved downstream QA performance. However, the current REALM setup uses limited resources and is not comparable in scale to more recent systems, contributing to its lower performance. Additionally, it relies on noisy supervision for retrieval during fine-tuning. We propose REALM++, where we improve upon the training and inference setups and introduce better supervision signal for improving performance, without any architectural changes. REALM++ achieves ~5.5% absolute accuracy gains over the baseline while being faster to train. It also matches the performance of large models which have 3x more parameters demonstrating the efficiency of our setup.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2021, "reference_count": 27, "citation_count": 4, "influential_paper_citations": 0, "is_open_access": true, "citations": ["c2b86e6dee44dd1dc711425e13eadcf04444dea9", "3a07a87090a061ca41dd30ac8398a9a5d9d39826", "471a49220cea2069e8b8a76821b1d2434204a732", "b2faac4ff48056f880bf836c1cc1f8a27ec0a73a"], "references": ["be68cd5d7f14e0b735cd5e9ab0c9e99c3ce290c9", "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "79cd9f77e5258f62c0e15d11534aea6393ef73fe", "832fff14d2ed50eb7969c4c4b976c35776548f56", "80376bdec5f534be78ba82821f540590ebce5559", "521b4e26df0f1cf5763dece14cbb218df152dc59", "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "1715aa36ccc851310308630d4db61dcecf49a50d", "30eff53e981695c7296d258b8dc44b4c7b482a0c", "4bf61dab8ad195e87b6f0496ec7bada5d37c476f", "17dbd7b72029181327732e4d11b52a08ed4630d0", "f5dcdf616c3af51210631d39981de7c131682887", "b29db655a18e7417e1188ba392a06b6314f0cb87", "a81874b4a651a740fffbfc47ef96515e8c7f782f", "e7512b84e923372ae410d7614e71224d573ed2ef", "2fe7dba5a58aee5156594b4d78634ecd6c7dcabd", "ba1382a0574baa0345fd727f259bc86797fe1381", "5237c5cefa83af4aef3251f518a74b598db964b2", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "104715e1097b7ebee436058bfd9f45540f269845", "a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee", "f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "7e5955481e9d197cc1cd1b64a90fbd245b88c886", "6dbffa57b3c6c5645cf701b9b444984a4b61bb57", "b29447ba499507a259ae9d8f685d60cc1597d7d3", "47ced790a563344efae66588b5fb7fe6cca29ed3", "646d4888871aca2a25111eb2520e4c47e253b014"], "url": "https://www.semanticscholar.org/paper/cea1f8894cb73f6b1ece7f465bad18fb4b79168d"}, "0170fc76e934ee643f869df18fb617d5357e8b4e": {"id": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "authors": [{"authorId": "4478284", "name": "Anmol Gulati", "paperCount": 19, "citationCount": 1420, "hIndex": 11}, {"authorId": "47901308", "name": "James Qin", "paperCount": 15, "citationCount": 1883, "hIndex": 12}, {"authorId": "145039780", "name": "C. Chiu", "paperCount": 66, "citationCount": 6676, "hIndex": 31}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "2153632494", "name": "Yu Zhang", "paperCount": 83, "citationCount": 9647, "hIndex": 35}, {"authorId": "150167366", "name": "Jiahui Yu", "paperCount": 59, "citationCount": 7640, "hIndex": 27}, {"authorId": "72549949", "name": "Wei Han", "paperCount": 349, "citationCount": 4233, "hIndex": 26}, {"authorId": "2108553866", "name": "Shibo Wang", "paperCount": 7, "citationCount": 1127, "hIndex": 6}, {"authorId": "2148905602", "name": "Zhengdong Zhang", "paperCount": 8, "citationCount": 1214, "hIndex": 8}, {"authorId": "48607963", "name": "Yonghui Wu", "paperCount": 84, "citationCount": 19126, "hIndex": 49}, {"authorId": "34320634", "name": "Ruoming Pang", "paperCount": 73, "citationCount": 14662, "hIndex": 39}], "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 36, "citation_count": 980, "influential_paper_citations": 191, "is_open_access": true, "citations": ["49a049dc85e2380dde80501a984878341dd8efdf", "4fffa5245d3972077c83614c2a08a47cb578631e", "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "e775e649d815a02373eac840cf5e33a04ff85c95", "47ae807cd511b35e78a2cd4e198283dea6dafd41", "0e2d8b8d81092037f9866c1ceddcebb87318e38b", "416dab850fda842b13a4f28164514d98f836fff7", "2fb4522a8560c8778c2a4e8a94690a24b9c4450a", "1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe", "9ff525d1ebd389c359ddbf06df3e99c433c2bf9e"], "references": ["1bd7d2932de819ed1087b6453ef2c0be9f781ac1", "8474155e4a0085c44b12817d5da5a20853b56285", "45df28a07f2061de04633df1bf40104981f0ca1d", "09e2c7adbed37440d4a339852cfa34e5b660f768", "4097148c147f06b54802000a8476d28e525c63cf", "a0070675b4a7f55777cc1fceb5463c368e236337", "13c0c8a6ded00d8a09415c2cacc52f41fd4211c1", "df67d46e78aae0d2fccfb6212d101a342259c01b", "24472a31618bbc260e2bf45bd72427097875142b", "703685e969fed715e13937c11d7ecc5cc7c4dfd0", "0ce184bd55a4736ec64e5d82a85421298e0373ea", "81e1d123a85562555befb0243256b1a0d9fca014", "a39398f68ae7e042f2ef5009e31b4e6a20fd5736", "27ac832ee83d8b5386917998a171a0257e2151e2", "b0fae9fbb4e580d92395eabafe73e317ae6510e3", "d85b2af4f163383bbfa62b73d5f0b179868cc9a8", "bdc046e65bc80cf13929ca0c3934d6faee830723", "ff413cae44ca5e14281ebe4659b8627c349e8493", "ef523bb9437178c50d1b1e3e3ca5fb230ab37e3f", "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "6dbcb288498657b7d3bb3e59383d0a790123afa3", "41a78e2885b5dc8c719495a33985b5f4880f5b48", "8c1b00128e74f1cd92aede3959690615695d5101", "c6b61535f1544835cca3851ceb34222ebc5b4377", "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "eac48f406c46527f5ca821de7fe8d62d6db56a27", "4f57f486adea0bf95c252620a4e8af39232ef8bc", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "88caa4a0253a8b0076176745ebc072864eab66e1", "34038d9424ce602d7ac917a4e582d977725d4393", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "86efe7769f2b8a0e15ca213ab09881e6705caeb0", "34f25a8704614163c4095b3ee2fc969b60de4698", "57a5fa22f10ce6ccf27286f74a050d2dac037e06", "7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f", "030a977bf32e81fb694117d78ac84a3fbe2a1d81"], "url": "https://www.semanticscholar.org/paper/0170fc76e934ee643f869df18fb617d5357e8b4e"}, "a9c8a5a95a6b62a661aa66394a1d35d372de8c60": {"id": "a9c8a5a95a6b62a661aa66394a1d35d372de8c60", "title": "Words aren\u2019t enough, their order matters: On the Robustness of Grounding Visual Referring Expressions", "authors": [{"authorId": "1630664874", "name": "Ashish Vaswani", "paperCount": 13, "citationCount": 611, "hIndex": 6}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "2086022524", "name": "Jakob", "paperCount": 2, "citationCount": 2, "hIndex": 1}, {"authorId": "2103415727", "name": "Uszkoreit", "paperCount": 2, "citationCount": 0, "hIndex": 0}, {"authorId": "145024664", "name": "Llion Jones", "paperCount": 21, "citationCount": 50175, "hIndex": 15}], "abstract": "Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn\u2019t matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn\u2019t. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-ofthe-art model for this task. Our datasets are publicly available at https://github.com/ aws/aws-refcocog-adv.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2020, "reference_count": 27, "citation_count": 0, "influential_paper_citations": 0, "is_open_access": false, "citations": [], "references": ["47f1eb0dc42189ba7cf21b76598c8217eb1b6e05", "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "455a8838cde44f288d456d01c76ede95b56dc675", "1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1", "9695676deace8c05d4e95274b92f20ed1e97470c", "72d7c465ef199a9670b3da7a318b0227f5cc3229", "b658b8a9fbe1d35cdf1942e5c1bdc546a4c39029", "fdce9cbe5c726201575b3c8a8c1af0752f1af53f", "f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8", "8eee0c0a566c9d59e264dd4119225840caa307dc", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "ce264a4e1490e959d84ddd60edbb0edcbfb3af38", "86eef3a1dff2bd2808847358cdb7f5ba2b7e0214", "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "d76c07211479e233f7c6a6f32d5346c983c5598f", "d696a1923288e6c15422660de9553f6fdb6a4fae", "def584565d05d6a8ba94de6621adab9e301d375d", "21c99706bb26e9012bfb4d8d48009a3d45af59b2", "e65142010431ffc089b272a1174214e00693e503", "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "2e36ea91a3c8fbff92be2989325531b4002e2afc", "92c141447f51b6732242376164ff961e464731c8", "71b7178df5d2b112d07e45038cb5637208659ff7", "44040913380206991b1991daf1192942e038fe31"], "url": "https://www.semanticscholar.org/paper/a9c8a5a95a6b62a661aa66394a1d35d372de8c60"}, "6c8b12d0bc64dd298e91d74f4277f7e2a5bc4477": {"id": "6c8b12d0bc64dd298e91d74f4277f7e2a5bc4477", "title": "High Resolution Medical Image Analysis with Spatial Partitioning", "authors": [{"authorId": "48557308", "name": "L. Hou", "paperCount": 32, "citationCount": 1883, "hIndex": 16}, {"authorId": "73416451", "name": "Youlong Cheng", "paperCount": 11, "citationCount": 553, "hIndex": 5}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "2110765504", "name": "Yeqing Li", "paperCount": 33, "citationCount": 1661, "hIndex": 15}, {"authorId": "1986491", "name": "P. Korfiatis", "paperCount": 60, "citationCount": 2086, "hIndex": 23}, {"authorId": "39785742", "name": "Travis M. Drucker", "paperCount": 14, "citationCount": 460, "hIndex": 9}, {"authorId": "2802969", "name": "D. Blezek", "paperCount": 55, "citationCount": 4466, "hIndex": 19}, {"authorId": "1718192", "name": "Xiaodan Song", "paperCount": 38, "citationCount": 6299, "hIndex": 19}], "abstract": "Medical images such as 3D computerized tomography (CT) scans and pathology images, have hundreds of millions or billions of voxels/pixels. It is infeasible to train CNN models directly on such high resolution images, because neural activations of a single image do not fit in the memory of a single GPU/TPU. Existing image analysis approaches alleviate this problem by cropping or down-sampling input images, which leads to complicated implementation and sub-optimal performance due to information loss. In this paper, we implement spatial partitioning, which internally distributes the input and output of convolutional layers across GPUs/TPUs. Our implementation is based on the Mesh-TensorFlow framework and the computation distribution is transparent to end users. With this technique, we train a 3D Unet on up to 512 by 512 by 512 resolution data. To the best of our knowledge, this is the first work for handling such high resolution images end-to-end.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 19, "citation_count": 19, "influential_paper_citations": 0, "is_open_access": false, "citations": ["b4dc902448bccea400958cb692563e69fe5b1ed3", "b431938300a5e2baa9c88b83d8fec13c087cb85b", "fd7336480ad3fa28730b7c86888552f26fa12fa5", "f199a82266caf5d11b32bf253eaa49e4f2790462", "3dce64743a1f8682ed02883dba1a399c2945c6b5", "38936d256e3a608f4c419b2e94c974834abed8b3", "3db75693b8c8d9c9eb5e7cb3cfffc25395c1689a", "3cbc2890aa893ffa21f4b028dc7fd29e1490b04c", "ee106ea6aa56fac9228b6a1d74b6e16d78ed3f57", "42d260e8eea41ca66388651b5a33f13c7b3df735"], "references": ["0655dcaa39cf41a3609974840f91300d73b4aed1", "72564a69bf339ff1d16a639c86a764db2321caab", "20ad74991d2239682bbbda1d113772aa26b94ca6", "77d1ce749b36c2c53c06a3003b2598bf4f8b2274", "f971658ab845d7573c4bbb760d5e7e5332025254", "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "e2e2e9f858ffc3bde5ee7a6929c9b0e7d52c1a43", "7b05161938f4e5f89ddd7ab7f432de8ecab1566e", "a86d7289c76d832e83c99539859b7b186e4ea6c8", "5397edea91c0b31c4b8c8671dd78ba47de201d5f", "2d8edc4e38bf9907170238726ec902cb3739393b", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "929c9a90e2ceac3f6684f2b29e6579c40b4ce3c1", "7fc464470b441c691d10e7331b14a525bc79b8bb", "9eb063db5a3f8f3f27bb3f60905f2e7263151ff2", "004ef85f9c53dc0db6dc3adc1ca8ecb71f7a06b4", "6364fdaa0a0eccd823a779fcdd489173f938e91a", "3127190433230b3dc1abd0680bb58dced4bcd90e"], "url": "https://www.semanticscholar.org/paper/6c8b12d0bc64dd298e91d74f4277f7e2a5bc4477"}, "fcae82bd4a5fbe2542533cea5ccf1a795c9f64c6": {"id": "fcae82bd4a5fbe2542533cea5ccf1a795c9f64c6", "title": "Corpora Generation for Grammatical Error Correction", "authors": [{"authorId": "51888730", "name": "Jared Lichtarge", "paperCount": 4, "citationCount": 134, "hIndex": 3}, {"authorId": "16618151", "name": "C. Alberti", "paperCount": 6, "citationCount": 239, "hIndex": 5}, {"authorId": "9567965", "name": "Shankar Kumar", "paperCount": 48, "citationCount": 2751, "hIndex": 22}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "2058178029", "name": "Simon Tong", "paperCount": 1, "citationCount": 90, "hIndex": 1}], "abstract": "Grammatical Error Correction (GEC) has been recently modeled using the sequence-to-sequence framework. However, unlike sequence transduction problems such as machine translation, GEC suffers from the lack of plentiful parallel data. We describe two approaches for generating large parallel datasets for GEC using publicly available Wikipedia data. The first method extracts source-target pairs from Wikipedia edit histories with minimal filtration heuristics while the second method introduces noise into Wikipedia sentences via round-trip translation through bridge languages. Both strategies yield similar sized parallel corpora containing around 4B tokens. We employ an iterative decoding strategy that is tailored to the loosely supervised nature of our constructed corpora. We demonstrate that neural GEC models trained using either type of corpora give similar performance. Fine-tuning these models on the Lang-8 corpus and ensembling allows us to surpass the state of the art on both the CoNLL \u201814 benchmark and the JFLEG task. We present systematic analysis that compares the two approaches to data generation and highlights the effectiveness of ensembling.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2019, "reference_count": 39, "citation_count": 90, "influential_paper_citations": 17, "is_open_access": true, "citations": ["7cc6f009feb5ad5ad0e1ff00c551fb318fc95016", "63d8426ba1f51a8525dd19fd8ec92934ec71aea5", "1a5ef51ae0c0ee1216e14aa390734cf7581c3b27", "9da95e99afd4ea899bd1fb40dd350e0be0a12a84", "5714c48293bf77e399fd1ba8ad62e74e252fdb9e", "2704b207c7b8f6fc32bd3d04690b2f4f745c460f", "c3661fc0091545c6b71f78141bdd2075614fe266", "5b834c1ce83767cbb45335e11d75dfdec46f9072", "9ad2f58f1e7f3b3d825d5c6b174fb38f601c6df0", "58ef9f9682c0ae4561dc30079a52867f108f704e"], "references": ["bf5c17840f9af1e4075b23163ad05069adcb7697", "2615e66b4f7783955e232be0ddcec2606e155342", "f6d06993e003fa6fec5bf630efded9e4fd90a030", "be70e163473c1c6e42d02b5c4711d0faa493a49b", "d8146d7f37516caa9446ebca302ed55ba4be57ff", "ca54d56d8942b02ed6e7d441d37372a65c1cc920", "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "6ed38b0cb510fa91434eb63ab464bee66c9323c6", "139768cf7714beb9309efba734460f8562c60c78", "4cac1e1eb876ffdcfda5a62d5237f942b519a502", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "a417fa9de77ea01cf145bcdf882f88efb67733fc", "a486e2839291111bb44fa1f07731ada123539f75", "77e2a8c7afa8694b5a96e1126b8ef9e8d7bf3791", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "b19f365aab0bf8c6cf712c07313b919556bfacc0", "9fa123f2de115cba39f57e30a20efdd03cb45f2e", "6f6b397267946e3faaac4e5fa587e1876fe69da2", "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb", "1af68821518f03568f913ab03fc02080247a27ff", "0a1e65ec196edc20d3d69c110fb259e761f6948f", "0f8361aa258ac81457dde0c04a041193c21afd50", "2826f9dccdcceb113b33ccf2841d488f1419bb30", "51e58ac760a466dda48dfe470b44688277fe2d89", "cea967b59209c6be22829699f05b8b1ac4dc092d", "7041280a628db8217ccdbb130b7211cda359e987", "9d5f5340667f3285a36f7e510d32dd54afb1dc37", "b05f79fe597128bf6e5e8b20ba94530ee6d0ee2f", "41438af89243bfc9e2947ec18a8ae25b4c733973", "516325dff4e0b3cb51600ff36f308d455df2b64a", "1a2ea7741b7fa1156d6785f13f3d6fa949b4f6bc", "ed6262b569c0a62c51d941228c54f34e563af022", "c101f974baf2c533166f93471a7c0563e841e974", "71efd9633bef36d837ecc6753e8c6e0c5f87c839", "c2379a551adb72c1d1d01e627f36ab0ccd977545", "b976c166f88d03d4c7c77405ba41a2013fb88537", "ab288676fad14fa06d7975120df19836ce99e365"], "url": "https://www.semanticscholar.org/paper/fcae82bd4a5fbe2542533cea5ccf1a795c9f64c6"}, "40a992829e1d1d04dfb9eedcccae0010f2c6939d": {"id": "40a992829e1d1d04dfb9eedcccae0010f2c6939d", "title": "Weakly Supervised Grammatical Error Correction using Iterative Decoding", "authors": [{"authorId": "51888730", "name": "Jared Lichtarge", "paperCount": 4, "citationCount": 134, "hIndex": 3}, {"authorId": "16618151", "name": "C. Alberti", "paperCount": 6, "citationCount": 239, "hIndex": 5}, {"authorId": "9567965", "name": "Shankar Kumar", "paperCount": 48, "citationCount": 2751, "hIndex": 22}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}], "abstract": "We describe an approach to Grammatical Error Correction (GEC) that is effective at making use of models trained on large amounts of weakly supervised bitext. We train the Transformer sequence-to-sequence model on 4B tokens of Wikipedia revisions and employ an iterative decoding strategy that is tailored to the loosely-supervised nature of the Wikipedia training corpus. Finetuning on the Lang-8 corpus and ensembling yields an F0.5 of 58.3 on the CoNLL'14 benchmark and a GLEU of 62.4 on JFLEG. The combination of weakly supervised training and iterative decoding obtains an F0.5 of 48.2 on CoNLL'14 even without using any labeled GEC data.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 16, "citation_count": 22, "influential_paper_citations": 9, "is_open_access": false, "citations": ["3f3704d87860a816ac3cc7257a9acccf0d463b7a", "7cc6f009feb5ad5ad0e1ff00c551fb318fc95016", "2704b207c7b8f6fc32bd3d04690b2f4f745c460f", "ebfab6d86bb1565ff4435d3e8afe44ce9526dd4e", "78a1a59b10570ad7b62d5f7675fd4f8b4c27dc2d", "904b7802d2c07161c11351fdc246f48a4a3da29b", "e492e2f6e54982fddc4e8d5448362ce890d57c31", "25dd4ef9d61ae7da1e3e3d9206ecac2966b1c880", "08567bab1d37d166a2ce6c10f7d4132769c438f6", "76bbf7de365a5633a7da442693ae829ffe23e019"], "references": ["2615e66b4f7783955e232be0ddcec2606e155342", "d8146d7f37516caa9446ebca302ed55ba4be57ff", "ca54d56d8942b02ed6e7d441d37372a65c1cc920", "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "6ed38b0cb510fa91434eb63ab464bee66c9323c6", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "b19f365aab0bf8c6cf712c07313b919556bfacc0", "9fa123f2de115cba39f57e30a20efdd03cb45f2e", "0f8361aa258ac81457dde0c04a041193c21afd50", "2826f9dccdcceb113b33ccf2841d488f1419bb30", "51e58ac760a466dda48dfe470b44688277fe2d89", "b05f79fe597128bf6e5e8b20ba94530ee6d0ee2f", "1a2ea7741b7fa1156d6785f13f3d6fa949b4f6bc", "ed6262b569c0a62c51d941228c54f34e563af022", "c101f974baf2c533166f93471a7c0563e841e974", "ab288676fad14fa06d7975120df19836ce99e365"], "url": "https://www.semanticscholar.org/paper/40a992829e1d1d04dfb9eedcccae0010f2c6939d"}, "72ad28675cf8786985523ac4e5f9125e9a1cede9": {"id": "72ad28675cf8786985523ac4e5f9125e9a1cede9", "title": "Image Tranformer", "authors": [{"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "1630664874", "name": "Ashish Vaswani", "paperCount": 13, "citationCount": 611, "hIndex": 6}, {"authorId": "39328010", "name": "Jakob Uszkoreit", "paperCount": 47, "citationCount": 64367, "hIndex": 30}, {"authorId": "69894932", "name": "\u0141ukasz Kaiser", "paperCount": 6, "citationCount": 310, "hIndex": 2}, {"authorId": "1846258", "name": "Noam M. Shazeer", "paperCount": 58, "citationCount": 62483, "hIndex": 32}, {"authorId": "31702389", "name": "Alexander Ku", "paperCount": 18, "citationCount": 1905, "hIndex": 11}], "abstract": null, "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 0, "citation_count": 1, "influential_paper_citations": 0, "is_open_access": false, "citations": ["63af6a793906a4d88c7a66aa5d42625a0a1edf3d"], "references": [], "url": "https://www.semanticscholar.org/paper/72ad28675cf8786985523ac4e5f9125e9a1cede9"}, "bb669de2fce407df2f5cb2f8c51dedee3f467e04": {"id": "bb669de2fce407df2f5cb2f8c51dedee3f467e04", "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation", "authors": [{"authorId": "48623026", "name": "M. Chen", "paperCount": 15, "citationCount": 1114, "hIndex": 9}, {"authorId": "2345617", "name": "Orhan Firat", "paperCount": 95, "citationCount": 8131, "hIndex": 33}, {"authorId": "12295226", "name": "Ankur Bapna", "paperCount": 44, "citationCount": 2234, "hIndex": 20}, {"authorId": "145657834", "name": "Melvin Johnson", "paperCount": 36, "citationCount": 9919, "hIndex": 22}, {"authorId": "3153147", "name": "Wolfgang Macherey", "paperCount": 45, "citationCount": 8305, "hIndex": 28}, {"authorId": "2458308", "name": "George F. Foster", "paperCount": 110, "citationCount": 5124, "hIndex": 34}, {"authorId": "145024664", "name": "Llion Jones", "paperCount": 21, "citationCount": 50175, "hIndex": 15}, {"authorId": "3877127", "name": "Niki Parmar", "paperCount": 24, "citationCount": 51642, "hIndex": 15}, {"authorId": "144927151", "name": "M. Schuster", "paperCount": 46, "citationCount": 28668, "hIndex": 20}, {"authorId": "2111317372", "name": "Zhifeng Chen", "paperCount": 15, "citationCount": 1613, "hIndex": 10}, {"authorId": "48607963", "name": "Yonghui Wu", "paperCount": 84, "citationCount": 19126, "hIndex": 49}, {"authorId": "48342565", "name": "Macduff Hughes", "paperCount": 7, "citationCount": 7241, "hIndex": 6}], "abstract": "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT\u201914 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.", "arxiv_id": null, "primary_category": null, "secondary_category": null, "year": 2018, "reference_count": 47, "citation_count": 365, "influential_paper_citations": 22, "is_open_access": true, "citations": ["ad4a0938c48e61b7827869e4ac3baffd0aefab35", "f6fbb6809374ca57205bd2cf1421d4f4fa04f975", "c143ea9e30b1f2d93a9c060253845423f9e60e1f", "c18663fea10c8a303d045fd2c1f33cacf9b73ca3", "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "949fef650da4c41afe6049a183b504b3cc91f4bd", "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "9784fbf77295860b2e412137b86356d70b25e3c0", "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5"], "references": ["de5168baf710e922bb042043c3cd8d2feb411aad", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "f4c8539bed600c9c652aba76a996b8188761d3fe", "43428880d75b3a14257c3ee9bda054e61eb869c0", "deff3b0635e141297238797d924d7a9aba3a132a", "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "4550a4c714920ef57d19878e31c9ebae37b049b2", "88caa4a0253a8b0076176745ebc072864eab66e1", "7dbb2d983ab95da04e5d47c87ddd2cd9a8f20786", "98445f4172659ec5e891e031d8202c102135c644", "30e0b5cb62be2dc2c14d3ecae7cccceddfeab4c8", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "97fb4e3d45bb098e27e0071448b6152217bd35a5", "b60abe57bc195616063be10638c6437358c81d1e", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "f48d5181ab3440f96e6289ce4a505d441e9f001b", "3d2c6941a9b4608ba52b328369a3352db2092ae0", "25fb5a6abcd88ee52bdb3165b844c941e90eb9bf", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "23ffaa0fe06eae05817f527a47ac3291077f9e58", "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "e0945081b5b87187a53d4329cf77cd8bff635795", "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "cea967b59209c6be22829699f05b8b1ac4dc092d", "0b544dfe355a5070b60986319a3f51fb45d1348e", "533ee188324b833e059cb59b654e6160776d5812", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "3127190433230b3dc1abd0680bb58dced4bcd90e", "ed6262b569c0a62c51d941228c54f34e563af022", "2f83f6e1afadf0963153974968af6b8342775d82", "e77ba3c0f2378937305f1eb174a012d45cc25e0f", "aed054834e2c696807cc8b227ac7a4197196e211", "11540131eae85b2e11d53df7f1360eeb6476e7f4", "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "501ffe07846b34c91b3dd6fdb4fc02f22087add2", "d0be39ee052d246ae99c082a565aba25b811be2d", "17594df98c222217a11510dd454ba52a5a737378", "3f3d13e95c25a8f6a753e38dfce88885097cbd43", "668087f0ae7ce1de6e0bd0965dbb480c08103260", "f22f6972e66bdd2e769fa64b0df0a13063c0c101", "69a9f949361cae3f3de992ea5819b10fbc0befba"], "url": "https://www.semanticscholar.org/paper/bb669de2fce407df2f5cb2f8c51dedee3f467e04"}}